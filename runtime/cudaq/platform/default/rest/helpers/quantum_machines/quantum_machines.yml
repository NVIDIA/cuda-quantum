# ============================================================================ #
# Copyright (c) 2022 - 2025 NVIDIA Corporation & Affiliates.                   #
# All rights reserved.                                                         #
#                                                                              #
# This source code and the accompanying materials are made available under     #
# the terms of the Apache License 2.0 which accompanies this distribution.     #
# ============================================================================ #

name: "quantum_machines"
description: "CUDA-Q target for Quantum Machines."

config:
  # Tell DefaultQuantumPlatform what QPU subtype to use
  platform-qpu: remote_rest
  # Add the rest-qpu library to the link list
  link-libs: ["-lcudaq-rest-qpu"]
  # Tell NVQ++ to generate glue code to set the target backend name
  gen-target-backend: true
  # Add preprocessor defines to compilation
  preprocessor-defines: ["-D CUDAQ_QUANTUM_DEVICE"]
  # Define the lowering pipeline
  platform-lowering-config: "classical-optimization-pipeline,globalize-array-values,func.func(state-prep),unitary-synthesis,canonicalize,apply-op-specialization,aggressive-early-inlining,classical-optimization-pipeline,func.func(lower-to-cfg),canonicalize,func.func(multicontrol-decomposition),decomposition{enable-patterns=SToR1,TToR1,CCZToCX,CRyToCX,CRxToCX,R1AdjToR1,RxAdjToRx,RyAdjToRy,RzAdjToRz},quake-to-cc-prep,func.func(memtoreg{quantum=0}),symbol-dce"
  # Tell the rest-qpu that we are generating OpenQASM 2.0.
  codegen-emission: qasm2
  # Library mode is only for simulators, physical backends must turn this off
  library-mode: false

target-arguments:
  - key: machine
    required: false
    type: string
    platform-arg: machine
    help-string: "Specify the Quantum Machines backend to run on."
  - key: method
    required: false
    type: string
    platform-arg: method
    help-string: "Specify the circuit execution type, either: dry-run (ideal simulation) or noise-sim (noisy-simulation)."
