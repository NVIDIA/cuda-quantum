<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Local Installation &mdash; NVIDIA CUDA-Q  documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/tabs.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/cudaq_override.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/_static/cudaq_override.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/tabs.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Installation from Source" href="data_center_install.html" />
    <link rel="prev" title="Installation Guide" href="install.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #76b900" > 

          
          
          <a href="../../index.html" class="icon icon-home">
            NVIDIA CUDA-Q
          </a>
              <div class="version">
                amd64-pr-1844
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

  <style>
    /* Sidebar header (and topbar for mobile) */
    .wy-side-nav-search, .wy-nav-top {
      background: #76b900;
    }

    .wy-side-nav-search a:link, .wy-nav-top a:link {
      color: #fff;
    }
    .wy-side-nav-search a:visited, .wy-nav-top a:visited {
      color: #fff;
    }
    .wy-side-nav-search a:hover, .wy-nav-top a:hover {
      color: #fff;
    }

    .wy-menu-vertical a:link, .wy-menu-vertical a:visited {
      color: #d9d9d9
    }

    .wy-menu-vertical a:active {
      background-color: #76b900
    }

    .wy-side-nav-search>div.version {
      color: rgba(0, 0, 0, 0.3)
    }

    /* override table width restrictions */
    .wy-table-responsive table td, .wy-table-responsive table th {
        white-space: normal;
    }

    .wy-table-responsive {
        margin-bottom: 24px;
        max-width: 100%;
        overflow: visible;
    }
  </style>
  
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../quick_start.html">   Quick Start</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../quick_start.html#install-cuda-q">Install CUDA-Q</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quick_start.html#validate-your-installation">Validate your Installation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../basics/basics.html">   Basics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../basics/kernel_intro.html">   What is a CUDA-Q Kernel?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/build_kernel.html">   Building your first CUDA-Q Program</a></li>
<li class="toctree-l2"><a class="reference internal" href="../basics/run_kernel.html">   Running your first CUDA-Q Program</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../basics/run_kernel.html#sample">Sample</a></li>
<li class="toctree-l3"><a class="reference internal" href="../basics/run_kernel.html#observe">Observe</a></li>
<li class="toctree-l3"><a class="reference internal" href="../basics/run_kernel.html#running-on-a-gpu">Running on a GPU</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../basics/troubleshooting.html">   Troubleshooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../basics/troubleshooting.html#debugging-and-verbose-simulation-output">Debugging and Verbose Simulation Output</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../examples/examples.html">   Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/introduction.html">   Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/quantum_operations.html">   Quantum Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantum_operations.html#quantum-states">Quantum States</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantum_operations.html#quantum-gates">Quantum Gates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/quantum_operations.html#measurements">Measurements</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/visualization.html">   Visualization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/visualization.html#Qubit-Visualization">Qubit Visualization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/visualization.html#Kernel-Visualization">Kernel Visualization</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/expectation_values.html">   Computing Expectation Values</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/expectation_values.html#parallelizing-across-multiple-processors">Parallelizing across Multiple Processors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/multi_control.html">   Multi-Control Synthesis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/multi_gpu_workflows.html">   Multi-GPU Workflows</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/multi_gpu_workflows.html#available-targets">Available Targets</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/multi_gpu_workflows.html#parallelization-across-multiple-processors">Parallelization across Multiple Processors</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../examples/multi_gpu_workflows.html#batching-hamiltonian-terms">Batching Hamiltonian Terms</a></li>
<li class="toctree-l4"><a class="reference internal" href="../examples/multi_gpu_workflows.html#circuit-batching">Circuit Batching</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../examples/bernstein_vazirani.html">   Bernstein-Vazirani</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/vqe.html">   Variational Quantum Eigensolver</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/qaoa.html">   Quantum Approximate Optimization Algorithm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/cuquantum.html">   Simulations with cuQuantum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/noisy_simulation.html">   Noisy Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/hardware_providers.html">   Using Quantum Hardware Providers</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../examples/hardware_providers.html#ionq">IonQ</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hardware_providers.html#iqm">IQM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hardware_providers.html#oqc">OQC</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hardware_providers.html#orca-computing">ORCA Computing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../examples/hardware_providers.html#quantinuum">Quantinuum</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">   Applications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html">Quantum Enhanced Auxiliary Field Quantum Monte Carlo</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html#Hamiltonian-preparation-for-VQE">Hamiltonian preparation for VQE</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html#Run-VQE-with-CUDA-Q">Run VQE with CUDA-Q</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html#Auxiliary-Field-Quantum-Monte-Carlo-(AFQMC)">Auxiliary Field Quantum Monte Carlo (AFQMC)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html#Preparation-of-the-molecular-Hamiltonian">Preparation of the molecular Hamiltonian</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html#Preparation-of-the-trial-wave-function">Preparation of the trial wave function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/afqmc.html#Setup-of-the-AFQMC-parameters">Setup of the AFQMC parameters</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/deutschs_algorithm.html">Deutsch’s Algorithm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/deutschs_algorithm.html#XOR-\oplus">XOR <span class="math notranslate nohighlight">\(\oplus\)</span></a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/deutschs_algorithm.html#Quantum-oracles">Quantum oracles</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/deutschs_algorithm.html#Phase-oracle">Phase oracle</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/deutschs_algorithm.html#Quantum-parallelism">Quantum parallelism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/deutschs_algorithm.html#Deutschs'-Algorithm:">Deutschs’ Algorithm:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/quantum_fourier_transform.html">Quantum Fourier Transform</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/quantum_fourier_transform.html#Quantum-Fourier-Transform-revisited">Quantum Fourier Transform revisited</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/cost_minimization.html">Cost Minimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/vqe.html">Variational Quantum Eigensolver</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/vqe.html#Using-CUDA-Q-Optimizers">Using CUDA-Q Optimizers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/vqe.html#Integration-with-Third-Party-Optimizers">Integration with Third-Party Optimizers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/qaoa.html">Max-Cut with QAOA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/hadamard_test.html">Hadamard Test</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/hadamard_test.html#A--Numerical-result-as-a-reference:">A- Numerical result as a reference:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/hadamard_test.html#B--Using-sample-algorithmic-primitive-to-sample-the-ancilla-qubit-and-compute-the-expectation-value.">B- Using <code class="docutils literal notranslate"><span class="pre">sample</span></code> algorithmic primitive to sample the ancilla qubit and compute the expectation value.</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/hadamard_test.html#C--Use-multi-GPUs-to-compute-the-matrix-elements">C- Use multi-GPUs to compute the matrix elements</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../examples/python/tutorials/hadamard_test.html#Diagonalize-the-matrix-using-for-example-Numpy-or-CuPy.-In-this-example,-since-we-are-having-2x2-matrix,-we-use-numpy.">Diagonalize the matrix using for example Numpy or CuPy. In this example, since we are having 2x2 matrix, we use numpy.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/hybrid_qnns.html">Hybrid Quantum Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/maximum_vertex_weight_clique.html">Molecular docking via DC-QAOA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/noisy_simulations.html">Noisy Simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/readout_error_mitigation.html">Readout Error Mitigation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/readout_error_mitigation.html#Inverse-confusion-matrix-from-single-qubit-noise-model">Inverse confusion matrix from single-qubit noise model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/readout_error_mitigation.html#Inverse-confusion-matrix-from-k-local-confusion-matrices">Inverse confusion matrix from k local confusion matrices</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/readout_error_mitigation.html#Inverse-of-full-confusion-matrix">Inverse of full confusion matrix</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/vqe_water_active_space.html">Water Molecule with Active Space (CPU vs. GPU)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/vqe_water_active_space.html#A--Classical-simulation-as-a-reference:-CCSD">A- Classical simulation as a reference: CCSD</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/vqe_water_active_space.html#B--VQE-UCCSD:">B- VQE-UCCSD:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../examples/python/tutorials/Divisive_clustering.html">Divisive Clustering With Coresets Using CUDA-Q</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/Divisive_clustering.html#Data-preprocessing">Data preprocessing</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/Divisive_clustering.html#Quantum-functions">Quantum functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/Divisive_clustering.html#Divisive-Clustering-Function">Divisive Clustering Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/Divisive_clustering.html#QAOA-Implementation">QAOA Implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../examples/python/tutorials/Divisive_clustering.html#Scaling-simulations-with-CUDA-Q">Scaling simulations with CUDA-Q</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../backends/backends.html">   Backends</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../backends/simulators.html">   Simulation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../backends/simulators.html#state-vector-simulators">State Vector Simulators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/simulators.html#features">Features</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/simulators.html#single-gpu">Single-GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/simulators.html#multi-node-multi-gpu">Multi-node multi-GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/simulators.html#openmp-cpu-only">OpenMP CPU-only</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/simulators.html#tensor-network-simulators">Tensor Network Simulators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/simulators.html#id2">Multi-node multi-GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/simulators.html#matrix-product-state">Matrix product state</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/simulators.html#default-simulator">Default Simulator</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../backends/hardware.html">   Quantum Hardware</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../backends/hardware.html#ionq">IonQ</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#setting-credentials">Setting Credentials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#submission-from-c">Submission from C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#submission-from-python">Submission from Python</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/hardware.html#iqm">IQM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id1">Setting Credentials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id2">Submission from C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id3">Submission from Python</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/hardware.html#oqc">OQC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id4">Setting Credentials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id5">Submission from C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id6">Submission from Python</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/hardware.html#orca-computing">ORCA Computing</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id7">Setting Credentials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id8">Submission from C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id9">Submission from Python</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/hardware.html#quantinuum">Quantinuum</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#quantinuum-backend">Setting Credentials</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id11">Submission from C++</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/hardware.html#id12">Submission from Python</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../backends/nvqc.html">   NVIDIA Quantum Cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../backends/nvqc.html#quick-start">Quick Start</a></li>
<li class="toctree-l3"><a class="reference internal" href="../backends/nvqc.html#simulator-backend-selection">Simulator Backend Selection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../backends/nvqc.html#multiple-gpus">Multiple GPUs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../backends/nvqc.html#multiple-qpus-asynchronous-execution">Multiple QPUs Asynchronous Execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../backends/nvqc.html#faq">FAQ</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../backends/platform.html">   Multi-Processor Platforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../backends/platform.html#nvidia-mqpu-platform">NVIDIA <code class="code docutils literal notranslate"><span class="pre">MQPU</span></code> Platform</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/platform.html#parallel-distribution-mode">Parallel distribution mode</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../backends/platform.html#remote-mqpu-platform">Remote <code class="code docutils literal notranslate"><span class="pre">MQPU</span></code> Platform</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../backends/platform.html#supported-kernel-arguments">Supported Kernel Arguments</a></li>
<li class="toctree-l4"><a class="reference internal" href="../backends/platform.html#accessing-simulated-quantum-state">Accessing Simulated Quantum State</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="install.html">   Installation</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Local Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#docker">Docker</a></li>
<li class="toctree-l4"><a class="reference internal" href="#singularity">Singularity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#python-wheels">Python wheels</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pre-built-binaries">Pre-built binaries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#development-with-vs-code">Development with VS Code</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-a-docker-container">Using a Docker container</a></li>
<li class="toctree-l4"><a class="reference internal" href="#using-a-singularity-container">Using a Singularity container</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#connecting-to-a-remote-host">Connecting to a Remote Host</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#developing-with-remote-tunnels">Developing with Remote Tunnels</a></li>
<li class="toctree-l4"><a class="reference internal" href="#remote-access-via-ssh">Remote Access via SSH</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#dgx-cloud">DGX Cloud</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#get-started">Get Started</a></li>
<li class="toctree-l4"><a class="reference internal" href="#use-jupyterlab">Use JupyterLab</a></li>
<li class="toctree-l4"><a class="reference internal" href="#use-vs-code">Use VS Code</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#additional-cuda-tools">Additional CUDA Tools</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#installation-via-pypi">Installation via PyPI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installation-in-container-images">Installation In Container Images</a></li>
<li class="toctree-l4"><a class="reference internal" href="#installing-pre-built-binaries">Installing Pre-built Binaries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-computing-with-mpi">Distributed Computing with MPI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#updating-cuda-q">Updating CUDA-Q</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dependencies-and-compatibility">Dependencies and Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="data_center_install.html">Data Center Installation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="data_center_install.html#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="data_center_install.html#build-dependencies">Build Dependencies</a><ul>
<li class="toctree-l4"><a class="reference internal" href="data_center_install.html#cuda">CUDA</a></li>
<li class="toctree-l4"><a class="reference internal" href="data_center_install.html#toolchain">Toolchain</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="data_center_install.html#building-cuda-q">Building CUDA-Q</a></li>
<li class="toctree-l3"><a class="reference internal" href="data_center_install.html#python-support">Python Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="data_center_install.html#c-support">C++ Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="data_center_install.html#installation-on-the-host">Installation on the Host</a><ul>
<li class="toctree-l4"><a class="reference internal" href="data_center_install.html#cuda-runtime-libraries">CUDA Runtime Libraries</a></li>
<li class="toctree-l4"><a class="reference internal" href="data_center_install.html#mpi">MPI</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../integration/integration.html">   Integration</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../integration/cmake_app.html">Downstream CMake Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../integration/cuda_gpu.html">Combining CUDA with CUDA-Q</a></li>
<li class="toctree-l2"><a class="reference internal" href="../integration/libraries.html">Integrating with Third-Party Libraries</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../integration/libraries.html#calling-a-cuda-q-library-from-c">Calling a CUDA-Q library from C++</a></li>
<li class="toctree-l3"><a class="reference internal" href="../integration/libraries.html#calling-an-c-library-from-cuda-q">Calling an C++ library from CUDA-Q</a></li>
<li class="toctree-l3"><a class="reference internal" href="../integration/libraries.html#interfacing-between-binaries-compiled-with-a-different-toolchains">Interfacing between binaries compiled with a different toolchains</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../extending/extending.html">   Extending</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../extending/nvqir_simulator.html">Create a new NVQIR Simulator</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../extending/nvqir_simulator.html#circuitsimulator"><code class="code docutils literal notranslate"><span class="pre">CircuitSimulator</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../extending/nvqir_simulator.html#let-s-see-this-in-action">Let’s see this in action</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../extending/cudaq_ir.html">Working with CUDA-Q IR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../extending/mlir_pass.html">Create an MLIR Pass for CUDA-Q</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../specification/index.html">   Specifications</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../specification/cudaq.html">   Language Specification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/machine_model.html">1. Machine Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/namespace.html">2. Namespace and Standard</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/types.html">3. Quantum Types</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/types.html#cudaq-qudit-levels">3.1. <code class="code docutils literal notranslate"><span class="pre">cudaq::qudit&lt;Levels&gt;</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/types.html#cudaq-qubit">3.2. <code class="code docutils literal notranslate"><span class="pre">cudaq::qubit</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/types.html#quantum-containers">3.3. Quantum Containers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/operators.html">4. Quantum Operators</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/operators.html#cudaq-spin-op">4.1. <code class="code docutils literal notranslate"><span class="pre">cudaq::spin_op</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/operations.html">5. Quantum Operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/operations.html#operations-on-cudaq-qubit">5.1. Operations on <code class="code docutils literal notranslate"><span class="pre">cudaq::qubit</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/kernels.html">6. Quantum Kernels</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/synthesis.html">7. Sub-circuit Synthesis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/control_flow.html">8. Control Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/dynamic_kernels.html">9. Just-in-Time Kernel Creation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/patterns.html">10. Quantum Patterns</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/patterns.html#compute-action-uncompute">10.1. Compute-Action-Uncompute</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/platform.html">11. Platform</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/algorithmic_primitives.html">12. Algorithmic Primitives</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/algorithmic_primitives.html#cudaq-sample">12.1. <code class="code docutils literal notranslate"><span class="pre">cudaq::sample</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/algorithmic_primitives.html#cudaq-observe">12.2. <code class="code docutils literal notranslate"><span class="pre">cudaq::observe</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/algorithmic_primitives.html#cudaq-optimizer-deprecated-functionality-moved-to-cuda-q-libraries">12.3. <code class="code docutils literal notranslate"><span class="pre">cudaq::optimizer</span></code> (deprecated, functionality moved to CUDA-Q libraries)</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/algorithmic_primitives.html#cudaq-gradient-deprecated-functionality-moved-to-cuda-q-libraries">12.4. <code class="code docutils literal notranslate"><span class="pre">cudaq::gradient</span></code> (deprecated, functionality moved to CUDA-Q libraries)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/cudaq/examples.html">13. Example Programs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/examples.html#hello-world-simple-bell-state">13.1. Hello World - Simple Bell State</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/examples.html#ghz-state-preparation-and-sampling">13.2. GHZ State Preparation and Sampling</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/examples.html#quantum-phase-estimation">13.3. Quantum Phase Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/examples.html#deuteron-binding-energy-parameter-sweep">13.4. Deuteron Binding Energy Parameter Sweep</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/examples.html#grover-s-algorithm">13.5. Grover’s Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../specification/cudaq/examples.html#iterative-phase-estimation">13.6. Iterative Phase Estimation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../specification/quake-dialect.html">   Quake Specification</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../specification/quake-dialect.html#general-introduction">General Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../specification/quake-dialect.html#motivation">Motivation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../api/api.html">   API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api/languages/cpp_api.html">C++ API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#operators">Operators</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#quantum">Quantum</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#common">Common</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#noise-modeling">Noise Modeling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#kernel-builder">Kernel Builder</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#algorithms">Algorithms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#platform">Platform</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#utilities">Utilities</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/cpp_api.html#namespaces">Namespaces</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api/languages/python_api.html">Python API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/python_api.html#program-construction">Program Construction</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.make_kernel"><code class="docutils literal notranslate"><span class="pre">make_kernel()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.PyKernel"><code class="docutils literal notranslate"><span class="pre">PyKernel</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.Kernel"><code class="docutils literal notranslate"><span class="pre">Kernel</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.PyKernelDecorator"><code class="docutils literal notranslate"><span class="pre">PyKernelDecorator</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.kernel"><code class="docutils literal notranslate"><span class="pre">kernel()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/python_api.html#kernel-execution">Kernel Execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.sample"><code class="docutils literal notranslate"><span class="pre">sample()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.sample_async"><code class="docutils literal notranslate"><span class="pre">sample_async()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.observe"><code class="docutils literal notranslate"><span class="pre">observe()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.observe_async"><code class="docutils literal notranslate"><span class="pre">observe_async()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.get_state"><code class="docutils literal notranslate"><span class="pre">get_state()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.get_state_async"><code class="docutils literal notranslate"><span class="pre">get_state_async()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.vqe"><code class="docutils literal notranslate"><span class="pre">vqe()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.draw"><code class="docutils literal notranslate"><span class="pre">draw()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.translate"><code class="docutils literal notranslate"><span class="pre">translate()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/python_api.html#backend-configuration">Backend Configuration</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.has_target"><code class="docutils literal notranslate"><span class="pre">has_target()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.get_target"><code class="docutils literal notranslate"><span class="pre">get_target()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.get_targets"><code class="docutils literal notranslate"><span class="pre">get_targets()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.set_target"><code class="docutils literal notranslate"><span class="pre">set_target()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.reset_target"><code class="docutils literal notranslate"><span class="pre">reset_target()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.set_noise"><code class="docutils literal notranslate"><span class="pre">set_noise()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.unset_noise"><code class="docutils literal notranslate"><span class="pre">unset_noise()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.initialize_cudaq"><code class="docutils literal notranslate"><span class="pre">initialize_cudaq()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.num_available_gpus"><code class="docutils literal notranslate"><span class="pre">num_available_gpus()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.set_random_seed"><code class="docutils literal notranslate"><span class="pre">set_random_seed()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/python_api.html#data-types">Data Types</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.SimulationPrecision"><code class="docutils literal notranslate"><span class="pre">SimulationPrecision</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.Target"><code class="docutils literal notranslate"><span class="pre">Target</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.State"><code class="docutils literal notranslate"><span class="pre">State</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.Tensor"><code class="docutils literal notranslate"><span class="pre">Tensor</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.QuakeValue"><code class="docutils literal notranslate"><span class="pre">QuakeValue</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.qubit"><code class="docutils literal notranslate"><span class="pre">qubit</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.qreg"><code class="docutils literal notranslate"><span class="pre">qreg</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.qvector"><code class="docutils literal notranslate"><span class="pre">qvector</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.ComplexMatrix"><code class="docutils literal notranslate"><span class="pre">ComplexMatrix</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.SpinOperator"><code class="docutils literal notranslate"><span class="pre">SpinOperator</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.spin.i"><code class="docutils literal notranslate"><span class="pre">spin.i()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.spin.x"><code class="docutils literal notranslate"><span class="pre">spin.x()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.spin.y"><code class="docutils literal notranslate"><span class="pre">spin.y()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.spin.z"><code class="docutils literal notranslate"><span class="pre">spin.z()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.SampleResult"><code class="docutils literal notranslate"><span class="pre">SampleResult</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.AsyncSampleResult"><code class="docutils literal notranslate"><span class="pre">AsyncSampleResult</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.ObserveResult"><code class="docutils literal notranslate"><span class="pre">ObserveResult</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.AsyncObserveResult"><code class="docutils literal notranslate"><span class="pre">AsyncObserveResult</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.AsyncStateResult"><code class="docutils literal notranslate"><span class="pre">AsyncStateResult</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.OptimizationResult"><code class="docutils literal notranslate"><span class="pre">OptimizationResult</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#optimizers">Optimizers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#gradients">Gradients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#noisy-simulation">Noisy Simulation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/languages/python_api.html#mpi-submodule">MPI Submodule</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.initialize"><code class="docutils literal notranslate"><span class="pre">initialize()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.rank"><code class="docutils literal notranslate"><span class="pre">rank()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.num_ranks"><code class="docutils literal notranslate"><span class="pre">num_ranks()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.all_gather"><code class="docutils literal notranslate"><span class="pre">all_gather()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.broadcast"><code class="docutils literal notranslate"><span class="pre">broadcast()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.is_initialized"><code class="docutils literal notranslate"><span class="pre">is_initialized()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/languages/python_api.html#cudaq.mpi.finalize"><code class="docutils literal notranslate"><span class="pre">finalize()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../api/default_ops.html">Quantum Operations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../api/default_ops.html#unitary-operations-on-qubits">Unitary Operations on Qubits</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#x"><code class="code docutils literal notranslate"><span class="pre">x</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#y"><code class="code docutils literal notranslate"><span class="pre">y</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#z"><code class="code docutils literal notranslate"><span class="pre">z</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#h"><code class="code docutils literal notranslate"><span class="pre">h</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#r1"><code class="code docutils literal notranslate"><span class="pre">r1</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#rx"><code class="code docutils literal notranslate"><span class="pre">rx</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#ry"><code class="code docutils literal notranslate"><span class="pre">ry</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#rz"><code class="code docutils literal notranslate"><span class="pre">rz</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#s"><code class="code docutils literal notranslate"><span class="pre">s</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#t"><code class="code docutils literal notranslate"><span class="pre">t</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#swap"><code class="code docutils literal notranslate"><span class="pre">swap</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#u3"><code class="code docutils literal notranslate"><span class="pre">u3</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/default_ops.html#adjoint-and-controlled-operations">Adjoint and Controlled Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../api/default_ops.html#measurements-on-qubits">Measurements on Qubits</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#mz"><code class="code docutils literal notranslate"><span class="pre">mz</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#mx"><code class="code docutils literal notranslate"><span class="pre">mx</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../../api/default_ops.html#my"><code class="code docutils literal notranslate"><span class="pre">my</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../api/default_ops.html#user-defined-custom-operations">User-Defined Custom Operations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../versions.html">   Other Versions</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #76b900" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">NVIDIA CUDA-Q</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="install.html">Installation Guide</a></li>
      <li class="breadcrumb-item active">Local Installation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/using/install/local_installation.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul><div class="rst-breadcrumbs-buttons" role="navigation" aria-label="Sequential page navigation">
        <a href="install.html" class="btn btn-neutral float-left" title="Installation Guide" accesskey="p"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_center_install.html" class="btn btn-neutral float-right" title="Installation from Source" accesskey="n">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
  </div>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="local-installation">
<h1>Local Installation<a class="headerlink" href="#local-installation" title="Permalink to this heading">¶</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>This guide walks through how to <a class="reference internal" href="install.html#install-cuda-quantum"><span class="std std-ref">install CUDA-Q</span></a> on your system, and how to set up <a class="reference internal" href="#local-development-with-vscode"><span class="std std-ref">VS Code for local development</span></a>.
The section on <a class="reference internal" href="#connect-to-remote"><span class="std std-ref">connecting to a remote host</span></a> contains some
guidance for application development on a remote host where CUDA-Q is installed.</p>
<p>The following sections contain instructions for how to install CUDA-Q on your machine using</p>
<ul class="simple">
<li><p><a class="reference internal" href="#install-docker-image"><span class="std std-ref">Docker</span></a>: A fully featured CUDA-Q installation including all C++ and Python tools is available as a <a class="reference external" href="https://docs.docker.com/get-started/overview/">Docker</a> image.</p></li>
<li><p><a class="reference internal" href="#install-singularity-image"><span class="std std-ref">Singularity</span></a>: A <a class="reference external" href="https://docs.sylabs.io/guides/latest/user-guide/introduction.html">Singularity</a> container can easily be created based on our Docker images.</p></li>
<li><p><a class="reference internal" href="#install-python-wheels"><span class="std std-ref">PyPI</span></a>: Additionally, we distribute pre-built Python wheels via <a class="reference external" href="https://pypi.org">PyPI</a>.</p></li>
<li><p><a class="reference internal" href="#install-prebuilt-binaries"><span class="std std-ref">Pre-built binaries</span></a>: We also provide pre-built C++ binaries, bundled as <a class="reference external" href="https://makeself.io/">self-extracting archive</a>, that work across a range of Linux operating systems.</p></li>
</ul>
<p>If you would like to build CUDA-Q from source to deploy on an HPC system without relying on a container runtime, please follow the instructions for <a class="reference internal" href="data_center_install.html"><span class="doc">Installation from Source</span></a>.
If, on the other hand, you want to contribute to the development of CUDA-Q itself and hence want to
build a custom version of CUDA-Q from source, follow the instructions on the
<a class="reference external" href="https://github.com/NVIDIA/cuda-quantum/blob/main/Building.md">CUDA-Q GitHub repository</a> instead.</p>
<p>If you are unsure which option suits you best, we recommend using our <a class="reference internal" href="#install-docker-image"><span class="std std-ref">Docker image</span></a> to develop your applications in a controlled environment that does not depend on, or interfere with, other software
that is installed on your system.</p>
<section id="docker">
<span id="install-docker-image"></span><h3>Docker<a class="headerlink" href="#docker" title="Permalink to this heading">¶</a></h3>
<p>To download and use our Docker images, you will need to install and launch the Docker engine.
If you do not already have Docker installed on your system, you can get it by downloading and installing <a class="reference external" href="https://docs.docker.com/get-docker/">Docker Desktop</a>.
If you do not have the necessary administrator permissions to install software on your machine,
take a look at the section below on how to use <a class="reference internal" href="#singularity">Singularity</a> instead.</p>
<p>Docker images for all CUDA-Q releases are available on the <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/quantum/containers/cuda-quantum">NGC Container Registry</a>.
In addition to publishing <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/quantum/containers/cuda-quantum/tags">stable releases</a>,
we also publish Docker images whenever we update certain branches on our <a class="reference external" href="https://github.com/NVIDIA/cuda-quantum">GitHub repository</a>.
These images are published in our <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nightly/containers/cuda-quantum/tags">nightly channel on NGC</a>.
To download the latest version on the main branch of our GitHub repository, for example, use the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker pull nvcr.io/nvidia/nightly/cuda-quantum:latest</span>
</pre></div>
</div>
<p>Early prototypes for features we are considering can be tried out by using the image tags starting
with <code class="code docutils literal notranslate"><span class="pre">experimental</span></code>. The <code class="code docutils literal notranslate"><span class="pre">README</span></code> in the <code class="code docutils literal notranslate"><span class="pre">/home/cudaq</span></code> folder in the container gives more details
about the feature. We welcome and appreciate your feedback about these early prototypes;
how popular they are will help inform whether we should include them in future releases.</p>
<p>Once you have downloaded an image, the container can be run using the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker run -it --name cuda-quantum nvcr.io/nvidia/nightly/cuda-quantum:latest</span>
</pre></div>
</div>
<p>Replace the image name and/or tag in the command above, if necessary, with the one you want to use.
This will give you terminal access to the created container. To enable support
for GPU-accelerated backends, you will need to pass the <code class="code docutils literal notranslate"><span class="pre">--gpus</span></code> flag when launching
the container, for example:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker run -it --gpus all --name cuda-quantum nvcr.io/nvidia/nightly/cuda-quantum:latest</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This command will fail if you do not have a suitable NVIDIA GPU available, or if your driver
version is insufficient. To improve compatibility with older drivers, you may need to install the
<a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">NVIDIA container toolkit</a>.</p>
</div>
<p>You can stop and exit the container by typing the command <code class="code docutils literal notranslate"><span class="pre">exit</span></code>. If you did not specify
<code class="code docutils literal notranslate"><span class="pre">--rm</span></code> flag when launching the container, the container still exists after exiting, as well as any
changes you made in it. You can get back to it using
the command <code class="code docutils literal notranslate"><span class="pre">docker</span> <span class="pre">start</span> <span class="pre">-i</span> <span class="pre">cuda-quantum</span></code>.
You can delete an existing container and any changes you made using <code class="code docutils literal notranslate"><span class="pre">docker</span> <span class="pre">rm</span> <span class="pre">-v</span> <span class="pre">cuda-quantum</span></code>.</p>
<p>When working with Docker images, the files inside the container are not visible outside the container environment.
To facilitate application development with, for example, debugging, code completion, hover information, and so on,
please take a look at the section on <a class="reference internal" href="#docker-in-vscode"><span class="std std-ref">Development with VS Code</span></a>.</p>
<p>Alternatively, it is possible, but not recommended, to launch an SSH server inside the container environment and connect an IDE using SSH. To do so, make sure you have generated a suitable RSA key pair; if your <code class="code docutils literal notranslate"><span class="pre">~/.ssh/</span></code> folder does not already contain the files <code class="code docutils literal notranslate"><span class="pre">id_rsa.pub</span></code> and <code class="code docutils literal notranslate"><span class="pre">id.rsa</span></code>,
follow the instructions for generating a new SSH key on <a class="reference external" href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent">this page</a>.
You can then launch the container and connect to it via SSH by executing the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">docker run -itd --gpus all --name cuda-quantum -p 2222:22 nvcr.io/nvidia/nightly/cuda-quantum:latest</span>
<span class="go">docker exec cuda-quantum bash -c &quot;sudo apt-get install -y --no-install-recommends openssh-server&quot;</span>
<span class="go">docker exec cuda-quantum bash -c &quot;sudo sed -i -E &quot;s/#?\s*UsePAM\s+.+/UsePAM yes/g&quot; /etc/ssh/sshd_config&quot;</span>
<span class="go">docker cp ~/.ssh/id_rsa.pub cuda-quantum:/home/cudaq/.ssh/authorized_keys</span>
<span class="go">docker exec -d cuda-quantum bash -c &quot;sudo -E /usr/sbin/sshd -D&quot;</span>
<span class="go">ssh cudaq@localhost -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o GlobalKnownHostsFile=/dev/null</span>
</pre></div>
</div>
</section>
<section id="singularity">
<span id="install-singularity-image"></span><h3>Singularity<a class="headerlink" href="#singularity" title="Permalink to this heading">¶</a></h3>
<p>You can use <a class="reference external" href="https://github.com/sylabs/singularity">Singularity</a> to run a CUDA-Q container in a folder without needing administrator permissions.
If you do not already have Singularity installed, you can build a relocatable installation from source.
To do so on Linux or WSL, make sure you have the <a class="reference external" href="https://docs.sylabs.io/guides/4.0/user-guide/quick_start.html#prerequisites">necessary prerequisites</a> installed, download a suitable version of the <a class="reference external" href="https://docs.sylabs.io/guides/4.0/user-guide/quick_start.html#install-go">go toolchain</a>, and make sure the <code class="code docutils literal notranslate"><span class="pre">go</span></code> binaries are on your <code class="code docutils literal notranslate"><span class="pre">PATH</span></code>. You can then build Singularity with the commands</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">wget https://github.com/sylabs/singularity/releases/download/v4.0.1/singularity-ce-4.0.1.tar.gz</span>
<span class="go">tar -xzf singularity-ce-4.0.1.tar.gz singularity-ce-4.0.1/ &amp;&amp; rm singularity-ce-4.0.1.tar.gz &amp;&amp; cd singularity-ce-4.0.1/</span>
<span class="go">./mconfig --without-suid --prefix=&quot;$HOME/.local/singularity&quot;</span>
<span class="go">make -C ./builddir &amp;&amp; make -C ./builddir install &amp;&amp; cd .. &amp;&amp; rm -rf singularity-ce-4.0.1/</span>
<span class="go">echo &#39;PATH=&quot;$PATH:$HOME/.local/singularity/bin/&quot;&#39; &gt;&gt; ~/.profile &amp;&amp; source ~/.profile</span>
</pre></div>
</div>
<p>For more information about using Singularity on other systems, take a look at the <a class="reference external" href="https://docs.sylabs.io/guides/4.0/admin-guide/installation.html#installation-on-windows-or-mac">admin guide</a>.</p>
<p>Once you have singularity installed, create a file <code class="code docutils literal notranslate"><span class="pre">cuda-quantum.def</span></code> with the following content:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Bootstrap: docker</span>
<span class="go">From: nvcr.io/nvidia/nightly/cuda-quantum:latest</span>

<span class="gp">%</span>runscript
<span class="go">    mount devpts /dev/pts -t devpts</span>
<span class="go">    cp -r /home/cudaq/* .</span>
<span class="go">    bash</span>
</pre></div>
</div>
<p>Replace the image name and/or tag in the <code class="code docutils literal notranslate"><span class="pre">From</span></code> line, if necessary, with the one you want to use;
In addition to publishing <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/quantum/containers/cuda-quantum/tags">stable releases</a>,
we also publish Docker images whenever we update certain branches on our <a class="reference external" href="https://github.com/NVIDIA/cuda-quantum">GitHub repository</a>.
These images are published in our <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nightly/containers/cuda-quantum/tags">nightly channel on NGC</a>.
Early prototypes for features we are considering can be tried out by using the image tags starting
with <code class="code docutils literal notranslate"><span class="pre">experimental</span></code>. We welcome and appreciate your feedback about these early prototypes;
how popular they are will help inform whether we should include them in future releases.</p>
<p>You can then create a CUDA-Q container by running the following commands:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity build --fakeroot cuda-quantum.sif cuda-quantum.def</span>
<span class="go">singularity run --writable --fakeroot cuda-quantum.sif</span>
</pre></div>
</div>
<p>In addition to the files in your current folder, you should now
see a <code class="code docutils literal notranslate"><span class="pre">README</span></code> file, as well as examples and tutorials.
To enable support for GPU-accelerated backends, you will need to pass the
the <code class="code docutils literal notranslate"><span class="pre">--nv</span></code> flag when running the container:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity run --writable --fakeroot --nv cuda-quantum.sif</span>
<span class="go">nvidia-smi</span>
</pre></div>
</div>
<p>The output of the command above lists the GPUs that are visible and accessible in the container environment.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you do not see any GPUs listed in the output of <code class="code docutils literal notranslate"><span class="pre">nvidia-smi</span></code>,
it means the container environment is unable to access a suitable NVIDIA GPU.
This can happen if your driver version is insufficient, or if you are
working on WSL. To improve compatibility with older drivers, or to enable GPU support
on WSL, please install the <a class="reference external" href="https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html">NVIDIA container toolkit</a>, and update the singularity configuration
to set <code class="code docutils literal notranslate"><span class="pre">use</span> <span class="pre">nvidia-container-cli</span></code> to <code class="code docutils literal notranslate"><span class="pre">yes</span></code> and configure the correct <code class="code docutils literal notranslate"><span class="pre">nvidia-container-cli</span> <span class="pre">path</span></code>.
The two commands below use <code class="code docutils literal notranslate"><span class="pre">sed</span></code> to do that:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sed -i &#39;s/use nvidia-container-cli = no/use nvidia-container-cli = yes/&#39; &quot;$HOME/.local/singularity/etc/singularity/singularity.conf&quot;</span>
<span class="go">sed -i &#39;s/# nvidia-container-cli path =/nvidia-container-cli path = \/usr\/bin\/nvidia-container-cli/&#39; &quot;$HOME/.local/singularity/etc/singularity/singularity.conf&quot;</span>
</pre></div>
</div>
</div>
<p>You can exit the container environment by typing the command <code class="code docutils literal notranslate"><span class="pre">exit</span></code>.
Any changes you made will still be visible after you exit the container, and you can re-enable the
container environment at any time using the <code class="code docutils literal notranslate"><span class="pre">run</span></code> command above.</p>
<p>To facilitate application development with, for example, debugging, code completion, hover information, and so on,
please take a look at the section on <a class="reference internal" href="#singularity-in-vscode"><span class="std std-ref">Development with VS Code</span></a>.</p>
</section>
<section id="python-wheels">
<span id="install-python-wheels"></span><h3>Python wheels<a class="headerlink" href="#python-wheels" title="Permalink to this heading">¶</a></h3>
<p>CUDA-Q Python wheels are available on <a class="reference external" href="https://pypi.org/project/cuda-quantum">PyPI.org</a>.
Installation instructions can be found in the <a class="reference external" href="https://pypi.org/project/cuda-quantum/#description">project description</a>.
For more information about available versions and documentation,
see <a class="reference internal" href="../../releases.html"><span class="doc">CUDA-Q Releases</span></a>.</p>
<p>There are currently no source distributions available on PyPI, but you can download the
source code for the latest version of the CUDA-Q Python wheels from our
<a class="reference external" href="https://github.com/NVIDIA/cuda-quantum">GitHub repository</a>.
The source code for previous versions can be downloaded from the respective
<a class="reference external" href="https://github.com/NVIDIA/cuda-quantum/releases">GitHub Release</a>.</p>
<p>At this time, wheels are distributed for Linux operating systems only.
If your platform is not <a class="reference internal" href="#dependencies-and-compatibility"><span class="std std-ref">officially supported</span></a> and
<code class="code docutils literal notranslate"><span class="pre">pip</span></code> does not find a compatible wheel to install, you can build your own
wheel from source following the instructions here: <a class="reference internal" href="data_center_install.html"><span class="doc">Installation from Source</span></a>.</p>
<p>To build the CUDA-Q Python API for the purpose of contributing to
our <a class="reference external" href="https://github.com/NVIDIA/cuda-quantum">GitHub repository</a>,
follow the instructions for
<a class="reference external" href="https://github.com/NVIDIA/cuda-quantum/blob/main/Dev_Setup.md">Setting up your Environment</a>,
and then run the following commands in the repository root:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">bash scripts/install_prerequisites.sh</span>
<span class="go">pip install . --user</span>
</pre></div>
</div>
</section>
<section id="pre-built-binaries">
<span id="install-prebuilt-binaries"></span><h3>Pre-built binaries<a class="headerlink" href="#pre-built-binaries" title="Permalink to this heading">¶</a></h3>
<p>Starting with the 0.6.0 release, we provide pre-built binaries for using
CUDA-Q with C++. Support for using CUDA-Q with Python can be installed
side-by-side with the pre-built binaries for C++ by following the instructions on
<a class="reference external" href="https://pypi.org/project/cuda-quantum">PyPI.org</a>.
The pre-built binaries work across a range of Linux operating systems listed
under <a class="reference internal" href="#dependencies-and-compatibility"><span class="std std-ref">Dependencies and Compatibility</span></a>.</p>
<p>Before installing our pre-built binaries, please make sure that your
operating system is using the <a class="reference external" href="https://www.gnu.org/software/libc/">GNU C library</a>
version 2.28 or newer. You can confirm this by checking the output of the command
<code class="code docutils literal notranslate"><span class="pre">ldd</span> <span class="pre">--version</span></code>. If this command does not exist, or shows an older version than 2.28,
please double check that your operating system is listed as
<a class="reference internal" href="#dependencies-and-compatibility"><span class="std std-ref">supported</span></a>. If you use an operating system
with an older GNU C library version, you will need to build the installer from
source following the instructions in <a class="reference internal" href="data_center_install.html"><span class="doc">Installation from Source</span></a>.</p>
<p>You can download the <code class="code docutils literal notranslate"><span class="pre">install_cuda_quantum</span></code> file for your processor architecture from
the assets of the respective <a class="reference external" href="https://github.com/NVIDIA/cuda-quantum/releases">GitHub release</a>.
The installer is a <a class="reference external" href="https://makeself.io/">self-extracting archive</a> that contains the
pre-built binaries as well as a script to move them to the correct locations. You will need
<code class="code docutils literal notranslate"><span class="pre">bash</span></code>, <code class="code docutils literal notranslate"><span class="pre">tar</span></code>, and <code class="code docutils literal notranslate"><span class="pre">gzip</span></code> (usually already installed on most Linux distributions) to run
the installer.
The installation location of CUDA-Q is not currently configurable and using the installer
hence requires admin privileges on the system. We may revise that in the future; please see and
upvote the corresponding <a class="reference external" href="https://github.com/NVIDIA/cuda-quantum/issues/1075">GitHub issue</a>.</p>
<p>To install CUDA-Q, execute the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">MPI_PATH</span><span class="o">=</span>/usr/local/openmpi<span class="w"> </span><span class="se">\</span>
sudo<span class="w"> </span>-E<span class="w"> </span>bash<span class="w"> </span>install_cuda_quantum.<span class="k">$(</span>uname<span class="w"> </span>-m<span class="k">)</span><span class="w"> </span>--accept<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span>.<span class="w"> </span>/etc/profile
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To use GPU-accelerated backends, you will need to install the necessary CUDA runtime libraries.
For more information see the corresponding section on
<a class="reference internal" href="#cuda-dependencies-prebuilt-binaries"><span class="std std-ref">Additional CUDA Tools</span></a>.</p>
</div>
<p>The installation ensures that the necessary environment variables for
using the CUDA-Q toolchain are set upon login for all POSIX shells.
Confirm that the <code class="code docutils literal notranslate"><span class="pre">nvq++</span></code> command is found. If it is not, please make sure
to set the environment variables defined by the <code class="code docutils literal notranslate"><span class="pre">set_env.sh</span></code> script in the
CUDA-Q installation folder (usually <code class="code docutils literal notranslate"><span class="pre">/usr/local/cudaq</span></code> or <code class="code docutils literal notranslate"><span class="pre">/opt/nvidia/cudaq</span></code>).</p>
<p>If an MPI installation is available in the directory defined by <code class="code docutils literal notranslate"><span class="pre">MPI_PATH</span></code>,
the installer automatically enables MPI support in CUDA-Q.
If you do not have MPI installed on your system, you can simply
leave that path empty, and CUDA-Q will be installed without MPI support.
If you install MPI at a later point in time, you can activate the MPI support in CUDA
Quantum by setting the <code class="code docutils literal notranslate"><span class="pre">MPI_PATH</span></code> variable to its installation location and
executing the commands</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">MPI_PATH=/usr/local/openmpi # update this path as needed</span>
<span class="go">bash &quot;${CUDA_QUANTUM_PATH}/distributed_interfaces/activate_custom_mpi.sh&quot;</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please make sure that you have the necessary development headers of the C standard
library installed. You can check this by searching for <code class="code docutils literal notranslate"><span class="pre">features.h</span></code>, commonly found
in <code class="code docutils literal notranslate"><span class="pre">/usr/include/</span></code>. You can install the necessary headers via package manager
(usually the package name is called something like <code class="code docutils literal notranslate"><span class="pre">glibc-devel</span></code> or <code class="code docutils literal notranslate"><span class="pre">libc6-dev</span></code>).
These headers are also included with any installation of GCC.</p>
</div>
</section>
</section>
<section id="development-with-vs-code">
<span id="local-development-with-vscode"></span><h2>Development with VS Code<a class="headerlink" href="#development-with-vs-code" title="Permalink to this heading">¶</a></h2>
<p>To facilitate application development with, for example, debugging, code completion, hover information, and so on,
we recommend using <a class="reference external" href="https://code.visualstudio.com/">VS Code</a>. VS Code provides a seamless
development experience on all platforms, and is also available without installation via web browser.
This sections describes how to connect VS Code to a running container on your machine.
The section on <a class="reference internal" href="#connect-to-remote"><span class="std std-ref">connecting to a remote host</span></a> contains information on
how to set up your development environment when accessing CUDA-Q on a remote host instead.</p>
<section id="using-a-docker-container">
<span id="docker-in-vscode"></span><h3>Using a Docker container<a class="headerlink" href="#using-a-docker-container" title="Permalink to this heading">¶</a></h3>
<p>Before connecting VS Code, open a terminal/shell,
and start the CUDA-Q Docker container following the
instructions in the <a class="reference internal" href="#install-docker-image"><span class="std std-ref">section above</span></a>.</p>
<p>If you have a local installation of <a class="reference external" href="https://code.visualstudio.com/">VS Code</a>
you can connect to the running container using the
<a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">Dev Containers</a> extension. If you want to use VS Code in the web browser, please follow the instructions
in the section <a class="reference internal" href="#developing-with-remote-tunnels">Developing with Remote Tunnels</a> instead.</p>
<p>After installing the
<a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers">Dev Containers</a> extension, launch VS Code, open the Command Palette with <code class="code docutils literal notranslate"><span class="pre">Ctrl+Shift+P</span></code>, and enter
”Dev Containers: Attach to Running Container”.
You should see and select the running <code class="code docutils literal notranslate"><span class="pre">cuda-quantum</span></code> container in the list.
After the window reloaded, enter “File: Open Folder” in the Command Palette to open the <code class="code docutils literal notranslate"><span class="pre">/home/cudaq/</span></code> folder.</p>
<p>To run the examples, open the Command Palette and enter “View: Show Terminal”
to launch an integrated terminal. You are now all set to
<a class="reference internal" href="#post-installation"><span class="std std-ref">get started</span></a> with CUDA-Q development.</p>
</section>
<section id="using-a-singularity-container">
<span id="singularity-in-vscode"></span><h3>Using a Singularity container<a class="headerlink" href="#using-a-singularity-container" title="Permalink to this heading">¶</a></h3>
<p>If you have a GitHub or Microsoft account, we recommend that you connect
to a CUDA-Q container using tunnels. To do so, launch a CUDA-Q Singularity
container following the instructions in the <a class="reference internal" href="#install-singularity-image"><span class="std std-ref">section above</span></a>,
and then follow the instructions in the section <a class="reference internal" href="#developing-with-remote-tunnels">Developing with Remote Tunnels</a>.</p>
<p>If you cannot use tunnels, you need a local installation of
<a class="reference external" href="https://code.visualstudio.com/">VS Code</a> and you need to install
the <a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh">Remote - SSH</a> extension.
Make sure you also have a suitable SSH key pair; if your <code class="code docutils literal notranslate"><span class="pre">~/.ssh/</span></code> folder does not already contain
the files <code class="code docutils literal notranslate"><span class="pre">id_rsa.pub</span></code> and <code class="code docutils literal notranslate"><span class="pre">id.rsa</span></code>, follow the instructions for generating a new SSH key on
<a class="reference external" href="https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent">this page</a>.</p>
<p>To connect VS Code to a running CUDA-Q container,
the most convenient setup is to install and run an SSH server
in the Singularity container. Open a terminal/shell in a separate window,
and enter the following commands to create a suitable sandbox:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity build --sandbox cuda-quantum-sandbox cuda-quantum.sif</span>
<span class="go">singularity exec --writable --fakeroot cuda-quantum-sandbox \</span>
<span class="go">  apt-get install -y --no-install-recommends openssh-server</span>
<span class="go">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span>
</pre></div>
</div>
<p>You can launch this sandbox by entering the commands below. Please see the <a class="reference internal" href="#singularity">Singularity</a> section above
for more information about how to get the <code class="code docutils literal notranslate"><span class="pre">cuda-quantum.sif</span></code> image, and how to enable GPU-acceleration
with the <code class="code docutils literal notranslate"><span class="pre">--nv</span></code> flag.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">singularity run --writable --fakeroot --nv --network-args=&quot;portmap=22:2222/tcp&quot; cuda-quantum-sandbox</span>
<span class="go">/usr/sbin/sshd -D -p 2222 -E sshd_output.txt</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Make sure to use a free port. You can check if the SSH server is ready and listening
by looking at the log in <code class="code docutils literal notranslate"><span class="pre">sshd_output.txt</span></code>. If the port is already in use, you can
replace the number <code class="code docutils literal notranslate"><span class="pre">2222</span></code> by any free TCP port in the range <code class="code docutils literal notranslate"><span class="pre">1025-65535</span></code> in all
commands.</p>
</div>
<p>Entering <code class="code docutils literal notranslate"><span class="pre">Ctrl+C</span></code> followed by <code class="code docutils literal notranslate"><span class="pre">exit</span></code> will stop the running container. You can re-start
it at any time by entering the two commands above. While the container is running,
open the Command Palette in VS Code with <code class="code docutils literal notranslate"><span class="pre">Ctrl+Shift+P</span></code>, enter “Remote-SSH: Add new
SSH Host”, and enter the following SSH command:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ssh root@localhost -p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o GlobalKnownHostsFile=/dev/null</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are working on Windows and are building and running the Singularity container in WSL,
make sure to copy the used SSH keys to the Windows partition, such that VS Code can connect with
the expected key. Alternatively, add the used public key to the <code class="code docutils literal notranslate"><span class="pre">/root/.ssh/authorized_keys</span></code> file in
the Singularity container.</p>
</div>
<p>You can then connect to the host by opening the Command Palette, entering
“Remote SSH: Connect Current Window to Host”, and choosing the newly created host.
After the window reloaded, enter “File: Open Folder” in the
Command Palette to open the desired folder.</p>
<p>To run the examples, open the Command Palette and enter “View: Show Terminal”
to launch an integrated terminal. You are now all set to
<a class="reference internal" href="#post-installation"><span class="std std-ref">get started</span></a> with CUDA-Q development.</p>
</section>
</section>
<section id="connecting-to-a-remote-host">
<span id="connect-to-remote"></span><h2>Connecting to a Remote Host<a class="headerlink" href="#connecting-to-a-remote-host" title="Permalink to this heading">¶</a></h2>
<p>Depending on the setup on the remote host, there are a couple of different options
for developing CUDA-Q applications.</p>
<ul class="simple">
<li><p>If a CUDA-Q container is running on the remote host,
and you have a GitHub or Microsoft account, take a look at
<a class="reference internal" href="#developing-with-remote-tunnels">Developing with Remote Tunnels</a>. This works for both Docker
and Singularity containers on the remote host, and should also
work for other containers.</p></li>
<li><p>If you cannot use tunnels, or if you want to work with an
existing CUDA-Q installation without using a container,
take a look at <a class="reference internal" href="#remote-access-via-ssh">Remote Access via SSH</a> instead.</p></li>
</ul>
<section id="developing-with-remote-tunnels">
<span id="connect-vscode-via-tunnel"></span><h3>Developing with Remote Tunnels<a class="headerlink" href="#developing-with-remote-tunnels" title="Permalink to this heading">¶</a></h3>
<p><a class="reference external" href="https://code.visualstudio.com/blogs/2022/12/07/remote-even-better">Remote access via tunnel</a>
can easily be enabled with the <a class="reference external" href="https://code.visualstudio.com/docs/editor/command-line">VS Code CLI</a>.
This allows to connect either a local installation of <a class="reference external" href="https://code.visualstudio.com/">VS Code</a>,
or the <a class="reference external" href="https://vscode.dev/">VS Code Web UI</a>, to a running CUDA-Q container on the same or a different machine.</p>
<p>Creating a secure connection requires authenticating with the same GitHub or Microsoft account on each end.
Once authenticated, an SSH connection over the tunnel provides end-to-end encryption. To download the VS Code CLI, if necessary, and create a tunnel, execute the
following command in the running CUDA-Q container,
and follow the instructions to authenticate:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">vscode-setup tunnel --name cuda-quantum-remote --accept-server-license-terms</span>
</pre></div>
</div>
<p>You can then either <a class="reference external" href="https://vscode.dev/tunnel/cuda-quantum-remote/home/cudaq/">open VS Code in a web browser</a>, or connect a local installation of VS Code.
To connect a local installation of VS Code, make sure you have the
<a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.remote-server">Remote - Tunnels</a> extension installed,
then open the Command Palette with <code class="code docutils literal notranslate"><span class="pre">Ctrl+Shift+P</span></code>, enter “Remote Tunnels: Connect to Tunnel”,
and enter <code class="code docutils literal notranslate"><span class="pre">cuda-quantum-remote</span></code>. After the window reloaded, enter “File: Open Folder” in the Command Palette
to open the <code class="code docutils literal notranslate"><span class="pre">/home/cudaq/</span></code> folder.</p>
<p>You should see a pop up asking if you want to install the recommended extensions. Selecting to install them will
configure VS Code with extensions for working with C++, Python, and Jupyter.
You can always see the list of recommended extensions that aren’t installed yet by clicking on the “Extensions” icon in the sidebar and navigating to the “Recommended” tab.</p>
</section>
<section id="remote-access-via-ssh">
<h3>Remote Access via SSH<a class="headerlink" href="#remote-access-via-ssh" title="Permalink to this heading">¶</a></h3>
<p>To facilitate application development with, for example, debugging, code completion, hover information, and so on,
you can connect a local installation of <a class="reference external" href="https://code.visualstudio.com/">VS Code</a> to a remote host via SSH.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For the best user experience, we recommend to launch a CUDA-Q container on the remote host,
and then connect <a class="reference internal" href="#connect-vscode-via-tunnel"><span class="std std-ref">VS Code using tunnels</span></a>.
If a connection via tunnel is not possible, this section describes using SSH instead.</p>
</div>
<p>To do so, make sure you have <a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-ssh">Remote - SSH</a> extension installed.
Open the Command Palette with <code class="code docutils literal notranslate"><span class="pre">Ctrl+Shift+P</span></code>, enter “Remote-SSH: Add new
SSH Host”, and enter the SSH command to connect to your account on the remote host.
You can then connect to the host by opening the Command Palette, entering
“Remote SSH: Connect Current Window to Host”, and choosing the newly created host.</p>
<p>When prompted, choose Linux as the operating system, and enter your
password. After the window reloaded, enter “File: Open Folder” in the
Command Palette to open the desired folder. Our GitHub repository contains
a folder with VS Code configurations including a list of recommended extensions for
working with CUDA-Q; you can copy <a class="reference external" href="https://github.com/NVIDIA/cuda-quantum/tree/main/docker/release/config/.vscode">these configurations</a> into the a folder named <code class="code docutils literal notranslate"><span class="pre">.vscode</span></code> in your workspace to use them.</p>
<p>If you want to work with an existing CUDA-Q installation on the remote host, you are all set.
Alternatively, you can use Singularity to build and run a container following the instructions in
<a class="reference internal" href="#install-singularity-image"><span class="std std-ref">this section</span></a>. Once the <code class="code docutils literal notranslate"><span class="pre">cuda-quantum.sif</span></code> image is built and
available in your home directory on the remote host, you can update your VS Code configuration
to enable/improve completion, hover information, and other development tools within the container.</p>
<p>To do so, open the Command Palette and enter “Remote-SSH: Open SSH Configuration File”.
Add a new entry to that file with the command to launch the container, and edit the configuration
of the remote host, titled <code class="code docutils literal notranslate"><span class="pre">remote-host</span></code> in the snippets below, to add a new identifier:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">Host cuda-quantum~*</span>
<span class="go">  RemoteCommand singularity run --writable --fakeroot --nv ~/cuda-quantum.sif</span>
<span class="go">  RequestTTY yes</span>

<span class="go">Host remote-host cuda-quantum~remote-host</span>
<span class="go">  HostName ...</span>
<span class="go">  ...</span>
</pre></div>
</div>
<p>You will need to edit a couple of VS Code setting to make use of the newly defined remote command;
open the Command Palette, enter “Preferences: Open User Settings (JSON)”, and add or update the
following configurations:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">&quot;remote.SSH.enableRemoteCommand&quot;: true,</span>
<span class="go">&quot;remote.SSH.useLocalServer&quot;: true,</span>
<span class="go">&quot;remote.SSH.remoteServerListenOnSocket&quot;: false,</span>
<span class="go">&quot;remote.SSH.connectTimeout&quot;: 120,</span>
<span class="go">&quot;remote.SSH.serverInstallPath&quot;: {</span>
<span class="go">    &quot;cuda-quantum~remote-host&quot;: &quot;~/.vscode-container/cuda-quantum&quot;,</span>
<span class="go">},</span>
</pre></div>
</div>
<p>After saving the changes, you should now be able to select <code class="code docutils literal notranslate"><span class="pre">cuda-quantum~remote-host</span></code> as the host
when connecting via SSH, which will launch the CUDA-Q container and connect VS Code to it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the connection to <code class="code docutils literal notranslate"><span class="pre">cuda-quantum~remote-host</span></code> fails, you may need to specify the full
path to the <code class="code docutils literal notranslate"><span class="pre">singularity</span></code> executable on the remote host, since environment variables,
and specifically the configured <code class="code docutils literal notranslate"><span class="pre">PATH</span></code> may be different during launch than in your user account.</p>
</div>
</section>
</section>
<section id="dgx-cloud">
<h2>DGX Cloud<a class="headerlink" href="#dgx-cloud" title="Permalink to this heading">¶</a></h2>
<p>If you are using <a class="reference external" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/">DGX Cloud</a>,
you can easily use it to run CUDA-Q applications.
While submitting jobs to DGX Cloud directly from within CUDA-Q is not (yet) supported,
you can use the NGC CLI to launch and interact with workloads in DGX Cloud.
The following sections detail how to do that, and how to connect JupyterLab and/or VS Code
to a running CUDA-Q job in DGX Cloud.</p>
<section id="get-started">
<span id="dgx-cloud-setup"></span><h3>Get Started<a class="headerlink" href="#get-started" title="Permalink to this heading">¶</a></h3>
<p>To get started with DGX Cloud, you can
<a class="reference external" href="https://www.nvidia.com/en-us/data-center/dgx-cloud/trial/">request access here</a>.
Once you have access, <a class="reference external" href="https://ngc.nvidia.com/signin">sign in</a> to your account,
and <a class="reference external" href="https://ngc.nvidia.com/setup/api-key">generate an API key</a>.
<a class="reference external" href="https://ngc.nvidia.com/setup/installers/cli">Install the NGC CLI</a>
and configure it with</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc config set</span>
</pre></div>
</div>
<p>entering the API key you just generated when prompted, and configure other settings as appropriate.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The rest of this section assumes you have CLI version 3.33.0. If you
have an older version installed, you can upgrade to the latest version using the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc version upgrade 3.33.0</span>
</pre></div>
</div>
<p>See also the <a class="reference external" href="https://docs.ngc.nvidia.com/cli/index.html">NGC CLI documentation</a>
for more information about available commands.</p>
</div>
<p>You can see all information about available compute resources and ace instances
with the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc base-command ace list</span>
</pre></div>
</div>
<p>Confirm that you can submit a job with the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc base-command job run \</span>
<span class="go">  --name Job-001 --total-runtime 60s \</span>
<span class="go">  --image nvcr.io/nvidia/nightly/cuda-quantum:latest --result /results \</span>
<span class="go">  --ace &lt;ace_name&gt; --instance &lt;instance_name&gt; \</span>
<span class="go">  --commandline &#39;echo &quot;Hello from DGX Cloud!&quot;&#39;</span>
</pre></div>
</div>
<p>replacing <code class="code docutils literal notranslate"><span class="pre">&lt;ace_name&gt;</span></code> and <code class="code docutils literal notranslate"><span class="pre">&lt;instance_name&gt;</span></code> with the name of the ace and instance you want
to execute the job on.
You should now see that job listed when you run the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc base-command job list</span>
</pre></div>
</div>
<p>Once it has completed you can download the job results using the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc base-command result download &lt;job_id&gt;</span>
</pre></div>
</div>
<p>replacing <code class="code docutils literal notranslate"><span class="pre">&lt;job_id&gt;</span></code> with the id of the job you just submitted.
You should see a new folder named <code class="code docutils literal notranslate"><span class="pre">&lt;job_id&gt;</span></code> with the job log that contains
the output “Hello from DGX Cloud!”.</p>
<p>For more information about how to use the NGC CLI to interact with DGX Cloud,
we refer to the <a class="reference external" href="https://docs.ngc.nvidia.com/cli/index.html">NGC CLI documentation</a>.</p>
</section>
<section id="use-jupyterlab">
<h3>Use JupyterLab<a class="headerlink" href="#use-jupyterlab" title="Permalink to this heading">¶</a></h3>
<p>Once you can <a class="reference internal" href="#dgx-cloud-setup"><span class="std std-ref">run jobs on DGX Cloud</span></a>, you can launch an interactive job
to use CUDA-Q with <a class="reference external" href="https://jupyterlab.readthedocs.io/en/latest/">JupyterLab</a>
running on DGX Cloud:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc base-command job run \</span>
<span class="go">  --name Job-interactive-001 --total-runtime 600s \</span>
<span class="go">  --image nvcr.io/nvidia/nightly/cuda-quantum:latest --result /results \</span>
<span class="go">  --ace &lt;ace_name&gt; --instance &lt;instance_name&gt; \</span>
<span class="go">  --port 8888 --commandline &#39;jupyter-lab-setup &lt;my-custom-token&gt; --port=8888&#39;</span>
</pre></div>
</div>
<p>Replace <code class="code docutils literal notranslate"><span class="pre">&lt;my-custom-token&gt;</span></code> in the command above with a custom token that you can freely choose.
You will use this token to authenticate with JupyterLab;
Go to the <a class="reference external" href="https://bc.ngc.nvidia.com/jobs">job portal</a>, click on the job you just launched, and click on the link
under ”URL/Hostname” in Service Mapped Ports.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It may take a couple of minutes for DGX Cloud to launch and for the URL to become active, even after it appears in the Service Mapped Ports section;
if you encounter a “404: Not Found” error, be patient and try again in a couple of minutes.</p>
</div>
<p>Once this URL opens, you should see the JupyterLab authentication page; enter the
token you selected above to get access to the running CUDA-Q container.
On the left you should see a folder with tutorials. Happy coding!</p>
</section>
<section id="use-vs-code">
<h3>Use VS Code<a class="headerlink" href="#use-vs-code" title="Permalink to this heading">¶</a></h3>
<p>Once you can <a class="reference internal" href="#dgx-cloud-setup"><span class="std std-ref">run jobs on DGX Cloud</span></a>, you can launch an interactive job
to use CUDA-Q with a local installation of <a class="reference external" href="https://code.visualstudio.com/">VS Code</a>,
or the <a class="reference external" href="https://vscode.dev/">VS Code Web UI</a>, running on DGX Cloud:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">ngc base-command job run \</span>
<span class="go">  --name Job-interactive-001 --total-runtime 600s \</span>
<span class="go">  --image nvcr.io/nvidia/nightly/cuda-quantum:latest --result /results \</span>
<span class="go">  --ace &lt;ace_name&gt; --instance &lt;instance_name&gt; \</span>
<span class="go">  --commandline &#39;vscode-setup tunnel --name cuda-quantum-dgx --accept-server-license-terms&#39;</span>
</pre></div>
</div>
<p>Go to the <a class="reference external" href="https://bc.ngc.nvidia.com/jobs">job portal</a>, click on the job you just launched, and select the “Log”
tab. Once the job is running, you should see instructions there for how to connect to the device the job is running on.
These instructions include a link to open and the code to enter on that page; follow the instructions to authenticate.
Once you have authenticated, you can either
<a class="reference external" href="https://vscode.dev/tunnel/cuda-quantum-dgx/home/cudaq/">open VS Code in a web browser</a>,
or connect a local installation of VS Code.
To connect a local installation of VS Code, make sure you have the
<a class="reference external" href="https://marketplace.visualstudio.com/items?itemName=ms-vscode.remote-server">Remote - Tunnels</a> extension installed,
then open the Command Palette with <code class="code docutils literal notranslate"><span class="pre">Ctrl+Shift+P</span></code>, enter “Remote Tunnels: Connect to Tunnel”,
and enter <code class="code docutils literal notranslate"><span class="pre">cuda-quantum-remote</span></code>. After the window reloaded, enter “File: Open Folder” in the Command Palette
to open the <code class="code docutils literal notranslate"><span class="pre">/home/cudaq/</span></code> folder.</p>
<p>You should see a pop up asking if you want to install the recommended extensions. Selecting to install them will
configure VS Code with extensions for working with C++, Python, and Jupyter.
You can always see the list of recommended extensions that aren’t installed yet by clicking on the “Extensions” icon in the sidebar and navigating to the “Recommended” tab.</p>
<p>If you enter “View: Show Explorer” in the Command Palette, you should see a folder with tutorials and examples
to help you get started. Take a look at <a class="reference internal" href="#next-steps">Next Steps</a> to dive into CUDA-Q development.</p>
</section>
</section>
<section id="additional-cuda-tools">
<span id="id7"></span><h2>Additional CUDA Tools<a class="headerlink" href="#additional-cuda-tools" title="Permalink to this heading">¶</a></h2>
<p>CUDA-Q makes use of GPU-acceleration in certain backends and components.
Depending on how you installed CUDA-Q, you may need to install
certain CUDA libraries separately to take advantage of these.</p>
<section id="installation-via-pypi">
<h3>Installation via PyPI<a class="headerlink" href="#installation-via-pypi" title="Permalink to this heading">¶</a></h3>
<p>If you installed CUDA-Q via <a class="reference external" href="https://pypi.org/project/cuda-quantum">PyPI</a>, please follow the installation instructions there to install the necessary CUDA dependencies.</p>
</section>
<section id="installation-in-container-images">
<h3>Installation In Container Images<a class="headerlink" href="#installation-in-container-images" title="Permalink to this heading">¶</a></h3>
<p>If you are using the CUDA-Q container image, the image already contains all necessary runtime libraries to use all CUDA-Q components. To take advantage of GPU-acceleration, make sure
to enable GPU support when you launch the container, that is pass the <code class="code docutils literal notranslate"><span class="pre">--gpus</span> <span class="pre">all</span></code> flag when launching
the container with Docker and the <code class="code docutils literal notranslate"><span class="pre">--nv</span></code> flag when launching the container with Singularity.</p>
<p>Note that the image does not contain all development dependencies for CUDA, such as, for example the <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code> compiler. You can install all CUDA development dependencies by running the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sudo apt-get install cuda-toolkit-11.8</span>
</pre></div>
</div>
<p>inside the container. Most Python packages that use GPU-acceleration, such as for example <a class="reference external" href="https://cupy.dev">CuPy</a>, require an existing CUDA installation. After installing the <code class="code docutils literal notranslate"><span class="pre">cuda-toolkit-11.8</span></code> you can install CuPy with the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python3 -m pip install cupy-cuda11x</span>
</pre></div>
</div>
</section>
<section id="installing-pre-built-binaries">
<span id="cuda-dependencies-prebuilt-binaries"></span><h3>Installing Pre-built Binaries<a class="headerlink" href="#installing-pre-built-binaries" title="Permalink to this heading">¶</a></h3>
<p>If you installed pre-built binaries for CUDA-Q, you will need to install
the necessary CUDA 11 runtime libraries to use GPU-acceleration in CUDA-Q.
If you prefer to only install the minimal set of runtime libraries, the following
commands, for example, install the necessary packages for RHEL 8:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">CUDA_VERSION</span><span class="o">=</span><span class="m">11</span>.8
<span class="nv">CUDA_DOWNLOAD_URL</span><span class="o">=</span>https://developer.download.nvidia.com/compute/cuda/repos
<span class="c1"># Go to the url above, set the variables below to a suitable distribution</span>
<span class="c1"># and subfolder for your platform, and uncomment the line below.</span>
<span class="c1"># DISTRIBUTION=rhel8 CUDA_ARCH_FOLDER=x86_64</span>

<span class="nv">version_suffix</span><span class="o">=</span><span class="k">$(</span><span class="nb">echo</span><span class="w"> </span><span class="si">${</span><span class="nv">CUDA_VERSION</span><span class="si">}</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>tr<span class="w"> </span>.<span class="w"> </span>-<span class="k">)</span>
dnf<span class="w"> </span>config-manager<span class="w"> </span>--add-repo<span class="w"> </span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">CUDA_DOWNLOAD_URL</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">DISTRIBUTION</span><span class="si">}</span><span class="s2">/</span><span class="si">${</span><span class="nv">CUDA_ARCH_FOLDER</span><span class="si">}</span><span class="s2">/cuda-</span><span class="si">${</span><span class="nv">DISTRIBUTION</span><span class="si">}</span><span class="s2">.repo&quot;</span>
dnf<span class="w"> </span>install<span class="w"> </span>-y<span class="w"> </span>--nobest<span class="w"> </span>--setopt<span class="o">=</span><span class="nv">install_weak_deps</span><span class="o">=</span>False<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>cuda-nvtx-<span class="si">${</span><span class="nv">version_suffix</span><span class="si">}</span><span class="w"> </span>cuda-cudart-<span class="si">${</span><span class="nv">version_suffix</span><span class="si">}</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>libcusolver-<span class="si">${</span><span class="nv">version_suffix</span><span class="si">}</span><span class="w"> </span>libcublas-<span class="si">${</span><span class="nv">version_suffix</span><span class="si">}</span>
</pre></div>
</div>
<p>More detailed instructions for your platform can be found in the online documentation
linked for that <a class="reference external" href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA version</a>.
Please make sure to install CUDA version 11.8, and confirm that your
<a class="reference external" href="https://www.nvidia.com/download/index.aspx">GPU driver</a> supports that version.
While the above packages are sufficient to use GPU-acceleration within CUDA-Q,
we recommend installing the complete CUDA toolkit (<code class="code docutils literal notranslate"><span class="pre">cuda-toolkit-11-8</span></code>) that also
includes the <code class="code docutils literal notranslate"><span class="pre">nvcc</span></code> compiler.</p>
</section>
</section>
<section id="distributed-computing-with-mpi">
<span id="id8"></span><h2>Distributed Computing with MPI<a class="headerlink" href="#distributed-computing-with-mpi" title="Permalink to this heading">¶</a></h2>
<p>CUDA-Q supports the Message Passing Interface (MPI) parallelism via a plugin interface.
It is possible to activate or replace such an MPI plugin without re-installing or re-compiling CUDA-Q.
MPI calls via CUDA-Q API for C++ and Python will be delegated to the currently activated plugin at runtime.</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">Built-in MPI Support</label><div class="tab-content docutils">
<p>The <a class="reference internal" href="#install-docker-image"><span class="std std-ref">CUDA-Q Docker image</span></a> is shipped with a pre-built MPI plugin based on an
optimized OpenMPI installation included in the image. No action is required to use this plugin.
We recommend using this plugin unless the container host has an existing MPI implementation other than OpenMPI.</p>
<p>If you are not using the Docker image, or are using the image on a system that has a
vendor-optimized MPI library pre-installed, please follow the instructions in the “Custom MPI Support” tab
to enable MPI support.</p>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Custom MPI Support</label><div class="tab-content docutils">
<p>If you are not using the Docker image, or are using the image on a system that has a
vendor-optimized MPI library pre-installed, CUDA-Q can be configured to use the local MPI installation by
manually activating a suitable plugin post-installation.
To do so,</p>
<ul>
<li><p>Make sure the environment variable <code class="code docutils literal notranslate"><span class="pre">CUDA_QUANTUM_PATH</span></code> points to the CUDA-Q installation directory.
If you installed CUDA-Q using the <code class="code docutils literal notranslate"><span class="pre">installer</span> <span class="pre">&lt;install-prebuilt-binaries&gt;</span></code>, or if you are using the CUDA-Q
container image, this variable should already be defined. If you installed the CUDA-Q
<code class="code docutils literal notranslate"><span class="pre">Python</span> <span class="pre">wheels</span> <span class="pre">&lt;install-python-wheels&gt;</span></code>, set this variable to the directory listed under “Location” when you run the
command <code class="code docutils literal notranslate"><span class="pre">pip</span> <span class="pre">show</span> <span class="pre">cuda-quantum</span></code>.</p></li>
<li><p>Set the environment variable <code class="code docutils literal notranslate"><span class="pre">MPI_PATH</span></code> to the location of your MPI installation. In particular, <code class="code docutils literal notranslate"><span class="pre">${MPI_PATH}/include</span></code>
is expected to contain the <code class="code docutils literal notranslate"><span class="pre">mpi.h</span></code> header and <code class="code docutils literal notranslate"><span class="pre">${MPI_PATH}/lib64</span></code> or <code class="code docutils literal notranslate"><span class="pre">${MPI_PATH}/lib</span></code> is expected to contain <code class="code docutils literal notranslate"><span class="pre">libmpi.so</span></code>.</p></li>
<li><p>Execute the following command to complete the activation:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">bash $CUDA_QUANTUM_PATH/distributed_interfaces/activate_custom_mpi.sh</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>HPC data centers often have a vendor-optimized MPI library pre-installed on their system.
If you are using our container images, installing that MPI implementation in the container
and manually activating the plugin following the steps above ensure the best performance,
and guarantee compatibility when MPI injection into a container occurs.</p>
</div>
<p>Manually activating an MPI plugin replaces any existing plugin; After the initial activation, the newly built
<code class="code docutils literal notranslate"><span class="pre">libcudaq_distributed_interface_mpi.so</span></code> in the installation directory will subsequently always be used to
handle CUDA-Q MPI calls.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Executing the activation script from the CUDA-Q installation directory requires <em>write</em> permissions to that directory.
If you do not have the necessary permissions, copy the <code class="code docutils literal notranslate"><span class="pre">distributed_interfaces</span></code> sub-directory to a local location and execute the
activation script from there.</p>
<p>In this scenario, since the activated plugin (<code class="code docutils literal notranslate"><span class="pre">libcudaq_distributed_interface_mpi.so</span></code>) is outside the CUDA-Q installation,
you must set the environment variable <code class="code docutils literal notranslate"><span class="pre">$CUDAQ_MPI_COMM_LIB</span></code> to the path of that shared library.
This is done automatically when executing that activation script, but you may wish to persist that environment variable
between bash sessions, e.g., by adding it to the <code class="code docutils literal notranslate"><span class="pre">.bashrc</span></code> file.</p>
</div>
</div>
</div>
</section>
<section id="updating-cuda-q">
<span id="updating-cuda-quantum"></span><h2>Updating CUDA-Q<a class="headerlink" href="#updating-cuda-q" title="Permalink to this heading">¶</a></h2>
<p>If you installed the CUDA-Q Python wheels, you can update to the latest release
by running the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python3 -m pip install --upgrade cuda-quantum</span>
</pre></div>
</div>
<p>If you previously installed the CUDA-Q pre-built binaries, you should first uninstall your
current CUDA-Q installation before installing the new version using the installer.
To uninstall your current CUDA-Q version, run the command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">sudo bash &quot;${CUDA_QUANTUM_PATH}/uninstall.sh&quot; -y</span>
</pre></div>
</div>
<p>The <code class="code docutils literal notranslate"><span class="pre">uninstall.sh</span></code> script is generated during installation, and will remove all files
and folders that were created as part of the installation, whether they were modified
in the meantime or not. It does not remove any additional files that existed prior
to the installation or that you have added to the installation location since then.
You can then download and install the new version of CUDA-Q following the
instructions <a class="reference internal" href="#install-prebuilt-binaries"><span class="std std-ref">above</span></a>.</p>
</section>
<section id="dependencies-and-compatibility">
<span id="id9"></span><h2>Dependencies and Compatibility<a class="headerlink" href="#dependencies-and-compatibility" title="Permalink to this heading">¶</a></h2>
<p>CUDA-Q can be used to compile and run quantum programs on a CPU-only system, but a GPU is highly recommended and necessary to use the GPU-based simulators, see also <a class="reference internal" href="../backends/simulators.html"><span class="doc">CUDA-Q Simulation Backends</span></a>.</p>
<p>The supported CPUs include x86_64 (x86-64-v3 architecture and newer) and ARM64 (ARM v8-A architecture and newer).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some of the components included in the CUDA-Q Python wheels depend on an existing CUDA installation on your system. For more information about installing the CUDA-Q Python wheels, take a look at <a class="reference internal" href="#install-python-wheels"><span class="std std-ref">this section</span></a>.</p>
</div>
<p>The following table summarizes the required components.</p>
<table class="docutils align-default" id="id10">
<caption><span class="caption-text">Supported Systems</span><a class="headerlink" href="#id10" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 38%" />
<col style="width: 63%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>CPU architectures</p></td>
<td><p>x86_64, ARM64</p></td>
</tr>
<tr class="row-even"><td><p>Operating System</p></td>
<td><p>Linux</p></td>
</tr>
<tr class="row-odd"><td><p>Tested Distributions</p></td>
<td><p>CentOS 8; Debian 11, 12; Fedora 38; OpenSUSE/SLED/SLES 15.5; RHEL 8, 9; Rocky 8, 9; Ubuntu 22.04</p></td>
</tr>
<tr class="row-even"><td><p>Python versions</p></td>
<td><p>3.8+</p></td>
</tr>
</tbody>
</table>
<table class="docutils align-default" id="id11">
<caption><span class="caption-text">Requirements for GPU Simulation</span><a class="headerlink" href="#id11" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 38%" />
<col style="width: 63%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>GPU Architectures</p></td>
<td><p>Volta, Turing, Ampere, Ada, Hopper</p></td>
</tr>
<tr class="row-even"><td><p>NVIDIA GPU with Compute Capability</p></td>
<td><p>7.0+</p></td>
</tr>
<tr class="row-odd"><td><p>CUDA</p></td>
<td><p>11.x (Driver 470.57.02+), 12.x (Driver 525.60.13+)</p></td>
</tr>
</tbody>
</table>
<p>Detailed information about supported drivers for different CUDA versions and be found <a class="reference external" href="https://docs.nvidia.com/deploy/cuda-compatibility/">here</a>.</p>
</section>
<section id="next-steps">
<span id="post-installation"></span><h2>Next Steps<a class="headerlink" href="#next-steps" title="Permalink to this heading">¶</a></h2>
<p>You can now compile and/or run the C++ and Python examples using the terminal.
To open a terminal in VS Code, open the Command Palette with <code class="code docutils literal notranslate"><span class="pre">Ctrl+Shift+P</span></code> and
enter “View: Show Terminal”.</p>
<img alt="../../_images/getToWork.png" src="../../_images/getToWork.png" />
<p>The CUDA-Q image contains a folder with examples and tutorials in the <code class="code docutils literal notranslate"><span class="pre">/home/cudaq</span></code> directory.
These examples are provided to get you started with CUDA-Q and understanding
the programming and execution model.
If you are not using a container image, you can find these examples on our
<a class="reference external" href="https://github.com/NVIDIA/cuda-quantum">GitHub repository</a>.</p>
<p>Let’s start by running a simple program to validate your installation.
The samples contain an implementation of a
<a class="reference external" href="https://en.wikipedia.org/wiki/Bernstein%E2%80%93Vazirani_algorithm">Bernstein-Vazirani algorithm</a>.
To run the example, execute the command:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">Python</label><div class="tab-content docutils">
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python examples/python/bernstein_vazirani.py --size 5</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">C++</label><div class="tab-content docutils">
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvq++ examples/cpp/algorithms/bernstein_vazirani.cpp &amp;&amp; ./a.out</span>
</pre></div>
</div>
</div>
</div>
<p>This will execute the program on the <a class="reference internal" href="../backends/simulators.html#id3"><span class="std std-ref">default simulator</span></a>, which will use GPU-acceleration if
a suitable GPU has been detected. To confirm that the GPU acceleration works, you can
increase the size of the secret string, and pass the target as a command line argument:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--2-input--1" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--1">Python</label><div class="tab-content docutils">
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python examples/python/bernstein_vazirani.py --size 25 --target nvidia</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--2-input--2" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--2">C++</label><div class="tab-content docutils">
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvq++ examples/cpp/algorithms/bernstein_vazirani.cpp -DSIZE=25 --target nvidia &amp;&amp; ./a.out</span>
</pre></div>
</div>
</div>
</div>
<p>This program should complete fairly quickly. Depending on the available memory on your GPU,
you can set the size of the secret string to up to 28-32 when running on the <code class="code docutils literal notranslate"><span class="pre">nvidia</span></code> target.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you get an error that the CUDA driver version is insufficient or no GPU has been detected,
check that you have enabled GPU support when launching the container by passing the <code class="code docutils literal notranslate"><span class="pre">--gpus</span> <span class="pre">all</span></code> flag
(for <a class="reference internal" href="#install-docker-image"><span class="std std-ref">Docker</span></a>) or the <code class="code docutils literal notranslate"><span class="pre">--nv</span></code> flag (for <a class="reference internal" href="#install-singularity-image"><span class="std std-ref">Singularity</span></a>).
If you are not running a container, you can execute the command <code class="code docutils literal notranslate"><span class="pre">nvidia-smi</span></code> to confirm your setup;
if the command is unknown or fails, you do not have a GPU or do not have a driver installed. If the command
succeeds, please confirm that your CUDA and driver version matches the
<a class="reference internal" href="#dependencies-and-compatibility"><span class="std std-ref">supported versions</span></a>.</p>
</div>
<p>Let’s compare that to using only your CPU:</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--3-input--1" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--1">Python</label><div class="tab-content docutils">
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">python examples/python/bernstein_vazirani.py --size 25 --target qpp-cpu</span>
</pre></div>
</div>
</div>
<input class="tab-input" id="tab-set--3-input--2" name="tab-set--3" type="radio"><label class="tab-label" for="tab-set--3-input--2">C++</label><div class="tab-content docutils">
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">nvq++ examples/cpp/algorithms/bernstein_vazirani.cpp -DSIZE=25 --target qpp-cpu &amp;&amp; ./a.out</span>
</pre></div>
</div>
</div>
</div>
<p>When you execute this command, the program simply seems to hang; that is because it takes
a long time for the CPU-only backend to simulate 28+ qubits! Cancel the execution with <code class="code docutils literal notranslate"><span class="pre">Ctrl+C</span></code>.</p>
<p>You are now all set to start developing quantum applications using CUDA-Q!
Please proceed to <a class="reference internal" href="../basics/basics.html"><span class="doc">Basics</span></a> for an introduction
to the fundamental features of CUDA-Q.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="install.html" class="btn btn-neutral float-left" title="Installation Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="data_center_install.html" class="btn btn-neutral float-right" title="Installation from Source" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, NVIDIA Corporation &amp; Affiliates.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script>  

  <style>
  a:link, a:visited {
    color: #76b900;
  }

  a:hover {
    color: #8c0;
  }

  .rst-content dl:not(.docutils) dt {
    background: rgba(118, 185, 0, 0.1);
    color: rgba(59,93,0,1);
    border-top: solid 3px rgba(59,93,0,1);
  }
  </style>
  

</body>
</html>