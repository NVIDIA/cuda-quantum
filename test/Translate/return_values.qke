// ========================================================================== //
// Copyright (c) 2022 - 2026 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt -add-dealloc -kernel-execution -canonicalize %s | \
// RUN: cudaq-translate --convert-to=qir | FileCheck %s

// NB: the mangled name map is required for the kernel-execution pass.
// QIR codegen requires the target triple.
module attributes{ quake.mangled_name_map = {
  __nvqpp__mlirgen__test_0 = "test_0",
  __nvqpp__mlirgen__test_1 = "test_1",
  __nvqpp__mlirgen__test_2 = "test_2",
  __nvqpp__mlirgen__test_3 = "test_3",
  __nvqpp__mlirgen__test_4 = "test_4",
  __nvqpp__mlirgen__test_5 = "test_5" },
  llvm.triple = "x86_64-unknown-linux-gnu"} {

func.func private @__nvqpp_vectorCopyCtor(%arg0: !cc.ptr<i8> , %arg1: i64 , %arg2: i64 ) -> !cc.ptr<i8>

// vector<bool> -> struct ptr sret
func.func @__nvqpp__mlirgen__test_0(%arg0: i32) -> !cc.stdvec<i1> {
  %c1_i64 = arith.constant 1 : i64
  %c1 = arith.constant 1 : i64
  %c0 = arith.constant 0 : i64
  %0 = cc.alloca i32
  cc.store %arg0, %0 : !cc.ptr<i32>
  %1 = cc.load %0 : !cc.ptr<i32>
  %2 = arith.extsi %1 : i32 to i64
  %3 = quake.alloca !quake.veq<?>[%2 : i64]
  %4 = quake.veq_size %3 : (!quake.veq<?>) -> i64
  %6 = cc.loop while ((%arg1 = %c0) -> (i64)) {
    %12 = arith.cmpi slt, %arg1, %4 : i64
    cc.condition %12(%arg1 : i64)
  } do {
  ^bb0(%arg1: i64):
    %12 = quake.extract_ref %3[%arg1] : (!quake.veq<?>, i64) -> !quake.ref
    quake.h %12 : (!quake.ref) -> ()
    cc.continue %arg1 : i64
  } step {
  ^bb0(%arg1: i64):
    %12 = arith.addi %arg1, %c1 : i64
    cc.continue %12 : i64
  } {invariant}
  %measOut = quake.mz %3 : (!quake.veq<?>) -> !cc.stdvec<!quake.measure>
  %7 = quake.discriminate %measOut : (!cc.stdvec<!quake.measure>) -> !cc.stdvec<i1>
  %8 = cc.stdvec_data %7 : (!cc.stdvec<i1>) -> !cc.ptr<i8>
  %9 = cc.stdvec_size %7 : (!cc.stdvec<i1>) -> i64
  %10 = call @__nvqpp_vectorCopyCtor(%8, %9, %c1_i64) : (!cc.ptr<i8>, i64, i64) -> !cc.ptr<i8>
  %11 = cc.stdvec_init %10, %9 : (!cc.ptr<i8>, i64) -> !cc.stdvec<i1>
  return %11 : !cc.stdvec<i1>
}

func.func @test_0(%1: !cc.ptr<!cc.struct<{!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>}>> {llvm.sret = !cc.struct<{!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>}>}, %this: !cc.ptr<i8>, %2: i32) {
  return
}

// CHECK-LABEL: define { i1*, i64 } @__nvqpp__mlirgen__test_0(i32 
// CHECK-SAME:                                                    %[[VAL_0:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_1:.*]] = sext i32 %[[VAL_0]] to i64
// CHECK:         %[[VAL_2:.*]] = tail call %[[VAL_3:.*]]* @__quantum__rt__qubit_allocate_array(i64 %[[VAL_1]])
// CHECK:         %[[VAL_4:.*]] = tail call i64 @__quantum__rt__array_get_size_1d(%[[VAL_3]]* %[[VAL_2]])
// CHECK:         %[[VAL_5:.*]] = icmp sgt i64 %[[VAL_4]], 0
// CHECK:         br i1 %[[VAL_5]], label %[[VAL_6:.*]], label %[[VAL_7:.*]]
// CHECK:       ._crit_edge.thread:                               ; preds = %[[VAL_8:.*]]
// CHECK:         %[[VAL_9:.*]] = alloca i8, i64 %[[VAL_4]], align 1
// CHECK:         br label %[[VAL_10:.*]]
// CHECK:       .lr.ph:                                           ; preds = %[[VAL_8]], %[[VAL_6]]
// CHECK:         %[[VAL_11:.*]] = phi i64 [ %[[VAL_12:.*]], %[[VAL_6]] ], [ 0, %[[VAL_8]] ]
// CHECK:         %[[VAL_13:.*]] = tail call %[[VAL_14:.*]]** @__quantum__rt__array_get_element_ptr_1d(%[[VAL_3]]* %[[VAL_2]], i64 %[[VAL_11]])
// CHECK:         %[[VAL_15:.*]] = load %[[VAL_14]]*, %[[VAL_14]]** %[[VAL_13]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_14]]* %[[VAL_15]])
// CHECK:         %[[VAL_12]] = add nuw nsw i64 %[[VAL_11]], 1
// CHECK:         %[[VAL_16:.*]] = icmp eq i64 %[[VAL_12]], %[[VAL_4]]
// CHECK:         br i1 %[[VAL_16]], label %[[VAL_17:.*]], label %[[VAL_6]]
// CHECK:       ._crit_edge:                                      ; preds = %[[VAL_6]]
// CHECK:         %[[VAL_18:.*]] = alloca i8, i64 %[[VAL_4]], align 1
// CHECK:         br i1 %[[VAL_5]], label %[[VAL_19:.*]], label %[[VAL_10]]
// CHECK:       .lr.ph4:                                          ; preds = %[[VAL_17]], %[[VAL_19]]
// CHECK:         %[[VAL_20:.*]] = phi i64 [ %[[VAL_21:.*]], %[[VAL_19]] ], [ 0, %[[VAL_17]] ]
// CHECK:         %[[VAL_22:.*]] = tail call %[[VAL_14]]** @__quantum__rt__array_get_element_ptr_1d(%[[VAL_3]]* %[[VAL_2]], i64 %[[VAL_20]])
// CHECK:         %[[VAL_23:.*]] = load %[[VAL_14]]*, %[[VAL_14]]** %[[VAL_22]], align 8
// CHECK:         %[[VAL_24:.*]] = tail call %[[VAL_25:.*]]* @__quantum__qis__mz(%[[VAL_14]]* %[[VAL_23]])
// CHECK:         %[[VAL_26:.*]] = bitcast %[[VAL_25]]* %[[VAL_24]] to i1*
// CHECK:         %[[VAL_27:.*]] = load i1, i1* %[[VAL_26]], align 1
// CHECK:         %[[VAL_28:.*]] = getelementptr i8, i8* %[[VAL_18]], i64 %[[VAL_20]]
// CHECK:         %[[VAL_29:.*]] = zext i1 %[[VAL_27]] to i8
// CHECK:         store i8 %[[VAL_29]], i8* %[[VAL_28]], align 1
// CHECK:         %[[VAL_21]] = add nuw nsw i64 %[[VAL_20]], 1
// CHECK:         %[[VAL_30:.*]] = icmp eq i64 %[[VAL_21]], %[[VAL_4]]
// CHECK:         br i1 %[[VAL_30]], label %[[VAL_10]], label %[[VAL_19]]
// CHECK:       ._crit_edge5:                                     ; preds = %[[VAL_19]], %[[VAL_7]], %[[VAL_17]]
// CHECK:         %[[VAL_31:.*]] = phi i8* [ %[[VAL_9]], %[[VAL_7]] ], [ %[[VAL_18]], %[[VAL_17]] ], [ %[[VAL_18]], %[[VAL_19]] ]
// CHECK:         %[[VAL_32:.*]] = call i8* @__nvqpp_vectorCopyCtor(i8* nonnull %[[VAL_31]], i64 %[[VAL_4]], i64 1)
// CHECK:         %[[VAL_33:.*]] = bitcast i8* %[[VAL_32]] to i1*
// CHECK:         %[[VAL_34:.*]] = insertvalue { i1*, i64 } undef, i1* %[[VAL_33]], 0
// CHECK:         %[[VAL_35:.*]] = insertvalue { i1*, i64 } %[[VAL_34]], i64 %[[VAL_4]], 1
// CHECK:         call void @__quantum__rt__qubit_release_array(%[[VAL_3]]* %[[VAL_2]])
// CHECK:         ret { i1*, i64 } %[[VAL_35]]
// CHECK:       }

// CHECK-LABEL: define void @test_0({ i8*, i8*, i8* }* sret({ i8*, i8*, i8* }) 
// CHECK-SAME:                                                                 %[[VAL_0:.*]], i8* nocapture readnone
// CHECK-SAME:                                                                 %[[VAL_1:.*]], i32
// CHECK-SAME:                                                                 %[[VAL_2:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_3:.*]] = alloca { i32, { i1*, i64 } }, align 4
// CHECK:         %[[VAL_4:.*]] = bitcast { i32, { i1*, i64 } }* %[[VAL_3]] to i8*
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds { i32, { i1*, i64 } }, { i32, { i1*, i64 } }* %[[VAL_3]], i64 0, i32 0
// CHECK:         store i32 %[[VAL_2]], i32* %[[VAL_5]], align 4
// CHECK:         %[[VAL_6:.*]] = alloca { i8**, i8**, i8** }, align 8
// CHECK:         %[[VAL_7:.*]] = alloca [1 x i8*], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds [1 x i8*], [1 x i8*]* %[[VAL_7]], i64 0, i64 0
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_6]], i64 0, i32 0
// CHECK:         store i8** %[[VAL_8]], i8*** %[[VAL_9]], align 8
// CHECK:         %[[VAL_10:.*]] = ptrtoint [1 x i8*]* %[[VAL_7]] to i64
// CHECK:         %[[VAL_11:.*]] = add i64 %[[VAL_10]], 8
// CHECK:         %[[VAL_12:.*]] = inttoptr i64 %[[VAL_11]] to i8**
// CHECK:         %[[VAL_13:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_6]], i64 0, i32 1
// CHECK:         store i8** %[[VAL_12]], i8*** %[[VAL_13]], align 8
// CHECK:         %[[VAL_14:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_6]], i64 0, i32 2
// CHECK:         store i8** %[[VAL_12]], i8*** %[[VAL_14]], align 8
// CHECK:         %[[VAL_15:.*]] = alloca i32, align 4
// CHECK:         store i32 %[[VAL_2]], i32* %[[VAL_15]], align 4
// CHECK:         %[[VAL_16:.*]] = bitcast [1 x i8*]* %[[VAL_7]] to i32**
// CHECK:         store i32* %[[VAL_15]], i32** %[[VAL_16]], align 8
// CHECK:         %[[VAL_17:.*]] = bitcast { i8**, i8**, i8** }* %[[VAL_6]] to i8*
// CHECK:         %[[VAL_18:.*]] = call { i8*, i64 } @hybridLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_0.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_0.thunk to i8*), i8* nonnull %[[VAL_4]], i64 24, i64 8, i8* nonnull %[[VAL_17]])
// CHECK:         %[[VAL_19:.*]] = extractvalue { i8*, i64 } %[[VAL_18]], 0
// CHECK:         %[[VAL_20:.*]] = icmp eq i8* %[[VAL_19]], null
// CHECK:         %[[VAL_21:.*]] = getelementptr i8, i8* %[[VAL_19]], i64 8
// CHECK:         %[[VAL_22:.*]] = bitcast i8* %[[VAL_21]] to { i1*, i64 }*
// CHECK:         %[[VAL_23:.*]] = getelementptr inbounds { i32, { i1*, i64 } }, { i32, { i1*, i64 } }* %[[VAL_3]], i64 0, i32 1
// CHECK:         %[[VAL_24:.*]] = select i1 %[[VAL_20]], { i1*, i64 }* %[[VAL_23]], { i1*, i64 }* %[[VAL_22]]
// CHECK:         %[[VAL_25:.*]] = bitcast { i1*, i64 }* %[[VAL_24]] to i8**
// CHECK:         %[[VAL_26:.*]] = load i8*, i8** %[[VAL_25]], align 8
// CHECK:         %[[VAL_27:.*]] = getelementptr inbounds { i32, { i1*, i64 } }, { i32, { i1*, i64 } }* %[[VAL_3]], i64 0, i32 1, i32 1
// CHECK:         %[[VAL_28:.*]] = getelementptr i8, i8* %[[VAL_19]], i64 16
// CHECK:         %[[VAL_29:.*]] = bitcast i8* %[[VAL_28]] to i64*
// CHECK:         %[[VAL_30:.*]] = select i1 %[[VAL_20]], i64* %[[VAL_27]], i64* %[[VAL_29]]
// CHECK:         %[[VAL_31:.*]] = load i64, i64* %[[VAL_30]], align 4
// CHECK:         %[[VAL_32:.*]] = bitcast { i8*, i8*, i8* }* %[[VAL_0]] to i8*
// CHECK:         call void @__nvqpp_initializer_list_to_vector_bool(i8* %[[VAL_32]], i8* %[[VAL_26]], i64 %[[VAL_31]])
// CHECK:         call void @free(i8* %[[VAL_19]])
// CHECK:         ret void
// CHECK:       }

// struct{bool, bool} -> i16
func.func @__nvqpp__mlirgen__test_1() -> !cc.struct<{i1, i1}> {
  %qubits = quake.alloca !quake.veq<2>
  %q0 = quake.extract_ref %qubits[0] : (!quake.veq<2>) -> !quake.ref
  %q1 = quake.extract_ref %qubits[1] : (!quake.veq<2>) -> !quake.ref
  quake.h %q0 : (!quake.ref) -> ()
  quake.x [%q0] %q1 : (!quake.ref, !quake.ref) -> ()
  %m0 = quake.mz %q0 : (!quake.ref) -> !quake.measure
  %m1 = quake.mz %q1 : (!quake.ref) -> !quake.measure
  %rv = cc.undef !cc.struct<{i1, i1}>
  %d1 = quake.discriminate %m0 : (!quake.measure) -> i1
  %rv1 = cc.insert_value %rv[0], %d1 : (!cc.struct<{i1, i1}>, i1) -> !cc.struct<{i1, i1}>
  %d2 = quake.discriminate %m1 : (!quake.measure) -> i1
  %rv2 = cc.insert_value %rv1[1], %d2 : (!cc.struct<{i1, i1}>, i1) -> !cc.struct<{i1, i1}>
  return %rv2 : !cc.struct<{i1, i1}>
}

func.func @test_1(%this: !cc.ptr<i8>) -> i16 {
  %0 = cc.undef i16
  return %0 : i16
}


// CHECK-LABEL: define { i1, i1 } @__nvqpp__mlirgen__test_1() local_unnamed_addr {
// CHECK:         %[[VAL_0:.*]] = tail call %[[VAL_1:.*]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK:         %[[VAL_2:.*]] = tail call %[[VAL_3:.*]]** @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 0)
// CHECK:         %[[VAL_4:.*]] = load %[[VAL_3]]*, %[[VAL_3]]** %[[VAL_2]], align 8
// CHECK:         %[[VAL_5:.*]] = tail call %[[VAL_3]]** @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 1)
// CHECK:         %[[VAL_6:.*]] = load %[[VAL_3]]*, %[[VAL_3]]** %[[VAL_5]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_3]]* %[[VAL_4]])
// CHECK:         tail call void (i64, i64, i64, i64, i8*, ...) @generalizedInvokeWithRotationsControlsTargets(i64 0, i64 0, i64 1, i64 1, i8* nonnull bitcast (void (%[[VAL_1]]*, %[[VAL_3]]*)* @__quantum__qis__x__ctl to i8*), %[[VAL_3]]* %[[VAL_4]], %[[VAL_3]]* %[[VAL_6]])
// CHECK:         %[[VAL_7:.*]] = tail call %[[VAL_8:.*]]* @__quantum__qis__mz(%[[VAL_3]]* %[[VAL_4]])
// CHECK:         %[[VAL_9:.*]] = tail call %[[VAL_8]]* @__quantum__qis__mz(%[[VAL_3]]* %[[VAL_6]])
// CHECK:         %[[VAL_10:.*]] = bitcast %[[VAL_8]]* %[[VAL_7]] to i1*
// CHECK:         %[[VAL_11:.*]] = load i1, i1* %[[VAL_10]], align 1
// CHECK:         %[[VAL_12:.*]] = insertvalue { i1, i1 } undef, i1 %[[VAL_11]], 0
// CHECK:         %[[VAL_13:.*]] = bitcast %[[VAL_8]]* %[[VAL_9]] to i1*
// CHECK:         %[[VAL_14:.*]] = load i1, i1* %[[VAL_13]], align 1
// CHECK:         %[[VAL_15:.*]] = insertvalue { i1, i1 } %[[VAL_12]], i1 %[[VAL_14]], 1
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_1]]* %[[VAL_0]])
// CHECK:         ret { i1, i1 } %[[VAL_15]]
// CHECK:       }

// CHECK-LABEL: define i16 @test_1(i8* nocapture readnone 
// CHECK-SAME:        %[[VAL_0:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_1:.*]] = alloca [0 x i8*], align 8
// CHECK:         %[[VAL_2:.*]] = alloca i16
// CHECK:         %[[VAL_3:.*]] = alloca { i8**, i8**, i8** }, align 8
// CHECK:         %[[VAL_4:.*]] = bitcast i16* %[[VAL_2]] to i8*
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds [0 x i8*], [0 x i8*]* %[[VAL_1]], i64 0, i64 0
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_3]], i64 0, i32 0
// CHECK:         store i8** %[[VAL_5]], i8*** %[[VAL_6]], align 8
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_3]], i64 0, i32 1
// CHECK:         store i8** %[[VAL_5]], i8*** %[[VAL_7]], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_3]], i64 0, i32 2
// CHECK:         store i8** %[[VAL_5]], i8*** %[[VAL_8]], align 8
// CHECK:         %[[VAL_9:.*]] = bitcast { i8**, i8**, i8** }* %[[VAL_3]] to i8*
// CHECK:         %[[VAL_10:.*]] = call { i8*, i64 } @hybridLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_1.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_1.thunk to i8*), i8* nonnull %[[VAL_4]], i64 2, i64 0, i8* nonnull %[[VAL_9]])
// CHECK:         %[[VAL_11:.*]] = load i16, i16* %[[VAL_2]]
// CHECK:         ret i16 %[[VAL_11]]
// CHECK:       }

// struct{i16, f32, f64, i64} -> sret ptr
func.func @__nvqpp__mlirgen__test_2() -> !cc.struct<{i16, f32, f64, i64}> {
  %rv = cc.undef !cc.struct<{i16, f32, f64, i64}>
  %c1 = arith.constant 8 : i16
  %rv1 = cc.insert_value %rv[0], %c1 : (!cc.struct<{i16, f32, f64, i64}>, i16) -> !cc.struct<{i16, f32, f64, i64}>
  %c2 = arith.constant 5.4 : f32
  %rv2 = cc.insert_value %rv1[1], %c2 : (!cc.struct<{i16, f32, f64, i64}>, f32) -> !cc.struct<{i16, f32, f64, i64}>
  %c3 = arith.constant 37.83 : f64
  %rv3 = cc.insert_value %rv2[2], %c3 : (!cc.struct<{i16, f32, f64, i64}>, f64) -> !cc.struct<{i16, f32, f64, i64}>
  %c4 = arith.constant 1479 : i64
  %rv4 = cc.insert_value %rv3[3], %c4 : (!cc.struct<{i16, f32, f64, i64}>, i64) -> !cc.struct<{i16, f32, f64, i64}>
  return %rv4 : !cc.struct<{i16, f32, f64, i64}>
}

func.func @test_2(%1: !cc.ptr<!cc.struct<{i16, f32, f64, i64}>> {llvm.sret = !cc.struct<{i16, f32, f64, i64}>}, %this: !cc.ptr<i8>) {
  return
}

// CHECK-LABEL: define { i16, float, double, i64 } @__nvqpp__mlirgen__test_2() local_unnamed_addr {{.*}} {
// CHECK:         ret { i16, float, double, i64 } { i16 8, float 0x40159999A0000000, double 3.783000e+01, i64 1479 }
// CHECK:       }

// CHECK-LABEL: define void @test_2({ i16, float, double, i64 }* nocapture writeonly sret({ i16, float, double, i64 }) 
// CHECK-SAME:      %[[VAL_0:.*]], i8* nocapture readnone
// CHECK-SAME:      %[[VAL_1:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_2:.*]] = alloca [0 x i8*], align 8
// CHECK:         %[[VAL_3:.*]] = alloca [24 x i8], align 1
// CHECK:         %[[VAL_4:.*]] = alloca { i8**, i8**, i8** }, align 8
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds [24 x i8], [24 x i8]* %[[VAL_3]], i64 0, i64 0
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds [0 x i8*], [0 x i8*]* %[[VAL_2]], i64 0, i64 0
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 0
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_7]], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 1
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_8]], align 8
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 2
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_9]], align 8
// CHECK:         %[[VAL_10:.*]] = bitcast { i8**, i8**, i8** }* %[[VAL_4]] to i8*
// CHECK:         %[[VAL_11:.*]] = call { i8*, i64 } @hybridLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_2.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_2.thunk to i8*), i8* nonnull %[[VAL_5]], i64 24, i64 0, i8* nonnull %[[VAL_10]])
// CHECK:         %[[VAL_12:.*]] = bitcast { i16, float, double, i64 }* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(24) %[[VAL_12]], i8* noundef nonnull align 1 dereferenceable(24) %[[VAL_5]], i64 24, i1 false)
// CHECK:         ret void
// CHECK:       }


// array<T x n> -> sret ptr
func.func @__nvqpp__mlirgen__test_3() -> !cc.array<i64 x 5> {
  %rv = cc.undef !cc.array<i64 x 5>
  %c1 = arith.constant 5 : i64
  %rv1 = cc.insert_value %rv[0], %c1 : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c2 = arith.constant 74 : i64
  %rv2 = cc.insert_value %rv1[1], %c2 : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c3 = arith.constant 299 : i64
  %rv3 = cc.insert_value %rv2[2], %c3 : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c4 = arith.constant 1659 : i64
  %rv4 = cc.insert_value %rv3[3], %c4 : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c5 = arith.constant 61234 : i64
  %rv5 = cc.insert_value %rv4[4], %c5 : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  return %rv5 : !cc.array<i64 x 5>
}

func.func @test_3(%1: !cc.ptr<!cc.array<i64 x 5>> {llvm.sret = !cc.array<i64 x 5>}, %this: !cc.ptr<i8>) {
  return
}

// CHECK-LABEL: define [5 x i64] @__nvqpp__mlirgen__test_3() local_unnamed_addr {{.*}} {
// CHECK:         ret [5 x i64] [i64 5, i64 74, i64 299, i64 1659, i64 61234]
// CHECK:       }

// CHECK-LABEL: define void @test_3([5 x i64]* nocapture writeonly sret([5 x i64]) 
// CHECK-SAME:                                                                     %[[VAL_0:.*]], i8* nocapture readnone
// CHECK-SAME:                                                                     %[[VAL_1:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_2:.*]] = alloca [0 x i8*], align 8
// CHECK:         %[[VAL_3:.*]] = alloca [40 x i8], align 1
// CHECK:         %[[VAL_4:.*]] = alloca { i8**, i8**, i8** }, align 8
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds [40 x i8], [40 x i8]* %[[VAL_3]], i64 0, i64 0
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds [0 x i8*], [0 x i8*]* %[[VAL_2]], i64 0, i64 0
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 0
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_7]], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 1
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_8]], align 8
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 2
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_9]], align 8
// CHECK:         %[[VAL_10:.*]] = bitcast { i8**, i8**, i8** }* %[[VAL_4]] to i8*
// CHECK:         %[[VAL_11:.*]] = call { i8*, i64 } @hybridLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_3.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_3.thunk to i8*), i8* nonnull %[[VAL_5]], i64 40, i64 0, i8* nonnull %[[VAL_10]])
// CHECK:         %[[VAL_12:.*]] = bitcast [5 x i64]* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(40) %[[VAL_12]], i8* noundef nonnull align 1 dereferenceable(40) %[[VAL_5]], i64 40, i1 false)
// CHECK:         ret void
// CHECK:       }

// small struct (<= 128) -> { i64, f64 }
func.func @__nvqpp__mlirgen__test_4() -> (i64, f64) {
  %c1 = arith.constant 537892 : i64
  %c2 = arith.constant 94.2134 : f64
  return %c1, %c2 : i64, f64
}

func.func @test_4(%sret: !cc.ptr<!cc.struct<{i64, f64}>> {llvm.sret = !cc.struct<{i64, f64}>}, %this: !cc.ptr<i8>) {
  return
}

// CHECK-LABEL: define { i64, double } @__nvqpp__mlirgen__test_4() local_unnamed_addr {{.*}} {
// CHECK:         ret { i64, double } { i64 537892, double 0x40578DA858793DD9 }
// CHECK:       }

// CHECK-LABEL: define void @test_4({ i64, double }* nocapture writeonly sret({ i64, double }) 
// CHECK-SAME:     %[[VAL_0:.*]], i8* nocapture readnone
// CHECK-SAME:     %[[VAL_1:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_2:.*]] = alloca [0 x i8*], align 8
// CHECK:         %[[VAL_3:.*]] = alloca [16 x i8], align 1
// CHECK:         %[[VAL_4:.*]] = alloca { i8**, i8**, i8** }, align 8
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds [16 x i8], [16 x i8]* %[[VAL_3]], i64 0, i64 0
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds [0 x i8*], [0 x i8*]* %[[VAL_2]], i64 0, i64 0
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 0
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_7]], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 1
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_8]], align 8
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_4]], i64 0, i32 2
// CHECK:         store i8** %[[VAL_6]], i8*** %[[VAL_9]], align 8
// CHECK:         %[[VAL_10:.*]] = bitcast { i8**, i8**, i8** }* %[[VAL_4]] to i8*
// CHECK:         %[[VAL_11:.*]] = call { i8*, i64 } @hybridLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_4.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_4.thunk to i8*), i8* nonnull %[[VAL_5]], i64 16, i64 0, i8* nonnull %[[VAL_10]])
// CHECK:         %[[VAL_12:.*]] = bitcast { i64, double }* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(16) %[[VAL_12]], i8* noundef nonnull align 1 dereferenceable(16) %[[VAL_5]], i64 16, i1 false)
// CHECK:         ret void
// CHECK:       }

func.func @__nvqpp__mlirgen__test_5() -> (i64, f64) attributes {no_this} {
  %c1 = arith.constant 537892 : i64
  %c2 = arith.constant 94.2134 : f64
  return %c1, %c2 : i64, f64
}

func.func @test_5(%sret: !cc.ptr<!cc.struct<{i64, f64}>> {llvm.sret = !cc.struct<{i64, f64}>}) {
  return
}

// CHECK-LABEL: define { i64, double } @__nvqpp__mlirgen__test_5() local_unnamed_addr {{.*}} {
// CHECK:         ret { i64, double } { i64 537892, double 0x40578DA858793DD9 }
// CHECK:       }

// CHECK-LABEL: define void @test_5({ i64, double }* nocapture writeonly sret({ i64, double }) 
// CHECK-SAME:                                                                                 %[[VAL_0:.*]]) local_unnamed_addr {
// CHECK:         %[[VAL_1:.*]] = alloca [0 x i8*], align 8
// CHECK:         %[[VAL_2:.*]] = alloca [16 x i8], align 1
// CHECK:         %[[VAL_3:.*]] = alloca { i8**, i8**, i8** }, align 8
// CHECK:         %[[VAL_4:.*]] = getelementptr inbounds [16 x i8], [16 x i8]* %[[VAL_2]], i64 0, i64 0
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds [0 x i8*], [0 x i8*]* %[[VAL_1]], i64 0, i64 0
// CHECK:         %[[VAL_6:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_3]], i64 0, i32 0
// CHECK:         store i8** %[[VAL_5]], i8*** %[[VAL_6]], align 8
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_3]], i64 0, i32 1
// CHECK:         store i8** %[[VAL_5]], i8*** %[[VAL_7]], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr inbounds { i8**, i8**, i8** }, { i8**, i8**, i8** }* %[[VAL_3]], i64 0, i32 2
// CHECK:         store i8** %[[VAL_5]], i8*** %[[VAL_8]], align 8
// CHECK:         %[[VAL_9:.*]] = bitcast { i8**, i8**, i8** }* %[[VAL_3]] to i8*
// CHECK:         %[[VAL_10:.*]] = call { i8*, i64 } @hybridLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_5.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_5.thunk to i8*), i8* nonnull %[[VAL_4]], i64 16, i64 0, i8* nonnull %[[VAL_9]])
// CHECK:         %[[VAL_11:.*]] = bitcast { i64, double }* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(16) %[[VAL_11]], i8* noundef nonnull align 1 dereferenceable(16) %[[VAL_4]], i64 16, i1 false)
// CHECK:         ret void
// CHECK:       }

}
//===----------------------------------------------------------------------===//

// CHECK-LABEL: define i64 @test_0.returnOffset() local_unnamed_addr {{.*}} {
// CHECK:         ret i64 8
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_0.thunk(i8* nocapture
// CHECK-SAME:       %[[VAL_0:.*]], i1 %[[VAL_1:.*]]) {
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i32*
// CHECK:         %[[VAL_3:.*]] = load i32, i32* %[[VAL_2]], align 4
// CHECK:         %[[VAL_4:.*]] = tail call { i1*, i64 } @__nvqpp__mlirgen__test_0(i32 %[[VAL_3]])
// CHECK:         tail call void @__nvqpp_cleanup_arrays()
// CHECK:         %[[VAL_5:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to i1**
// CHECK:         %[[VAL_7:.*]] = extractvalue { i1*, i64 } %[[VAL_4]], 0
// CHECK:         store i1* %[[VAL_7]], i1** %[[VAL_6]], align 8
// CHECK:         %[[VAL_8:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 16
// CHECK:         %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to i64*
// CHECK:         %[[VAL_10:.*]] = extractvalue { i1*, i64 } %[[VAL_4]], 1
// CHECK:         store i64 %[[VAL_10]], i64* %[[VAL_9]], align 8
// CHECK:         br i1 %[[VAL_1]], label %[[VAL_11:.*]], label %[[VAL_12:.*]]
// CHECK:       common.ret:                                       ; preds = %[[VAL_13:.*]], %[[VAL_11]]
// CHECK:         %[[VAL_14:.*]] = phi { i8*, i64 } [ %[[VAL_15:.*]], %[[VAL_11]] ], [ zeroinitializer, %[[VAL_13]] ]
// CHECK:         ret { i8*, i64 } %[[VAL_14]]
// CHECK:       8:                                                ; preds = %[[VAL_13]]
// CHECK:         %[[VAL_16:.*]] = bitcast i1* %[[VAL_7]] to i8*
// CHECK:         %[[VAL_17:.*]] = add i64 %[[VAL_10]], 24
// CHECK:         %[[VAL_18:.*]] = tail call i8* @malloc(i64 %[[VAL_17]])
// CHECK:         tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 1 dereferenceable(24) %[[VAL_18]], i8* noundef nonnull align 1 dereferenceable(24) %[[VAL_0]], i64 24, i1 false)
// CHECK:         %[[VAL_19:.*]] = getelementptr i8, i8* %[[VAL_18]], i64 24
// CHECK:         tail call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %[[VAL_19]], i8* align 1 %[[VAL_16]], i64 %[[VAL_10]], i1 false)
// CHECK:         %[[VAL_20:.*]] = insertvalue { i8*, i64 } undef, i8* %[[VAL_18]], 0
// CHECK:         %[[VAL_15]] = insertvalue { i8*, i64 } %[[VAL_20]], i64 %[[VAL_17]], 1
// CHECK:         %[[VAL_21:.*]] = getelementptr i8, i8* %[[VAL_18]], i64 8
// CHECK:         %[[VAL_22:.*]] = bitcast i8* %[[VAL_21]] to i8**
// CHECK:         store i8* %[[VAL_19]], i8** %[[VAL_22]], align 8
// CHECK:         br label %[[VAL_12]]
// CHECK:       }

// CHECK-LABEL: define i64 @test_0.argsCreator(i8** nocapture readonly 
// CHECK-SAME:                                                         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:                                                         %[[VAL_1:.*]]) {{.*}} {
// CHECK:         %[[VAL_2:.*]] = bitcast i8** %[[VAL_0]] to i32**
// CHECK:         %[[VAL_3:.*]] = load i32*, i32** %[[VAL_2]], align 8
// CHECK:         %[[VAL_4:.*]] = load i32, i32* %[[VAL_3]], align 4
// CHECK:         %[[VAL_5:.*]] = tail call dereferenceable_or_null(24) i8* @malloc(i64 24)
// CHECK:         %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to i32*
// CHECK:         store i32 %[[VAL_4]], i32* %[[VAL_6]], align 4
// CHECK:         store i8* %[[VAL_5]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 24
// CHECK:       }

// CHECK-LABEL: define void @test_0.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_0.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_0.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_0.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_1.returnOffset() local_unnamed_addr {{.*}} {
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_1.thunk(i8* nocapture writeonly
// CHECK-SAME:    %[[VAL_0:.*]], i1 %[[VAL_1:.*]]) {
// CHECK:         %[[VAL_2:.*]] = tail call %[[VAL_3:.*]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK:         %[[VAL_4:.*]] = tail call %[[VAL_5:.*]]** @__quantum__rt__array_get_element_ptr_1d(%[[VAL_3]]* %[[VAL_2]], i64 0)
// CHECK:         %[[VAL_6:.*]] = load %[[VAL_5]]*, %[[VAL_5]]** %[[VAL_4]], align 8
// CHECK:         %[[VAL_7:.*]] = tail call %[[VAL_5]]** @__quantum__rt__array_get_element_ptr_1d(%[[VAL_3]]* %[[VAL_2]], i64 1)
// CHECK:         %[[VAL_8:.*]] = load %[[VAL_5]]*, %[[VAL_5]]** %[[VAL_7]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_5]]* %[[VAL_6]])
// CHECK:         tail call void (i64, i64, i64, i64, i8*, ...) @generalizedInvokeWithRotationsControlsTargets(i64 0, i64 0, i64 1, i64 1, i8* nonnull bitcast (void (%[[VAL_3]]*, %[[VAL_5]]*)* @__quantum__qis__x__ctl to i8*), %[[VAL_5]]* %[[VAL_6]], %[[VAL_5]]* %[[VAL_8]])
// CHECK:         %[[VAL_9:.*]] = tail call %[[VAL_10:.*]]* @__quantum__qis__mz(%[[VAL_5]]* %[[VAL_6]])
// CHECK:         %[[VAL_11:.*]] = tail call %[[VAL_10]]* @__quantum__qis__mz(%[[VAL_5]]* %[[VAL_8]])
// CHECK:         %[[VAL_12:.*]] = bitcast %[[VAL_10]]* %[[VAL_9]] to i1*
// CHECK:         %[[VAL_13:.*]] = load i1, i1* %[[VAL_12]], align 1
// CHECK:         %[[VAL_14:.*]] = bitcast %[[VAL_10]]* %[[VAL_11]] to i1*
// CHECK:         %[[VAL_15:.*]] = load i1, i1* %[[VAL_14]], align 1
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_3]]* %[[VAL_2]])
// CHECK:         %[[VAL_16:.*]] = bitcast i8* %[[VAL_0]] to i1*
// CHECK:         store i1 %[[VAL_13]], i1* %[[VAL_16]], align 1
// CHECK:         %[[VAL_17:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 1
// CHECK:         %[[VAL_18:.*]] = bitcast i8* %[[VAL_17]] to i1*
// CHECK:         store i1 %[[VAL_15]], i1* %[[VAL_18]], align 1
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_1.argsCreator(i8** nocapture readnone 
// CHECK-SAME:                                                         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:                                                         %[[VAL_1:.*]]) {{.*}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(2) i8* @malloc(i64 2)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 2
// CHECK:       }

// CHECK-LABEL: define void @test_1.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_1.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_1.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_1.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_2.returnOffset() local_unnamed_addr {{.*}} {
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_2.thunk(i8* nocapture writeonly 
// CHECK-SAME:                                                            %[[VAL_0:.*]], i1
// CHECK-SAME:                                                            %[[VAL_1:.*]]) {
// CHECK:         tail call void @__nvqpp_cleanup_arrays()
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to { i16, float, double, i64 }*
// CHECK:         store { i16, float, double, i64 } { i16 8, float 0x40159999A0000000, double 3.783000e+01, i64 1479 }, { i16, float, double, i64 }* %[[VAL_2]], align 8
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_2.argsCreator(i8** nocapture readnone 
// CHECK-SAME:                                                         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:                                                         %[[VAL_1:.*]]) {{.*}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(24) i8* @malloc(i64 24)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 24
// CHECK:       }

// CHECK-LABEL: define void @test_2.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_2.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_2.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_2.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_3.returnOffset() local_unnamed_addr {{.*}} {
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_3.thunk(i8* nocapture writeonly 
// CHECK-SAME:                                                            %[[VAL_0:.*]], i1
// CHECK-SAME:                                                            %[[VAL_1:.*]]) {
// CHECK:         tail call void @__nvqpp_cleanup_arrays()
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i64*
// CHECK:         store i64 5, i64* %[[VAL_2]], align 4
// CHECK:         %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to i64*
// CHECK:         store i64 74, i64* %[[VAL_4]], align 4
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 16
// CHECK:         %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to i64*
// CHECK:         store i64 299, i64* %[[VAL_6]], align 4
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 24
// CHECK:         %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to i64*
// CHECK:         store i64 1659, i64* %[[VAL_8]], align 4
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 32
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to i64*
// CHECK:         store i64 61234, i64* %[[VAL_10]], align 4
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_3.argsCreator(i8** nocapture readnone 
// CHECK-SAME:                                                         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:                                                         %[[VAL_1:.*]]) {{.*}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(40) i8* @malloc(i64 40)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 40
// CHECK:       }

// CHECK-LABEL: define void @test_3.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_3.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_3.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_3.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_4.returnOffset() local_unnamed_addr {{.*}} {
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_4.thunk(i8* nocapture writeonly 
// CHECK-SAME:                                                            %[[VAL_0:.*]], i1
// CHECK-SAME:                                                            %[[VAL_1:.*]]) {
// CHECK:         tail call void @__nvqpp_cleanup_arrays()
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i64*
// CHECK:         store i64 537892, i64* %[[VAL_2]], align 4
// CHECK:         %[[VAL_3:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to double*
// CHECK:         store double 0x40578DA858793DD9, double* %[[VAL_4]], align 8
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_4.argsCreator(i8** nocapture readnone 
// CHECK-SAME:                                                         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:                                                         %[[VAL_1:.*]]) {{.*}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(16) i8* @malloc(i64 16)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 16
// CHECK:       }

// CHECK-LABEL: define void @test_4.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_4.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_4.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_4.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_5.returnOffset() local_unnamed_addr {{.*}} {
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_5.thunk(i8* nocapture writeonly 
// CHECK-SAME:                                                            %[[VAL_0:.*]], i1
// CHECK-SAME:                                                            %[[VAL_1:.*]]) {
// CHECK:         tail call void @__nvqpp_cleanup_arrays()
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i64*
// CHECK:         store i64 537892, i64* %[[VAL_2]], align 4
// CHECK:         %[[VAL_3:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to double*
// CHECK:         store double 0x40578DA858793DD9, double* %[[VAL_4]], align 8
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_5.argsCreator(i8** nocapture readnone 
// CHECK-SAME:                                                         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:                                                         %[[VAL_1:.*]]) {{.*}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(16) i8* @malloc(i64 16)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 16
// CHECK:       }

// CHECK-LABEL: define void @test_5.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_5.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_5.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_5.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }
