// ========================================================================== //
// Copyright (c) 2022 - 2026 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt -kernel-execution=codegen=1 %s | FileCheck --check-prefix=ALT %s
// RUN: cudaq-opt -kernel-execution=codegen=2 %s | FileCheck --check-prefix=STREAMLINED %s
// RUN: cudaq-opt -kernel-execution %s | FileCheck --check-prefix=HYBRID %s

module attributes {quake.mangled_name_map = {
  __nvqpp__mlirgen__ghz = "_ZN3ghzclEi"}} {

  func.func @__nvqpp__mlirgen__ghz(%arg0: i32) -> f64 {
    %0 = cc.alloca i32
    cc.store %arg0, %0 : !cc.ptr<i32>
    %1 = cc.load %0 : !cc.ptr<i32>
    %2 = arith.extsi %1 : i32 to i64
    %3 = quake.alloca !quake.veq<?>[%2 : i64]
    %c0_i32 = arith.constant 0 : i32
    %4 = arith.extsi %c0_i32 : i32 to i64
    %5 = quake.extract_ref %3[%4] : (!quake.veq<?>,i64) -> !quake.ref
    quake.h %5 : (!quake.ref) -> ()
    cc.scope {
      %7 = cc.alloca i32
      cc.store %c0_i32, %7 : !cc.ptr<i32>
      %8 = cc.load %7 : !cc.ptr<i32>
      %9 = cc.load %0 : !cc.ptr<i32>
      %c1_i32 = arith.constant 1 : i32
      %10 = arith.subi %9, %c1_i32 : i32
      %11 = arith.index_cast %10 : i32 to index
      %12 = cc.load %7 : !cc.ptr<i32>
      %13 = arith.index_cast %12 : i32 to index
      cc.loop while ((%arg1 = %13) -> index) {
        %cond = arith.cmpi slt, %arg1, %11 : index
	cc.condition %cond (%arg1 : index)
      } do {
      ^bb1 (%arg11 : index):
        %18 = arith.index_cast %arg11 : index to i64
        %19 = quake.extract_ref %3[%18] : (!quake.veq<?>,i64) -> !quake.ref
        %20 = arith.trunci %18 : i64 to i32
        %21 = arith.addi %20, %c1_i32 : i32
        %22 = arith.extsi %21 : i32 to i64
        %23 = quake.extract_ref %3[%22] : (!quake.veq<?>,i64) -> !quake.ref
        quake.x [%19] %23 : (!quake.ref, !quake.ref) -> ()
	cc.continue %arg11 : index
      } step {
      ^bb2 (%arg21 : index):
        %c1_index = arith.constant 1 : index
        %incr = arith.addi %arg21, %c1_index : index
        cc.continue %incr : index
      }
    }
    %15 = quake.veq_size %3 : (!quake.veq<?>) -> i64
    %16 = arith.index_cast %15 : i64 to index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cc.loop while ((%argx1 = %c0) -> index) {
        %cond = arith.cmpi slt, %argx1, %16 : index
	cc.condition %cond (%argx1 : index)
    } do {
    ^bb1 (%arg31 : index):
      %18 = quake.extract_ref %3[%arg31] : (!quake.veq<?>,index) -> !quake.ref
      %19 = quake.mz %18 : (!quake.ref) -> !quake.measure
      cc.continue %arg31 : index
    } step {
    ^bb1 (%arg32 : index):
      %c1_index = arith.constant 1 : index
      %incr.2 = arith.addi %arg32, %c1_index : index
      cc.continue %incr.2 : index
    }
    %cst = arith.constant 1.000000e+00 : f64
    return %cst : f64
  }

  func.func @_ZN3ghzclEi(%0: !cc.ptr<i8>, %1: i32) -> f64 {
    %2 = cc.undef f64
    return %2 : f64
  }
}

// ALT-LABEL:   func.func @_ZN3ghzclEi(
// ALT-SAME:                           %[[VAL_0:.*]]: !cc.ptr<i8>,
// ALT-SAME:                           %[[VAL_1:.*]]: i32) -> f64 {
// ALT:           %[[VAL_2:.*]] = cc.alloca i64
// ALT:           %[[VAL_3:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// ALT:           %[[VAL_4:.*]] = cc.alloca i8{{\[}}%[[VAL_3]] : i64]
// ALT:           %[[VAL_5:.*]] = cc.cast %[[VAL_4]] : (!cc.ptr<!cc.array<i8 x ?>>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// ALT:           %[[VAL_6:.*]] = cc.compute_ptr %[[VAL_5]][0] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i32>
// ALT:           cc.store %[[VAL_1]], %[[VAL_6]] : !cc.ptr<i32>
// ALT:           %[[VAL_7:.*]] = constant @ghz.thunk : (!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_8:.*]] = cc.func_ptr %[[VAL_7]] : ((!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>) -> !cc.ptr<i8>
// ALT:           %[[VAL_9:.*]] = cc.cast %[[VAL_5]] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i8>
// ALT:           %[[VAL_10:.*]] = cc.offsetof !cc.struct<{i32, f64}> [1] : i64
// ALT:           %[[VAL_11:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// ALT:           %[[VAL_12:.*]] = cc.cast %[[VAL_11]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// ALT:           %[[VAL_13:.*]] = call @altLaunchKernel(%[[VAL_12]], %[[VAL_8]], %[[VAL_9]], %[[VAL_3]], %[[VAL_10]]) : (!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_14:.*]] = cc.extract_value %[[VAL_13]][0] : (!cc.struct<{!cc.ptr<i8>, i64}>) -> !cc.ptr<i8>
// ALT:           %[[VAL_15:.*]] = cc.cast %[[VAL_14]] : (!cc.ptr<i8>) -> i64
// ALT:           %[[VAL_16:.*]] = arith.constant 0 : i64
// ALT:           %[[VAL_17:.*]] = arith.cmpi ne, %[[VAL_15]], %[[VAL_16]] : i64
// ALT:           cf.cond_br %[[VAL_17]], ^bb1, ^bb2
// ALT:         ^bb1:
// ALT:           %[[VAL_18:.*]] = cc.cast %[[VAL_14]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// ALT:           %[[VAL_19:.*]] = cc.compute_ptr %[[VAL_18]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// ALT:           cf.br ^bb3(%[[VAL_19]] : !cc.ptr<f64>)
// ALT:         ^bb2:
// ALT:           %[[VAL_20:.*]] = cc.compute_ptr %[[VAL_5]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// ALT:           cf.br ^bb3(%[[VAL_20]] : !cc.ptr<f64>)
// ALT:         ^bb3(%[[VAL_21:.*]]: !cc.ptr<f64>):
// ALT:           %[[VAL_22:.*]] = cc.compute_ptr %[[VAL_5]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// ALT:           %[[VAL_23:.*]] = cc.load %[[VAL_22]] : !cc.ptr<f64>
// ALT:           return %[[VAL_23]] : f64
// ALT:         }
// ALT:         func.func private @altLaunchKernel(!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:         func.func private @cudaqRegisterArgsCreator(!cc.ptr<i8>, !cc.ptr<i8>)
// ALT:         llvm.func @cudaqRegisterLambdaName(!llvm.ptr<i8>, !llvm.ptr<i8>) attributes {sym_visibility = "private"}
// ALT:         func.func private @__cudaq_registerLinkableKernel(!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>)
// ALT:         func.func private @__cudaq_getLinkableKernelKey(!cc.ptr<i8>) -> i64
// ALT:         func.func private @cudaqRegisterKernelName(!cc.ptr<i8>)
// ALT:         func.func private @malloc(i64) -> !cc.ptr<i8>
// ALT:         func.func private @free(!cc.ptr<i8>)
// ALT:         func.func private @__nvqpp_initializer_list_to_vector_bool(!cc.ptr<none>, !cc.ptr<none>, i64)
// ALT:         func.func private @__nvqpp_vector_bool_to_initializer_list(!cc.ptr<!cc.struct<{!cc.ptr<i1>, !cc.ptr<i1>, !cc.ptr<i1>}>>, !cc.ptr<!cc.struct<{!cc.ptr<i1>, !cc.array<i8 x 32>}>>, !cc.ptr<!cc.ptr<i8>>)
// ALT:         func.func private @llvm.memcpy.p0i8.p0i8.i64(!cc.ptr<i8>, !cc.ptr<i8>, i64, i1)

// ALT-LABEL:   func.func private @__nvqpp_zeroDynamicResult() -> !cc.struct<{!cc.ptr<i8>, i64}> {
// ALT:           %[[VAL_0:.*]] = arith.constant 0 : i64
// ALT:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (i64) -> !cc.ptr<i8>
// ALT:           %[[VAL_2:.*]] = cc.undef !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_3:.*]] = cc.insert_value %[[VAL_2]][0], %[[VAL_1]] : (!cc.struct<{!cc.ptr<i8>, i64}>, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_4:.*]] = cc.insert_value %[[VAL_3]][1], %[[VAL_0]] : (!cc.struct<{!cc.ptr<i8>, i64}>, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           return %[[VAL_4]] : !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:         }

// ALT-LABEL:   func.func private @__nvqpp_createDynamicResult(
// ALT-SAME:                                                   %[[VAL_0:.*]]: !cc.ptr<i8>,
// ALT-SAME:                                                   %[[VAL_1:.*]]: i64,
// ALT-SAME:                                                   %[[VAL_2:.*]]: !cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>,
// ALT-SAME:                                                   %[[VAL_3:.*]]: i64) -> !cc.struct<{!cc.ptr<i8>, i64}> {
// ALT:           %[[VAL_4:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>) -> !cc.ptr<i64>
// ALT:           %[[VAL_5:.*]] = cc.load %[[VAL_4]] : !cc.ptr<i64>
// ALT:           %[[VAL_6:.*]] = arith.addi %[[VAL_1]], %[[VAL_5]] : i64
// ALT:           %[[VAL_7:.*]] = call @malloc(%[[VAL_6]]) : (i64) -> !cc.ptr<i8>
// ALT:           %[[VAL_8:.*]] = cc.cast %[[VAL_7]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.array<i8 x ?>>
// ALT:           %[[VAL_9:.*]] = arith.constant false
// ALT:           call @llvm.memcpy.p0i8.p0i8.i64(%[[VAL_7]], %[[VAL_0]], %[[VAL_1]], %[[VAL_9]]) : (!cc.ptr<i8>, !cc.ptr<i8>, i64, i1) -> ()
// ALT:           %[[VAL_10:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>) -> !cc.ptr<!cc.ptr<i8>>
// ALT:           %[[VAL_11:.*]] = cc.load %[[VAL_10]] : !cc.ptr<!cc.ptr<i8>>
// ALT:           %[[VAL_12:.*]] = cc.compute_ptr %[[VAL_8]]{{\[}}%[[VAL_1]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// ALT:           call @llvm.memcpy.p0i8.p0i8.i64(%[[VAL_12]], %[[VAL_11]], %[[VAL_5]], %[[VAL_9]]) : (!cc.ptr<i8>, !cc.ptr<i8>, i64, i1) -> ()
// ALT:           %[[VAL_13:.*]] = cc.undef !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_14:.*]] = cc.insert_value %[[VAL_13]][0], %[[VAL_7]] : (!cc.struct<{!cc.ptr<i8>, i64}>, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_15:.*]] = cc.insert_value %[[VAL_14]][1], %[[VAL_6]] : (!cc.struct<{!cc.ptr<i8>, i64}>, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           %[[VAL_16:.*]] = cc.compute_ptr %[[VAL_8]]{{\[}}%[[VAL_3]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// ALT:           %[[VAL_17:.*]] = cc.cast %[[VAL_16]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.ptr<i8>>
// ALT:           cc.store %[[VAL_12]], %[[VAL_17]] : !cc.ptr<!cc.ptr<i8>>
// ALT:           return %[[VAL_15]] : !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:         }
// ALT:         llvm.mlir.global external constant @ghz.kernelName("ghz\00") {addr_space = 0 : i32}

// ALT-LABEL:   func.func @ghz.returnOffset() -> i64 {
// ALT:           %[[VAL_0:.*]] = cc.offsetof !cc.struct<{i32, f64}> [1] : i64
// ALT:           return %[[VAL_0]] : i64
// ALT:         }

// ALT-LABEL:   func.func @ghz.thunk(
// ALT-SAME:                         %[[VAL_0:.*]]: !cc.ptr<i8>,
// ALT-SAME:                         %[[VAL_1:.*]]: i1) -> !cc.struct<{!cc.ptr<i8>, i64}> {
// ALT:           %[[VAL_2:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// ALT:           %[[VAL_3:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// ALT:           %[[VAL_4:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.array<i8 x ?>>
// ALT:           %[[VAL_5:.*]] = cc.compute_ptr %[[VAL_4]]{{\[}}%[[VAL_3]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// ALT:           %[[VAL_6:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i32>
// ALT:           %[[VAL_7:.*]] = cc.load %[[VAL_6]] : !cc.ptr<i32>
// ALT:           %[[VAL_8:.*]] = cc.noinline_call @__nvqpp__mlirgen__ghz(%[[VAL_7]]) : (i32) -> f64
// ALT:           %[[VAL_9:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// ALT:           cc.store %[[VAL_8]], %[[VAL_9]] : !cc.ptr<f64>
// ALT:           %[[VAL_10:.*]] = call @__nvqpp_zeroDynamicResult() : () -> !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:           return %[[VAL_10]] : !cc.struct<{!cc.ptr<i8>, i64}>
// ALT:         }

// ALT-LABEL:   func.func @ghz.argsCreator(
// ALT-SAME:                               %[[VAL_0:.*]]: !cc.ptr<!cc.ptr<i8>>,
// ALT-SAME:                               %[[VAL_1:.*]]: !cc.ptr<!cc.ptr<i8>>) -> i64 {
// ALT:           %[[VAL_2:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<!cc.ptr<i8>>) -> !cc.ptr<!cc.array<!cc.ptr<i8> x ?>>
// ALT:           %[[VAL_3:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x ?>>) -> !cc.ptr<!cc.ptr<i8>>
// ALT:           %[[VAL_4:.*]] = cc.load %[[VAL_3]] : !cc.ptr<!cc.ptr<i8>>
// ALT:           %[[VAL_5:.*]] = cc.cast %[[VAL_4]] : (!cc.ptr<i8>) -> !cc.ptr<i32>
// ALT:           %[[VAL_6:.*]] = cc.load %[[VAL_5]] : !cc.ptr<i32>
// ALT:           %[[VAL_7:.*]] = cc.alloca i64
// ALT:           %[[VAL_8:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// ALT:           %[[VAL_9:.*]] = call @malloc(%[[VAL_8]]) : (i64) -> !cc.ptr<i8>
// ALT:           %[[VAL_10:.*]] = cc.cast %[[VAL_9]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// ALT:           %[[VAL_11:.*]] = cc.compute_ptr %[[VAL_10]][0] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i32>
// ALT:           cc.store %[[VAL_6]], %[[VAL_11]] : !cc.ptr<i32>
// ALT:           cc.store %[[VAL_9]], %[[VAL_1]] : !cc.ptr<!cc.ptr<i8>>
// ALT:           return %[[VAL_8]] : i64
// ALT:         }

// ALT-LABEL:   llvm.func @ghz.kernelRegFunc() {
// ALT:           %[[VAL_0:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// ALT:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// ALT:           func.call @cudaqRegisterKernelName(%[[VAL_1]]) : (!cc.ptr<i8>) -> ()
// ALT:           %[[VAL_2:.*]] = func.constant @ghz.argsCreator : (!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>) -> i64
// ALT:           %[[VAL_3:.*]] = cc.func_ptr %[[VAL_2]] : ((!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>) -> i64) -> !cc.ptr<i8>
// ALT:           func.call @cudaqRegisterArgsCreator(%[[VAL_1]], %[[VAL_3]]) : (!cc.ptr<i8>, !cc.ptr<i8>) -> ()
// ALT:           llvm.return
// ALT:         }
// ALT:         llvm.mlir.global_ctors {ctors = [@ghz.kernelRegFunc], priorities = [17 : i32]}

// STREAMLINED-LABEL:   func.func @_ZN3ghzclEi(
// STREAMLINED-SAME:                           %[[VAL_0:.*]]: !cc.ptr<i8>,
// STREAMLINED-SAME:                           %[[VAL_1:.*]]: i32) -> f64 {
// STREAMLINED:           %[[VAL_2:.*]] = cc.alloca i64
// STREAMLINED:           %[[VAL_3:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// STREAMLINED:           %[[VAL_4:.*]] = cc.alloca !cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>
// STREAMLINED:           %[[VAL_5:.*]] = cc.alloca !cc.array<!cc.ptr<i8> x 1>
// STREAMLINED:           %[[VAL_6:.*]] = cc.sizeof !cc.array<!cc.ptr<i8> x 1> : i64
// STREAMLINED:           %[[VAL_7:.*]] = cc.cast %[[VAL_5]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           %[[VAL_8:.*]] = cc.cast %[[VAL_4]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAMLINED:           cc.store %[[VAL_7]], %[[VAL_8]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAMLINED:           %[[VAL_9:.*]] = cc.cast %[[VAL_5]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> i64
// STREAMLINED:           %[[VAL_10:.*]] = arith.addi %[[VAL_9]], %[[VAL_6]] : i64
// STREAMLINED:           %[[VAL_11:.*]] = cc.cast %[[VAL_10]] : (i64) -> !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           %[[VAL_12:.*]] = cc.compute_ptr %[[VAL_4]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAMLINED:           cc.store %[[VAL_11]], %[[VAL_12]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAMLINED:           %[[VAL_13:.*]] = cc.compute_ptr %[[VAL_4]][2] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAMLINED:           cc.store %[[VAL_11]], %[[VAL_13]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAMLINED:           %[[VAL_14:.*]] = cc.compute_ptr %[[VAL_5]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           %[[VAL_15:.*]] = cc.alloca i32
// STREAMLINED:           cc.store %[[VAL_1]], %[[VAL_15]] : !cc.ptr<i32>
// STREAMLINED:           %[[VAL_16:.*]] = cc.cast %[[VAL_15]] : (!cc.ptr<i32>) -> !cc.ptr<i8>
// STREAMLINED:           cc.store %[[VAL_16]], %[[VAL_14]] : !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           %[[VAL_17:.*]] = cc.cast %[[VAL_4]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<i8>
// STREAMLINED:           %[[VAL_18:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// STREAMLINED:           %[[VAL_19:.*]] = cc.cast %[[VAL_18]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// STREAMLINED:           call @streamlinedLaunchKernel(%[[VAL_19]], %[[VAL_17]]) : (!cc.ptr<i8>, !cc.ptr<i8>) -> ()
// STREAMLINED:           %[[VAL_20:.*]] = cc.undef f64
// STREAMLINED:           return %[[VAL_20]] : f64
// STREAMLINED:         }
// STREAMLINED:         func.func private @streamlinedLaunchKernel(!cc.ptr<i8>, !cc.ptr<i8>)
// STREAMLINED:         func.func private @cudaqRegisterArgsCreator(!cc.ptr<i8>, !cc.ptr<i8>)
// STREAMLINED:         llvm.func @cudaqRegisterLambdaName(!llvm.ptr<i8>, !llvm.ptr<i8>) attributes {sym_visibility = "private"}
// STREAMLINED:         func.func private @__cudaq_registerLinkableKernel(!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>)
// STREAMLINED:         func.func private @__cudaq_getLinkableKernelKey(!cc.ptr<i8>) -> i64
// STREAMLINED:         func.func private @cudaqRegisterKernelName(!cc.ptr<i8>)
// STREAMLINED:         func.func private @malloc(i64) -> !cc.ptr<i8>
// STREAMLINED:         func.func private @free(!cc.ptr<i8>)
// STREAMLINED:         func.func private @__nvqpp_initializer_list_to_vector_bool(!cc.ptr<none>, !cc.ptr<none>, i64)
// STREAMLINED:         func.func private @__nvqpp_vector_bool_to_initializer_list(!cc.ptr<!cc.struct<{!cc.ptr<i1>, !cc.ptr<i1>, !cc.ptr<i1>}>>, !cc.ptr<!cc.struct<{!cc.ptr<i1>, !cc.array<i8 x 32>}>>, !cc.ptr<!cc.ptr<i8>>)
// STREAMLINED:         func.func private @llvm.memcpy.p0i8.p0i8.i64(!cc.ptr<i8>, !cc.ptr<i8>, i64, i1)

// STREAMLINED-LABEL:   func.func private @__nvqpp_zeroDynamicResult() -> !cc.struct<{!cc.ptr<i8>, i64}> {
// STREAMLINED:           %[[VAL_0:.*]] = arith.constant 0 : i64
// STREAMLINED:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (i64) -> !cc.ptr<i8>
// STREAMLINED:           %[[VAL_2:.*]] = cc.undef !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:           %[[VAL_3:.*]] = cc.insert_value %[[VAL_2]][0], %[[VAL_1]] : (!cc.struct<{!cc.ptr<i8>, i64}>, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:           %[[VAL_4:.*]] = cc.insert_value %[[VAL_3]][1], %[[VAL_0]] : (!cc.struct<{!cc.ptr<i8>, i64}>, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:           return %[[VAL_4]] : !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:         }

// STREAMLINED-LABEL:   func.func private @__nvqpp_createDynamicResult(
// STREAMLINED-SAME:                                                   %[[VAL_0:.*]]: !cc.ptr<i8>,
// STREAMLINED-SAME:                                                   %[[VAL_1:.*]]: i64,
// STREAMLINED-SAME:                                                   %[[VAL_2:.*]]: !cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>,
// STREAMLINED-SAME:                                                   %[[VAL_3:.*]]: i64) -> !cc.struct<{!cc.ptr<i8>, i64}> {
// STREAMLINED:           %[[VAL_4:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>) -> !cc.ptr<i64>
// STREAMLINED:           %[[VAL_5:.*]] = cc.load %[[VAL_4]] : !cc.ptr<i64>
// STREAMLINED:           %[[VAL_6:.*]] = arith.addi %[[VAL_1]], %[[VAL_5]] : i64
// STREAMLINED:           %[[VAL_7:.*]] = call @malloc(%[[VAL_6]]) : (i64) -> !cc.ptr<i8>
// STREAMLINED:           %[[VAL_8:.*]] = cc.cast %[[VAL_7]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.array<i8 x ?>>
// STREAMLINED:           %[[VAL_9:.*]] = arith.constant false
// STREAMLINED:           call @llvm.memcpy.p0i8.p0i8.i64(%[[VAL_7]], %[[VAL_0]], %[[VAL_1]], %[[VAL_9]]) : (!cc.ptr<i8>, !cc.ptr<i8>, i64, i1) -> ()
// STREAMLINED:           %[[VAL_10:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>) -> !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           %[[VAL_11:.*]] = cc.load %[[VAL_10]] : !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           %[[VAL_12:.*]] = cc.compute_ptr %[[VAL_8]]{{\[}}%[[VAL_1]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// STREAMLINED:           call @llvm.memcpy.p0i8.p0i8.i64(%[[VAL_12]], %[[VAL_11]], %[[VAL_5]], %[[VAL_9]]) : (!cc.ptr<i8>, !cc.ptr<i8>, i64, i1) -> ()
// STREAMLINED:           %[[VAL_13:.*]] = cc.undef !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:           %[[VAL_14:.*]] = cc.insert_value %[[VAL_13]][0], %[[VAL_7]] : (!cc.struct<{!cc.ptr<i8>, i64}>, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:           %[[VAL_15:.*]] = cc.insert_value %[[VAL_14]][1], %[[VAL_6]] : (!cc.struct<{!cc.ptr<i8>, i64}>, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:           %[[VAL_16:.*]] = cc.compute_ptr %[[VAL_8]]{{\[}}%[[VAL_3]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// STREAMLINED:           %[[VAL_17:.*]] = cc.cast %[[VAL_16]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           cc.store %[[VAL_12]], %[[VAL_17]] : !cc.ptr<!cc.ptr<i8>>
// STREAMLINED:           return %[[VAL_15]] : !cc.struct<{!cc.ptr<i8>, i64}>
// STREAMLINED:         }
// STREAMLINED:         llvm.mlir.global external constant @ghz.kernelName("ghz\00") {addr_space = 0 : i32}

// STREAMLINED-LABEL:   llvm.func @ghz.kernelRegFunc() {
// STREAMLINED:           %[[VAL_0:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// STREAMLINED:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// STREAMLINED:           func.call @cudaqRegisterKernelName(%[[VAL_1]]) : (!cc.ptr<i8>) -> ()
// STREAMLINED:           llvm.return
// STREAMLINED:         }
// STREAMLINED:         llvm.mlir.global_ctors {ctors = [@ghz.kernelRegFunc], priorities = [17 : i32]}



// HYBRID-LABEL:   func.func @_ZN3ghzclEi(
// HYBRID-SAME:                           %[[VAL_0:.*]]: !cc.ptr<i8>,
// HYBRID-SAME:                           %[[VAL_1:.*]]: i32) -> f64 {
// HYBRID:           %[[VAL_2:.*]] = cc.alloca i64
// HYBRID:           %[[VAL_3:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// HYBRID:           %[[VAL_4:.*]] = cc.alloca i8{{\[}}%[[VAL_3]] : i64]
// HYBRID:           %[[VAL_5:.*]] = cc.cast %[[VAL_4]] : (!cc.ptr<!cc.array<i8 x ?>>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// HYBRID:           %[[VAL_6:.*]] = cc.compute_ptr %[[VAL_5]][0] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i32>
// HYBRID:           cc.store %[[VAL_1]], %[[VAL_6]] : !cc.ptr<i32>
// HYBRID:           %[[VAL_7:.*]] = constant @ghz.thunk : (!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_8:.*]] = cc.func_ptr %[[VAL_7]] : ((!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_9:.*]] = cc.cast %[[VAL_5]] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_10:.*]] = cc.offsetof !cc.struct<{i32, f64}> [1] : i64
// HYBRID:           %[[VAL_11:.*]] = cc.alloca !cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>
// HYBRID:           %[[VAL_12:.*]] = cc.alloca !cc.array<!cc.ptr<i8> x 1>
// HYBRID:           %[[VAL_13:.*]] = cc.sizeof !cc.array<!cc.ptr<i8> x 1> : i64
// HYBRID:           %[[VAL_14:.*]] = cc.cast %[[VAL_12]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_15:.*]] = cc.cast %[[VAL_11]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           cc.store %[[VAL_14]], %[[VAL_15]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           %[[VAL_16:.*]] = cc.cast %[[VAL_12]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> i64
// HYBRID:           %[[VAL_17:.*]] = arith.addi %[[VAL_16]], %[[VAL_13]] : i64
// HYBRID:           %[[VAL_18:.*]] = cc.cast %[[VAL_17]] : (i64) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_19:.*]] = cc.compute_ptr %[[VAL_11]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           cc.store %[[VAL_18]], %[[VAL_19]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           %[[VAL_20:.*]] = cc.compute_ptr %[[VAL_11]][2] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           cc.store %[[VAL_18]], %[[VAL_20]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           %[[VAL_21:.*]] = cc.compute_ptr %[[VAL_12]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_22:.*]] = cc.alloca i32
// HYBRID:           cc.store %[[VAL_1]], %[[VAL_22]] : !cc.ptr<i32>
// HYBRID:           %[[VAL_23:.*]] = cc.cast %[[VAL_22]] : (!cc.ptr<i32>) -> !cc.ptr<i8>
// HYBRID:           cc.store %[[VAL_23]], %[[VAL_21]] : !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_24:.*]] = cc.cast %[[VAL_11]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_25:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// HYBRID:           %[[VAL_26:.*]] = cc.cast %[[VAL_25]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_27:.*]] = call @hybridLaunchKernel(%[[VAL_26]], %[[VAL_8]], %[[VAL_9]], %[[VAL_3]], %[[VAL_10]], %[[VAL_24]]) : (!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_28:.*]] = cc.extract_value %[[VAL_27]][0] : (!cc.struct<{!cc.ptr<i8>, i64}>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_29:.*]] = cc.cast %[[VAL_28]] : (!cc.ptr<i8>) -> i64
// HYBRID:           %[[VAL_30:.*]] = arith.constant 0 : i64
// HYBRID:           %[[VAL_31:.*]] = arith.cmpi ne, %[[VAL_29]], %[[VAL_30]] : i64
// HYBRID:           cf.cond_br %[[VAL_31]], ^bb1, ^bb2
// HYBRID:         ^bb1:
// HYBRID:           %[[VAL_32:.*]] = cc.cast %[[VAL_28]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// HYBRID:           %[[VAL_33:.*]] = cc.compute_ptr %[[VAL_32]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// HYBRID:           cf.br ^bb3(%[[VAL_33]] : !cc.ptr<f64>)
// HYBRID:         ^bb2:
// HYBRID:           %[[VAL_34:.*]] = cc.compute_ptr %[[VAL_5]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// HYBRID:           cf.br ^bb3(%[[VAL_34]] : !cc.ptr<f64>)
// HYBRID:         ^bb3(%[[VAL_35:.*]]: !cc.ptr<f64>):
// HYBRID:           %[[VAL_36:.*]] = cc.compute_ptr %[[VAL_5]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// HYBRID:           %[[VAL_37:.*]] = cc.load %[[VAL_36]] : !cc.ptr<f64>
// HYBRID:           return %[[VAL_37]] : f64
// HYBRID:         }
// HYBRID:         func.func private @hybridLaunchKernel(!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:         func.func private @cudaqRegisterArgsCreator(!cc.ptr<i8>, !cc.ptr<i8>)
// HYBRID:         llvm.func @cudaqRegisterLambdaName(!llvm.ptr<i8>, !llvm.ptr<i8>) attributes {sym_visibility = "private"}
// HYBRID:         func.func private @__cudaq_registerLinkableKernel(!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>)
// HYBRID:         func.func private @__cudaq_getLinkableKernelKey(!cc.ptr<i8>) -> i64
// HYBRID:         func.func private @cudaqRegisterKernelName(!cc.ptr<i8>)
// HYBRID:         func.func private @malloc(i64) -> !cc.ptr<i8>
// HYBRID:         func.func private @free(!cc.ptr<i8>)
// HYBRID:         func.func private @__nvqpp_initializer_list_to_vector_bool(!cc.ptr<none>, !cc.ptr<none>, i64)
// HYBRID:         func.func private @__nvqpp_vector_bool_to_initializer_list(!cc.ptr<!cc.struct<{!cc.ptr<i1>, !cc.ptr<i1>, !cc.ptr<i1>}>>, !cc.ptr<!cc.struct<{!cc.ptr<i1>, !cc.array<i8 x 32>}>>, !cc.ptr<!cc.ptr<i8>>)
// HYBRID:         func.func private @llvm.memcpy.p0i8.p0i8.i64(!cc.ptr<i8>, !cc.ptr<i8>, i64, i1)

// HYBRID-LABEL:   func.func private @__nvqpp_zeroDynamicResult() -> !cc.struct<{!cc.ptr<i8>, i64}> {
// HYBRID:           %[[VAL_0:.*]] = arith.constant 0 : i64
// HYBRID:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (i64) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_2:.*]] = cc.undef !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_3:.*]] = cc.insert_value %[[VAL_2]][0], %[[VAL_1]] : (!cc.struct<{!cc.ptr<i8>, i64}>, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_4:.*]] = cc.insert_value %[[VAL_3]][1], %[[VAL_0]] : (!cc.struct<{!cc.ptr<i8>, i64}>, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           return %[[VAL_4]] : !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:         }

// HYBRID-LABEL:   func.func private @__nvqpp_createDynamicResult(
// HYBRID-SAME:                                                   %[[VAL_0:.*]]: !cc.ptr<i8>,
// HYBRID-SAME:                                                   %[[VAL_1:.*]]: i64,
// HYBRID-SAME:                                                   %[[VAL_2:.*]]: !cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>,
// HYBRID-SAME:                                                   %[[VAL_3:.*]]: i64) -> !cc.struct<{!cc.ptr<i8>, i64}> {
// HYBRID:           %[[VAL_4:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>) -> !cc.ptr<i64>
// HYBRID:           %[[VAL_5:.*]] = cc.load %[[VAL_4]] : !cc.ptr<i64>
// HYBRID:           %[[VAL_6:.*]] = arith.addi %[[VAL_1]], %[[VAL_5]] : i64
// HYBRID:           %[[VAL_7:.*]] = call @malloc(%[[VAL_6]]) : (i64) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_8:.*]] = cc.cast %[[VAL_7]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.array<i8 x ?>>
// HYBRID:           %[[VAL_9:.*]] = arith.constant false
// HYBRID:           call @llvm.memcpy.p0i8.p0i8.i64(%[[VAL_7]], %[[VAL_0]], %[[VAL_1]], %[[VAL_9]]) : (!cc.ptr<i8>, !cc.ptr<i8>, i64, i1) -> ()
// HYBRID:           %[[VAL_10:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.struct<{!cc.ptr<i8>, i64}>>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_11:.*]] = cc.load %[[VAL_10]] : !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_12:.*]] = cc.compute_ptr %[[VAL_8]]{{\[}}%[[VAL_1]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// HYBRID:           call @llvm.memcpy.p0i8.p0i8.i64(%[[VAL_12]], %[[VAL_11]], %[[VAL_5]], %[[VAL_9]]) : (!cc.ptr<i8>, !cc.ptr<i8>, i64, i1) -> ()
// HYBRID:           %[[VAL_13:.*]] = cc.undef !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_14:.*]] = cc.insert_value %[[VAL_13]][0], %[[VAL_7]] : (!cc.struct<{!cc.ptr<i8>, i64}>, !cc.ptr<i8>) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_15:.*]] = cc.insert_value %[[VAL_14]][1], %[[VAL_6]] : (!cc.struct<{!cc.ptr<i8>, i64}>, i64) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_16:.*]] = cc.compute_ptr %[[VAL_8]]{{\[}}%[[VAL_3]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_17:.*]] = cc.cast %[[VAL_16]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           cc.store %[[VAL_12]], %[[VAL_17]] : !cc.ptr<!cc.ptr<i8>>
// HYBRID:           return %[[VAL_15]] : !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:         }
// HYBRID:         llvm.mlir.global external constant @ghz.kernelName("ghz\00") {addr_space = 0 : i32}

// HYBRID-LABEL:   func.func @ghz.returnOffset() -> i64 {
// HYBRID:           %[[VAL_0:.*]] = cc.offsetof !cc.struct<{i32, f64}> [1] : i64
// HYBRID:           return %[[VAL_0]] : i64
// HYBRID:         }

// HYBRID-LABEL:   func.func @ghz.thunk(
// HYBRID-SAME:        %[[VAL_0:.*]]: !cc.ptr<i8>,
// HYBRID-SAME:        %[[VAL_1:.*]]: i1) -> !cc.struct<{!cc.ptr<i8>, i64}> {
// HYBRID:           %[[VAL_2:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// HYBRID:           %[[VAL_3:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// HYBRID:           %[[VAL_4:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.array<i8 x ?>>
// HYBRID:           %[[VAL_5:.*]] = cc.compute_ptr %[[VAL_4]]{{\[}}%[[VAL_3]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_6:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i32>
// HYBRID:           %[[VAL_7:.*]] = cc.load %[[VAL_6]] : !cc.ptr<i32>
// HYBRID:           %[[VAL_8:.*]] = cc.noinline_call @__nvqpp__mlirgen__ghz(%[[VAL_7]]) : (i32) -> f64
// HYBRID:           %[[VAL_9:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// HYBRID:           cc.store %[[VAL_8]], %[[VAL_9]] : !cc.ptr<f64>
// HYBRID:           %[[VAL_10:.*]] = call @__nvqpp_zeroDynamicResult() : () -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           return %[[VAL_10]] : !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:         }

// HYBRID-LABEL:   func.func @ghz.argsCreator(
// HYBRID-SAME:                               %[[VAL_0:.*]]: !cc.ptr<!cc.ptr<i8>>,
// HYBRID-SAME:                               %[[VAL_1:.*]]: !cc.ptr<!cc.ptr<i8>>) -> i64 {
// HYBRID:           %[[VAL_2:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<!cc.ptr<i8>>) -> !cc.ptr<!cc.array<!cc.ptr<i8> x ?>>
// HYBRID:           %[[VAL_3:.*]] = cc.compute_ptr %[[VAL_2]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x ?>>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_4:.*]] = cc.load %[[VAL_3]] : !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_5:.*]] = cc.cast %[[VAL_4]] : (!cc.ptr<i8>) -> !cc.ptr<i32>
// HYBRID:           %[[VAL_6:.*]] = cc.load %[[VAL_5]] : !cc.ptr<i32>
// HYBRID:           %[[VAL_7:.*]] = cc.alloca i64
// HYBRID:           %[[VAL_8:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// HYBRID:           %[[VAL_9:.*]] = call @malloc(%[[VAL_8]]) : (i64) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_10:.*]] = cc.cast %[[VAL_9]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// HYBRID:           %[[VAL_11:.*]] = cc.compute_ptr %[[VAL_10]][0] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<i32>
// HYBRID:           cc.store %[[VAL_6]], %[[VAL_11]] : !cc.ptr<i32>
// HYBRID:           cc.store %[[VAL_9]], %[[VAL_1]] : !cc.ptr<!cc.ptr<i8>>
// HYBRID:           return %[[VAL_8]] : i64
// HYBRID:         }

// HYBRID-LABEL:   llvm.func @ghz.kernelRegFunc() {
// HYBRID:           %[[VAL_0:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// HYBRID:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// HYBRID:           func.call @cudaqRegisterKernelName(%[[VAL_1]]) : (!cc.ptr<i8>) -> ()
// HYBRID:           %[[VAL_2:.*]] = func.constant @ghz.argsCreator : (!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>) -> i64
// HYBRID:           %[[VAL_3:.*]] = cc.func_ptr %[[VAL_2]] : ((!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>) -> i64) -> !cc.ptr<i8>
// HYBRID:           func.call @cudaqRegisterArgsCreator(%[[VAL_1]], %[[VAL_3]]) : (!cc.ptr<i8>, !cc.ptr<i8>) -> ()
// HYBRID:           llvm.return
// HYBRID:         }
// HYBRID:         llvm.mlir.global_ctors {ctors = [@ghz.kernelRegFunc], priorities = [17 : i32]}

