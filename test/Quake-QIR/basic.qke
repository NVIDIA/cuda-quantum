// ========================================================================== //
// Copyright (c) 2022 - 2023 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt %s --quake-add-deallocs | cudaq-translate --convert-to=qir | FileCheck %s

// CHECK:         %[[VAL_0:.*]] = zext i32
// CHECK:         %[[VAL_1:.*]] to i64
// CHECK:         %[[VAL_2:.*]] = tail call %[[VAL_3:.*]]* @__quantum__rt__qubit_allocate_array(i64 %[[VAL_0]])
// CHECK:         %[[VAL_4:.*]] = tail call %[[VAL_3]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK-DAG:     tail call void @__quantum__rt__qubit_release_array(%[[VAL_3]]* %[[VAL_2]])
// CHECK-DAG:     tail call void @__quantum__rt__qubit_release_array(%[[VAL_3]]* %[[VAL_4]])
// CHECK:         ret void

// CHECK:         %[[VAL_5:.*]] = tail call %[[VAL_6:.*]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK:         %[[VAL_7:.*]] = tail call %[[VAL_6]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK:         %[[VAL_8:.*]] = tail call %[[VAL_6]]* @__quantum__rt__qubit_allocate_array(i64 1)
// CHECK:         %[[VAL_9:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_6]]* %[[VAL_5]], i64 0)
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to %[[VAL_11:.*]]**
// CHECK:         %[[VAL_12:.*]] = load %[[VAL_11]]*, %[[VAL_11]]** %[[VAL_10]], align 8
// CHECK:         %[[VAL_13:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_6]]* %[[VAL_7]], i64 1)
// CHECK:         %[[VAL_14:.*]] = bitcast i8* %[[VAL_13]] to %[[VAL_11]]**
// CHECK:         %[[VAL_15:.*]] = load %[[VAL_11]]*, %[[VAL_11]]** %[[VAL_14]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_11]]* %[[VAL_12]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_12]], %Qubit* %[[VAL_15]])
// CHECK:         tail call void @__quantum__qis__rx(double 4.300000e-01, %[[VAL_11]]* %[[VAL_12]])
// CHECK:         %[[VAL_16:.*]] = tail call %[[VAL_17:.*]]* @__quantum__qis__mz(%[[VAL_11]]* %[[VAL_12]])
// CHECK-DAG:     tail call void @__quantum__rt__qubit_release_array(%[[VAL_6]]* %[[VAL_5]])
// CHECK-DAG:     tail call void @__quantum__rt__qubit_release_array(%[[VAL_6]]* %[[VAL_7]])
// CHECK-DAG:     tail call void @__quantum__rt__qubit_release_array(%[[VAL_6]]* %[[VAL_8]])
// CHECK:         ret void

module {
     func.func @test_func(%p : i32) {
          %qv = quake.alloca [%p : i32] !quake.qvec<?>
          %t = arith.constant 2 : i32
          %v = quake.alloca [%t : i32] !quake.qvec<?>
          return
     }

    func.func @test_func2(){

      %zero = arith.constant 0 : i32
      %one = arith.constant 1 : i32
      %neg = arith.constant -5 : i32
      %two = arith.constant 2 : i32
      %0 = quake.alloca [%two : i32] !quake.qvec<?>
     
      %1 = quake.alloca [%two : i32] !quake.qvec<2>
      %2 = quake.alloca [%one : i32] !quake.qvec<?>
      
      %qr1 = quake.extract_ref %0[%zero] : (!quake.qvec<?>,i32) -> !quake.qref
      %qr2 = quake.extract_ref %1[%one]  : (!quake.qvec<2>,i32) -> !quake.qref

      %fl = arith.constant 0.43 : f64
      %fl2 = arith.constant 0.33 : f64
      %fl3 = arith.constant 0.73 : f64
      quake.h %qr1 : (!quake.qref) -> ()  
      quake.x [%qr1] %qr2 : (!quake.qref, !quake.qref) -> ()
      quake.rx (%fl) %qr1 : (f64, !quake.qref) -> ()

      quake.mz %qr1 : (!quake.qref) -> i1
      return 
    }
}

