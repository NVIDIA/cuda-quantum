// ========================================================================== //
// Copyright (c) 2022 - 2024 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-translate --convert-to=qir %s | FileCheck %s

module {

  func.func @__nvqpp__mlirgen__function_kernel._Z6kernelv() attributes {"cudaq-entrypoint", "cudaq-kernel", no_this} {
    %0 = quake.alloca !quake.ref
    quake.h %0 : (!quake.ref) -> ()
    quake.custom_op @__nvqpp__mlirgen__function_custom_s_generator_1._Z20custom_s_generator_1RKSt6vectorIdSaIdEE.rodata_0<adj> %0 : (!quake.ref) -> ()
    quake.custom_op @__nvqpp__mlirgen__function_custom_s_adj_generator_1._Z24custom_s_adj_generator_1RKSt6vectorIdSaIdEE.rodata_1 %0 : (!quake.ref) -> ()
    quake.h %0 : (!quake.ref) -> ()
    %measOut = quake.mz %0 : (!quake.ref) -> !quake.measure
    return
  }
  cc.global constant @__nvqpp__mlirgen__function_custom_s_generator_1._Z20custom_s_generator_1RKSt6vectorIdSaIdEE.rodata_0 (dense<[(1.000000e+00,0.000000e+00), (0.000000e+00,0.000000e+00), (0.000000e+00,0.000000e+00), (0.000000e+00,1.000000e+00)]> : tensor<4xcomplex<f64>>) : !cc.array<complex<f64> x 4>
  cc.global constant @__nvqpp__mlirgen__function_custom_s_adj_generator_1._Z24custom_s_adj_generator_1RKSt6vectorIdSaIdEE.rodata_1 (dense<[(1.000000e+00,0.000000e+00), (0.000000e+00,0.000000e+00), (0.000000e+00,0.000000e+00), (0.000000e+00,-1.000000e+00)]> : tensor<4xcomplex<f64>>) : !cc.array<complex<f64> x 4>
    
}
 
// CHECK:         %[[VAL_0:.*]] = tail call
// CHECK:         %[[VAL_1:.*]]* @__quantum__rt__qubit_allocate_array(i64 1)
// CHECK:         %[[VAL_2:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 0)
// CHECK:         %[[VAL_3:.*]] = bitcast i8* %[[VAL_2]] to %[[VAL_4:.*]]**
// CHECK:         %[[VAL_5:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_3]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         %[[VAL_6:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_7:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_6]], i64 0)
// CHECK:         %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to %[[VAL_4]]**
// CHECK:         store %[[VAL_4]]* %[[VAL_5]], %[[VAL_4]]** %[[VAL_8]], align 8
// CHECK:         %[[VAL_9:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 0)
// CHECK:         tail call void @__quantum__qis__custom_unitary__adj({ double, double }* nonnull getelementptr inbounds ([4 x { double, double }], [4 x { double, double }]* @__nvqpp__mlirgen__function_custom_s_generator_1._Z20custom_s_generator_1RKSt6vectorIdSaIdEE.rodata_0, i64 0, i64 0), %[[VAL_1]]* %[[VAL_9]], %[[VAL_1]]* %[[VAL_6]], i8* nonnull getelementptr inbounds ([18 x i8], [18 x i8]* @cstr.{{.*}}, i64 0, i64 0))
// CHECK:         %[[VAL_10:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_11:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_10]], i64 0)
// CHECK:         %[[VAL_12:.*]] = bitcast i8* %[[VAL_11]] to %[[VAL_4]]**
// CHECK:         store %[[VAL_4]]* %[[VAL_5]], %[[VAL_4]]** %[[VAL_12]], align 8
// CHECK:         %[[VAL_13:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 0)
// CHECK:         tail call void @__quantum__qis__custom_unitary({ double, double }* nonnull getelementptr inbounds ([4 x { double, double }], [4 x { double, double }]* @__nvqpp__mlirgen__function_custom_s_adj_generator_1._Z24custom_s_adj_generator_1RKSt6vectorIdSaIdEE.rodata_1, i64 0, i64 0), %[[VAL_1]]* %[[VAL_13]], %[[VAL_1]]* %[[VAL_10]], i8* nonnull getelementptr inbounds ([22 x i8], [22 x i8]* @cstr.{{.*}}, i64 0, i64 0))
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         %[[VAL_14:.*]] = tail call %[[VAL_15:.*]]* @__quantum__qis__mz(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_1]]* %[[VAL_0]])
// CHECK:         ret void
