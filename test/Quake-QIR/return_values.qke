// ========================================================================== //
// Copyright (c) 2022 - 2024 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt --add-dealloc --kernel-execution=codegen=1 --canonicalize %s | cudaq-translate --convert-to=qir | FileCheck %s

// NB: the mangled name map is required for the kernel-execution pass.
// QIR codegen requires the target triple.
module attributes{ quake.mangled_name_map = {
  __nvqpp__mlirgen__test_0 = "test_0",
  __nvqpp__mlirgen__test_1 = "test_1",
  __nvqpp__mlirgen__test_2 = "test_2",
  __nvqpp__mlirgen__test_3 = "test_3",
  __nvqpp__mlirgen__test_4 = "test_4",
  __nvqpp__mlirgen__test_5 = "test_5" },
  llvm.triple = "x86_64-unknown-linux-gnu"} {

func.func private @__nvqpp_vectorCopyCtor(%arg0: !cc.ptr<i8> , %arg1: i64 , %arg2: i64 ) -> !cc.ptr<i8>

// vector<bool> -> struct ptr sret
func.func @__nvqpp__mlirgen__test_0(%arg0: i32) -> !cc.stdvec<i1> {
  %c1_i64 = arith.constant 1 : i64
  %c1 = arith.constant 1 : i64
  %c0 = arith.constant 0 : i64
  %0 = cc.alloca i32
  cc.store %arg0, %0 : !cc.ptr<i32>
  %1 = cc.load %0 : !cc.ptr<i32>
  %2 = arith.extsi %1 : i32 to i64
  %3 = quake.alloca !quake.veq<?>[%2 : i64]
  %4 = quake.veq_size %3 : (!quake.veq<?>) -> i64
  %6 = cc.loop while ((%arg1 = %c0) -> (i64)) {
    %12 = arith.cmpi slt, %arg1, %4 : i64
    cc.condition %12(%arg1 : i64)
  } do {
  ^bb0(%arg1: i64):
    %12 = quake.extract_ref %3[%arg1] : (!quake.veq<?>, i64) -> !quake.ref
    quake.h %12 : (!quake.ref) -> ()
    cc.continue %arg1 : i64
  } step {
  ^bb0(%arg1: i64):
    %12 = arith.addi %arg1, %c1 : i64
    cc.continue %12 : i64
  } {invariant}
  %measOut = quake.mz %3 : (!quake.veq<?>) -> !cc.stdvec<!quake.measure>
  %7 = quake.discriminate %measOut : (!cc.stdvec<!quake.measure>) -> !cc.stdvec<i1>
  %8 = cc.stdvec_data %7 : (!cc.stdvec<i1>) -> !cc.ptr<i8>
  %9 = cc.stdvec_size %7 : (!cc.stdvec<i1>) -> i64
  %10 = call @__nvqpp_vectorCopyCtor(%8, %9, %c1_i64) : (!cc.ptr<i8>, i64, i64) -> !cc.ptr<i8>
  %11 = cc.stdvec_init %10, %9 : (!cc.ptr<i8>, i64) -> !cc.stdvec<i1>
  return %11 : !cc.stdvec<i1>
}

func.func @test_0(%1: !cc.ptr<!cc.struct<{!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>}>> {llvm.sret = !cc.struct<{!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>}>}, %this: !cc.ptr<i8>, %2: i32) {
  return
}

// CHECK-LABEL: define { i1*, i64 } @__nvqpp__mlirgen__test_0(
// CHECK-SAME:    i32 %[[VAL_1:.*]]) {{.*}}{
// CHECK:         %[[VAL_2:.*]] = sext i32 %[[VAL_1]] to i64
// CHECK:         %[[VAL_3:.*]] = tail call %[[VAL_4:.*]]* @__quantum__rt__qubit_allocate_array(i64 %[[VAL_2]])
// CHECK:         %[[VAL_5:.*]] = tail call i64 @__quantum__rt__array_get_size_1d(%[[VAL_4]]* %[[VAL_3]])
// CHECK:         %[[VAL_6:.*]] = icmp sgt i64 %[[VAL_5]], 0
// CHECK:         br i1 %[[VAL_6]], label %[[VAL_7:.*]], label %[[VAL_8:.*]]
// CHECK:       ._crit_edge.thread:                               ; preds = %[[VAL_9:.*]]
// CHECK:         %[[VAL_10:.*]] = alloca i8, i64 %[[VAL_5]], align 1
// CHECK:         br label %[[VAL_11:.*]]
// CHECK:       .lr.ph:                                           ; preds = %[[VAL_9]], %[[VAL_7]]
// CHECK:         %[[VAL_12:.*]] = phi i64 [ %[[VAL_13:.*]], %[[VAL_7]] ], [ 0, %[[VAL_9]] ]
// CHECK:         %[[VAL_14:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_4]]* %[[VAL_3]], i64 %[[VAL_12]])
// CHECK:         %[[VAL_15:.*]] = bitcast i8* %[[VAL_14]] to %[[VAL_16:.*]]**
// CHECK:         %[[VAL_17:.*]] = load %[[VAL_16]]*, %[[VAL_16]]** %[[VAL_15]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_16]]* %[[VAL_17]])
// CHECK:         %[[VAL_13]] = add nuw nsw i64 %[[VAL_12]], 1
// CHECK:         %[[VAL_18:.*]] = icmp eq i64 %[[VAL_13]], %[[VAL_5]]
// CHECK:         br i1 %[[VAL_18]], label %[[VAL_19:.*]], label %[[VAL_7]]
// CHECK:       ._crit_edge:                                      ; preds = %[[VAL_7]]
// CHECK:         %[[VAL_20:.*]] = alloca i8, i64 %[[VAL_5]], align 1
// CHECK:         br i1 %[[VAL_6]], label %[[VAL_21:.*]], label %[[VAL_11]]
// CHECK:       .lr.ph4:                                          ; preds = %[[VAL_19]], %[[VAL_21]]
// CHECK:         %[[VAL_22:.*]] = phi i64 [ %[[VAL_23:.*]], %[[VAL_21]] ], [ 0, %[[VAL_19]] ]
// CHECK:         %[[VAL_24:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_4]]* %[[VAL_3]], i64 %[[VAL_22]])
// CHECK:         %[[VAL_25:.*]] = bitcast i8* %[[VAL_24]] to %[[VAL_16]]**
// CHECK:         %[[VAL_26:.*]] = load %[[VAL_16]]*, %[[VAL_16]]** %[[VAL_25]], align 8
// CHECK:         %[[VAL_27:.*]] = tail call %[[VAL_28:.*]]* @__quantum__qis__mz(%[[VAL_16]]* %[[VAL_26]])
// CHECK:         %[[VAL_29:.*]] = bitcast %[[VAL_28]]* %[[VAL_27]] to i1*
// CHECK:         %[[VAL_30:.*]] = load i1, i1* %[[VAL_29]], align 1
// CHECK:         %[[VAL_31:.*]] = getelementptr i8, i8* %[[VAL_20]], i64 %[[VAL_22]]
// CHECK:         %[[VAL_32:.*]] = zext i1 %[[VAL_30]] to i8
// CHECK:         store i8 %[[VAL_32]], i8* %[[VAL_31]], align 1
// CHECK:         %[[VAL_23]] = add nuw nsw i64 %[[VAL_22]], 1
// CHECK:         %[[VAL_33:.*]] = icmp eq i64 %[[VAL_23]], %[[VAL_5]]
// CHECK:         br i1 %[[VAL_33]], label %[[VAL_11]], label %[[VAL_21]]
// CHECK:       ._crit_edge5:                                     ; preds = %[[VAL_21]], %[[VAL_8]], %[[VAL_19]]
// CHECK:         %[[VAL_34:.*]] = phi i8* [ %[[VAL_10]], %[[VAL_8]] ], [ %[[VAL_20]], %[[VAL_19]] ], [ %[[VAL_20]], %[[VAL_21]] ]
// CHECK:         %[[VAL_35:.*]] = call i8* @__nvqpp_vectorCopyCtor(i8* nonnull %[[VAL_34]], i64 %[[VAL_5]], i64 1)
// CHECK:         %[[VAL_36:.*]] = bitcast i8* %[[VAL_35]] to i1*
// CHECK:         %[[VAL_37:.*]] = insertvalue { i1*, i64 } undef, i1* %[[VAL_36]], 0
// CHECK:         %[[VAL_38:.*]] = insertvalue { i1*, i64 } %[[VAL_37]], i64 %[[VAL_5]], 1
// CHECK:         call void @__quantum__rt__qubit_release_array(%Array* %[[VAL_3]])
// CHECK:         ret { i1*, i64 } %[[VAL_38]]
// CHECK:       }

// CHECK-LABEL: define void @test_0({ i8*, i8*, i8* }* sret({ i8*, i8*, i8* }) 
// CHECK-SAME:       %[[VAL_0:.*]], i8* nocapture readnone %[[VAL_1:.*]], i32 %[[VAL_2:.*]]) {{.*}}{
// CHECK:         %[[VAL_3:.*]] = alloca { i32, { i1*, i64 } }, align 8
// CHECK:         %[[VAL_4:.*]] = bitcast { i32, { i1*, i64 } }* %[[VAL_3]] to i8*
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds { i32, { i1*, i64 } }, { i32, { i1*, i64 } }* %[[VAL_3]], i64 0, i32 0
// CHECK:         store i32 %[[VAL_2]], i32* %[[VAL_5]], align 8
// CHECK:         %[[VAL_6:.*]] = call { i8*, i64 } @altLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_0.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_0.thunk to i8*), i8* nonnull %[[VAL_4]], i64 24, i64 8)
// CHECK:         %[[VAL_7:.*]] = extractvalue { i8*, i64 } %[[VAL_6]], 0
// CHECK:         %[[VAL_8:.*]] = icmp eq i8* %[[VAL_7]], null
// CHECK:         %[[VAL_9:.*]] = getelementptr i8, i8* %[[VAL_7]], i64 8
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to { i1*, i64 }*
// CHECK:         %[[VAL_11:.*]] = getelementptr inbounds { i32, { i1*, i64 } }, { i32, { i1*, i64 } }* %[[VAL_3]], i64 0, i32 1
// CHECK:         %[[VAL_12:.*]] = select i1 %[[VAL_8]], { i1*, i64 }* %[[VAL_11]], { i1*, i64 }* %[[VAL_10]]
// CHECK:         %[[VAL_13:.*]] = bitcast { i1*, i64 }* %[[VAL_12]] to i8**
// CHECK:         %[[VAL_14:.*]] = load i8*, i8** %[[VAL_13]], align 8
// CHECK:         %[[VAL_15:.*]] = getelementptr inbounds { i32, { i1*, i64 } }, { i32, { i1*, i64 } }* %[[VAL_3]], i64 0, i32 1, i32 1
// CHECK:         %[[VAL_16:.*]] = getelementptr i8, i8* %[[VAL_7]], i64 16
// CHECK:         %[[VAL_17:.*]] = bitcast i8* %[[VAL_16]] to i64*
// CHECK:         %[[VAL_18:.*]] = select i1 %[[VAL_8]], i64* %[[VAL_15]], i64* %[[VAL_17]]
// CHECK:         %[[VAL_19:.*]] = load i64, i64* %[[VAL_18]], align 4
// CHECK:         %[[VAL_20:.*]] = bitcast { i8*, i8*, i8* }* %[[VAL_0]] to i8*
// CHECK:         call void @__nvqpp_initializer_list_to_vector_bool(i8* %[[VAL_20]], i8* %[[VAL_14]], i64 %[[VAL_19]])
// CHECK:         call void @free(i8* %[[VAL_7]])
// CHECK:         ret void
// CHECK:       }

// struct{bool, bool} -> i16
func.func @__nvqpp__mlirgen__test_1() -> !cc.struct<{i1, i1}> {
  %qubits = quake.alloca !quake.veq<2>
  %q0 = quake.extract_ref %qubits[0] : (!quake.veq<2>) -> !quake.ref
  %q1 = quake.extract_ref %qubits[1] : (!quake.veq<2>) -> !quake.ref
  quake.h %q0 : (!quake.ref) -> ()
  quake.x [%q0] %q1 : (!quake.ref, !quake.ref) -> ()
  %m0 = quake.mz %q0 : (!quake.ref) -> !quake.measure
  %m1 = quake.mz %q1 : (!quake.ref) -> !quake.measure
  %rv = cc.undef !cc.struct<{i1, i1}>
  %d1 = quake.discriminate %m0 : (!quake.measure) -> i1
  %rv1 = cc.insert_value %d1, %rv[0] : (!cc.struct<{i1, i1}>, i1) -> !cc.struct<{i1, i1}>
  %d2 = quake.discriminate %m1 : (!quake.measure) -> i1
  %rv2 = cc.insert_value %d2, %rv1[1] : (!cc.struct<{i1, i1}>, i1) -> !cc.struct<{i1, i1}>
  return %rv2 : !cc.struct<{i1, i1}>
}

func.func @test_1(%this: !cc.ptr<i8>) -> i16 {
  %0 = cc.undef i16
  return %0 : i16
}

// CHECK-LABEL: define { i1, i1 } @__nvqpp__mlirgen__test_1()
// CHECK:         %[[VAL_1:.*]] = tail call %[[VAL_2:.*]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK:         %[[VAL_3:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_2]]* %[[VAL_1]], i64 0)
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to %[[VAL_5:.*]]**
// CHECK:         %[[VAL_6:.*]] = load %[[VAL_5]]*, %[[VAL_5]]** %[[VAL_4]], align 8
// CHECK:         %[[VAL_7:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_2]]* %[[VAL_1]], i64 1)
// CHECK:         %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to %[[VAL_5]]**
// CHECK:         %[[VAL_9:.*]] = load %[[VAL_5]]*, %[[VAL_5]]** %[[VAL_8]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_5]]* %[[VAL_6]])
// CHECK:         tail call void (i64, void (%[[VAL_2]]*, %[[VAL_5]]*)*, ...) @invokeWithControlQubits(i64 1, void (%[[VAL_2]]*, %[[VAL_5]]*)* nonnull @__quantum__qis__x__ctl, %[[VAL_5]]* %[[VAL_6]], %[[VAL_5]]* %[[VAL_9]])
// CHECK:         %[[VAL_10:.*]] = tail call %[[VAL_11:.*]]* @__quantum__qis__mz(%[[VAL_5]]* %[[VAL_6]])
// CHECK:         %[[VAL_12:.*]] = bitcast %Result* %[[VAL_10]] to i1*
// CHECK:         %[[VAL_13:.*]] = load i1, i1* %[[VAL_12]], align 1
// CHECK:         %[[VAL_14:.*]] = tail call %[[VAL_11]]* @__quantum__qis__mz(%[[VAL_5]]* %[[VAL_9]])
// CHECK:         %[[VAL_15:.*]] = bitcast %Result* %[[VAL_14]] to i1*
// CHECK:         %[[VAL_16:.*]] = load i1, i1* %[[VAL_15]], align 1
// CHECK:         %[[VAL_20:.*]] = insertvalue { i1, i1 } undef, i1 %[[VAL_13]], 0
// CHECK:         %[[VAL_19:.*]] = insertvalue { i1, i1 } %[[VAL_20]], i1 %[[VAL_16]], 1
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_2]]* %[[VAL_1]])
// CHECK:         ret { i1, i1 } %[[VAL_19]]
// CHECK:       }

// CHECK-LABEL: define i16 @test_1(i8* nocapture readnone
// CHECK-SAME:    %[[VAL_0:.*]]) {{.*}}{
// CHECK-NEXT:    %[[VAL_2:.*]] = alloca i16, align 8
// CHECK:         %[[VAL_3:.*]] = bitcast i16* %[[VAL_2]] to i8*
// CHECK:         call { i8*, i64 } @altLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_1.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_1.thunk to i8*), i8* nonnull %[[VAL_3]], i64 2, i64 0)
// CHECK:         %[[VAL_4:.*]] = load i16, i16* %[[VAL_2]], align 8
// CHECK:         ret i16 %[[VAL_4]]
// CHECK:       }

// struct{i16, f32, f64, i64} -> sret ptr
func.func @__nvqpp__mlirgen__test_2() -> !cc.struct<{i16, f32, f64, i64}> {
  %rv = cc.undef !cc.struct<{i16, f32, f64, i64}>
  %c1 = arith.constant 8 : i16
  %rv1 = cc.insert_value %c1, %rv[0] : (!cc.struct<{i16, f32, f64, i64}>, i16) -> !cc.struct<{i16, f32, f64, i64}>
  %c2 = arith.constant 5.4 : f32
  %rv2 = cc.insert_value %c2, %rv1[1] : (!cc.struct<{i16, f32, f64, i64}>, f32) -> !cc.struct<{i16, f32, f64, i64}>
  %c3 = arith.constant 37.83 : f64
  %rv3 = cc.insert_value %c3, %rv2[2] : (!cc.struct<{i16, f32, f64, i64}>, f64) -> !cc.struct<{i16, f32, f64, i64}>
  %c4 = arith.constant 1479 : i64
  %rv4 = cc.insert_value %c4, %rv3[3] : (!cc.struct<{i16, f32, f64, i64}>, i64) -> !cc.struct<{i16, f32, f64, i64}>
  return %rv4 : !cc.struct<{i16, f32, f64, i64}>
}

func.func @test_2(%1: !cc.ptr<!cc.struct<{i16, f32, f64, i64}>> {llvm.sret = !cc.struct<{i16, f32, f64, i64}>}, %this: !cc.ptr<i8>) {
  return
}

// CHECK-LABEL: define { i16, float, double, i64 } @__nvqpp__mlirgen__test_2()
// CHECK:         ret { i16, float, double, i64 } { i16 8, float 0x40159999A0000000, double 3.783000e+01, i64 1479 }
// CHECK:       }

// CHECK-LABEL: define void @test_2({ i16, float, double, i64 }* nocapture writeonly sret({ i16, float, double, i64 }) 
// CHECK-SAME:          %[[VAL_0:.*]], i8* nocapture readnone %[[VAL_1:.*]]) {{.*}}{
// CHECK:         %[[VAL_2:.*]] = alloca { { i16, float, double, i64 } }, align 8
// CHECK:         %[[VAL_3:.*]] = bitcast { { i16, float, double, i64 } }* %[[VAL_2]] to i8*
// CHECK:         call { i8*, i64 } @altLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_2.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_2.thunk to i8*), i8* nonnull %[[VAL_3]], i64 24, i64 0)
// CHECK:         %[[VAL_4:.*]] = bitcast { i16, float, double, i64 }* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(24) %[[VAL_4]], i8* noundef nonnull align 8 dereferenceable(24) %[[VAL_3]], i64 24, i1 false)
// CHECK:         ret void
// CHECK:       }

// array<T x n> -> sret ptr
func.func @__nvqpp__mlirgen__test_3() -> !cc.array<i64 x 5> {
  %rv = cc.undef !cc.array<i64 x 5>
  %c1 = arith.constant 5 : i64
  %rv1 = cc.insert_value %c1, %rv[0] : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c2 = arith.constant 74 : i64
  %rv2 = cc.insert_value %c2, %rv1[1] : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c3 = arith.constant 299 : i64
  %rv3 = cc.insert_value %c3, %rv2[2] : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c4 = arith.constant 1659 : i64
  %rv4 = cc.insert_value %c4, %rv3[3] : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  %c5 = arith.constant 61234 : i64
  %rv5 = cc.insert_value %c5, %rv4[4] : (!cc.array<i64 x 5>, i64) -> !cc.array<i64 x 5>
  return %rv5 : !cc.array<i64 x 5>
}

func.func @test_3(%1: !cc.ptr<!cc.array<i64 x 5>> {llvm.sret = !cc.array<i64 x 5>}, %this: !cc.ptr<i8>) {
  return
}

// CHECK-LABEL: define [5 x i64] @__nvqpp__mlirgen__test_3(
// CHECK:         ret [5 x i64] [i64 5, i64 74, i64 299, i64 1659, i64 61234]
// CHECK:       }

// CHECK-LABEL: define void @test_3([5 x i64]* nocapture writeonly sret([5 x i64]) 
// CHECK-SAME:      %[[VAL_0:.*]], i8* nocapture readnone %[[VAL_1:.*]]) {{.*}}{
// CHECK:         %[[VAL_2:.*]] = alloca { [5 x i64] }, align 8
// CHECK:         %[[VAL_3:.*]] = bitcast { [5 x i64] }* %[[VAL_2]] to i8*
// CHECK:         call { i8*, i64 } @altLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_3.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_3.thunk to i8*), i8* nonnull %[[VAL_3]], i64 40, i64 0)
// CHECK:         %[[VAL_4:.*]] = bitcast [5 x i64]* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(40) %[[VAL_4]], i8* noundef nonnull align 8 dereferenceable(40) %[[VAL_3]], i64 40, i1 false)
// CHECK:         ret void
// CHECK:       }

// small struct (<= 128) -> { i64, f64 }
func.func @__nvqpp__mlirgen__test_4() -> (i64, f64) {
  %c1 = arith.constant 537892 : i64
  %c2 = arith.constant 94.2134 : f64
  return %c1, %c2 : i64, f64
}

func.func @test_4(%sret: !cc.ptr<!cc.struct<{i64, f64}>> {llvm.sret = !cc.struct<{i64, f64}>}, %this: !cc.ptr<i8>) {
  return
}

// CHECK-LABEL: define { i64, double } @__nvqpp__mlirgen__test_4() {{.*}}{
// CHECK:         ret { i64, double } { i64 537892, double 0x40578DA858793DD9 }
// CHECK:       }

// CHECK-LABEL: define void @test_4({ i64, double }* nocapture writeonly sret({ i64, double }) 
// CHECK-SAME:     %[[VAL_0:.*]], i8* nocapture readnone %[[VAL_1:.*]]) {{.*}}{
// CHECK:         %[[VAL_2:.*]] = alloca { i64, double }, align 8
// CHECK:         %[[VAL_3:.*]] = bitcast { i64, double }* %[[VAL_2]] to i8*
// CHECK:         call { i8*, i64 } @altLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_4.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_4.thunk to i8*), i8* nonnull %[[VAL_3]], i64 16, i64 0)
// CHECK:         %[[VAL_4:.*]] = bitcast { i64, double }* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(16) %[[VAL_4]], i8* noundef nonnull align 8 dereferenceable(16) %[[VAL_3]], i64 16, i1 false)
// CHECK:         ret void
// CHECK:       }

func.func @__nvqpp__mlirgen__test_5() -> (i64, f64) attributes {no_this} {
  %c1 = arith.constant 537892 : i64
  %c2 = arith.constant 94.2134 : f64
  return %c1, %c2 : i64, f64
}

func.func @test_5(%sret: !cc.ptr<!cc.struct<{i64, f64}>> {llvm.sret = !cc.struct<{i64, f64}>}) {
  return
}

// CHECK-LABEL: define { i64, double } @__nvqpp__mlirgen__test_5() {{.*}}{
// CHECK:         ret { i64, double } { i64 537892, double 0x40578DA858793DD9 }
// CHECK:       }

// CHECK-LABEL: define void @test_5({ i64, double }* nocapture writeonly sret({ i64, double }) 
// CHECK-SAME:       %[[VAL_0:.*]]) {{.*}}{
// CHECK:         %[[VAL_1:.*]] = alloca { i64, double }, align 8
// CHECK:         %[[VAL_2:.*]] = bitcast { i64, double }* %[[VAL_1]] to i8*
// CHECK:         call { i8*, i64 } @altLaunchKernel(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_5.kernelName, i64 0, i64 0), i8* nonnull bitcast ({ i8*, i64 } (i8*, i1)* @test_5.thunk to i8*), i8* nonnull %[[VAL_2]], i64 16, i64 0)
// CHECK:         %[[VAL_3:.*]] = bitcast { i64, double }* %[[VAL_0]] to i8*
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 8 dereferenceable(16) %[[VAL_3]], i8* noundef nonnull align 8 dereferenceable(16) %[[VAL_2]], i64 16, i1 false)
// CHECK:         ret void
// CHECK:       }

}

//===----------------------------------------------------------------------===//

// CHECK-LABEL: define i64 @test_0.returnOffset()
// CHECK:         ret i64 8
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_0.thunk(i8* nocapture 
// CHECK-SAME:     %[[VAL_0:.*]], i1 %[[VAL_1:.*]]) {
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i32*
// CHECK:         %[[VAL_3:.*]] = load i32, i32* %[[VAL_2]], align 4
// CHECK:         %[[VAL_5:.*]] = sext i32 %[[VAL_3]] to i64
// CHECK:         %[[VAL_6:.*]] = tail call %[[VAL_7:.*]]* @__quantum__rt__qubit_allocate_array(i64 %[[VAL_5]])
// CHECK:         %[[VAL_8:.*]] = tail call i64 @__quantum__rt__array_get_size_1d(%[[VAL_7]]* %[[VAL_6]])
// CHECK:         %[[VAL_9:.*]] = icmp sgt i64 %[[VAL_8]], 0
// CHECK:         br i1 %[[VAL_9]], label %[[VAL_10:.*]], label %[[VAL_11:.*]]
// CHECK:       ._crit_edge.thread:                               ; preds = %[[VAL_12:.*]]
// CHECK:         %[[VAL_13:.*]] = alloca i8, i64 %[[VAL_8]], align 1
// CHECK:         br label %[[VAL_14:.*]]
// CHECK:       .lr.ph:                                           ; preds = %[[VAL_12]], %[[VAL_10]]
// CHECK:         %[[VAL_15:.*]] = phi i64 [ %[[VAL_16:.*]], %[[VAL_10]] ], [ 0, %[[VAL_12]] ]
// CHECK:         %[[VAL_17:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_7]]* %[[VAL_6]], i64 %[[VAL_15]])
// CHECK:         %[[VAL_18:.*]] = bitcast i8* %[[VAL_17]] to %[[VAL_19:.*]]**
// CHECK:         %[[VAL_20:.*]] = load %[[VAL_19]]*, %[[VAL_19]]** %[[VAL_18]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_19]]* %[[VAL_20]])
// CHECK:         %[[VAL_16]] = add nuw nsw i64 %[[VAL_15]], 1
// CHECK:         %[[VAL_21:.*]] = icmp eq i64 %[[VAL_16]], %[[VAL_8]]
// CHECK:         br i1 %[[VAL_21]], label %[[VAL_22:.*]], label %[[VAL_10]]
// CHECK:       ._crit_edge:                                      ; preds = %[[VAL_10]]
// CHECK:         %[[VAL_23:.*]] = alloca i8, i64 %[[VAL_8]], align 1
// CHECK:         br i1 %[[VAL_9]], label %[[VAL_24:.*]], label %[[VAL_14]]
// CHECK:       [[VAL_24]]:                                          ; preds = %[[VAL_22]], %[[VAL_24]]
// CHECK:         %[[VAL_25:.*]] = phi i64 [ %[[VAL_26:.*]], %[[VAL_24]] ], [ 0, %[[VAL_22]] ]
// CHECK:         %[[VAL_27:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_7]]* %[[VAL_6]], i64 %[[VAL_25]])
// CHECK:         %[[VAL_28:.*]] = bitcast i8* %[[VAL_27]] to %[[VAL_19]]**
// CHECK:         %[[VAL_29:.*]] = load %[[VAL_19]]*, %[[VAL_19]]** %[[VAL_28]], align 8
// CHECK:         %[[VAL_30:.*]] = tail call %[[VAL_31:.*]]* @__quantum__qis__mz(%[[VAL_19]]* %[[VAL_29]])
// CHECK:         %[[VAL_32:.*]] = bitcast %[[VAL_31]]* %[[VAL_30]] to i1*
// CHECK:         %[[VAL_33:.*]] = load i1, i1* %[[VAL_32]], align 1
// CHECK:         %[[VAL_34:.*]] = getelementptr i8, i8* %[[VAL_23]], i64 %[[VAL_25]]
// CHECK:         %[[VAL_35:.*]] = zext i1 %[[VAL_33]] to i8
// CHECK:         store i8 %[[VAL_35]], i8* %[[VAL_34]], align 1
// CHECK:         %[[VAL_26]] = add nuw nsw i64 %[[VAL_25]], 1
// CHECK:         %[[VAL_36:.*]] = icmp eq i64 %[[VAL_26]], %[[VAL_8]]
// CHECK:         br i1 %[[VAL_36]], label %[[VAL_14]], label %[[VAL_24]]
// CHECK:       [[VAL_14]]:                                     ; preds = %[[VAL_24]], %[[VAL_11]], %[[VAL_22]]
// CHECK:         %[[VAL_37:.*]] = phi i8* [ %[[VAL_13]], %[[VAL_11]] ], [ %[[VAL_23]], %[[VAL_22]] ], [ %[[VAL_23]], %[[VAL_24]] ]
// CHECK:         %[[VAL_38:.*]] = call i8* @__nvqpp_vectorCopyCtor(i8* nonnull %[[VAL_37]], i64 %[[VAL_8]], i64 1)
// CHECK:         call void @__quantum__rt__qubit_release_array(%[[VAL_7]]* %[[VAL_6]])
// CHECK:         %[[VAL_50:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_51:.*]] = bitcast i8* %[[VAL_50]] to i8**
// CHECK:         store i8* %[[VAL_38]], i8** %[[VAL_51]], align 8
// CHECK:         %[[VAL_52:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 16
// CHECK:         %[[VAL_53:.*]] = bitcast i8* %[[VAL_52]] to i64*
// CHECK:         store i64 %[[VAL_8]], i64* %[[VAL_53]], align 8
// CHECK:         br i1 %[[VAL_1]], label %[[VAL_42:.*]], label %[[VAL_43:.*]]
// CHECK:       [[VAL_43]]:                                       ; preds = %[[VAL_14]], %[[VAL_42]]
// CHECK:         %[[VAL_44:.*]] = phi { i8*, i64 } [ %[[VAL_45:.*]], %[[VAL_42]] ], [ zeroinitializer, %[[VAL_14]] ]
// CHECK:         ret { i8*, i64 } %[[VAL_44]]
// CHECK:       [[VAL_42]]:                                               ; preds = %[[VAL_14]]
// CHECK:         %[[VAL_46:.*]] = add i64 %[[VAL_8]], 24
// CHECK:         %[[VAL_47:.*]] = call i8* @malloc(i64 %[[VAL_46]])
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* noundef nonnull align 1 dereferenceable(24) %[[VAL_47]], i8* noundef nonnull align 1 dereferenceable(24) %[[VAL_0]], i64 24, i1 false)
// CHECK:         %[[VAL_48:.*]] = getelementptr i8, i8* %[[VAL_47]], i64 24
// CHECK:         call void @llvm.memcpy.p0i8.p0i8.i64(i8* align 1 %[[VAL_48]], i8* align 1 %[[VAL_38]], i64 %[[VAL_8]], i1 false)
// CHECK:         %[[VAL_49:.*]] = insertvalue { i8*, i64 } undef, i8* %[[VAL_47]], 0
// CHECK:         %[[VAL_45]] = insertvalue { i8*, i64 } %[[VAL_49]], i64 %[[VAL_46]], 1
// CHECK:         br label %[[VAL_43]]
// CHECK:       }

// CHECK-LABEL: define i64 @test_0.argsCreator(i8** nocapture readonly 
// CHECK-SAME:        %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:        %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = bitcast i8** %[[VAL_0]] to i32**
// CHECK:         %[[VAL_3:.*]] = load i32*, i32** %[[VAL_2]], align 8
// CHECK:         %[[VAL_4:.*]] = load i32, i32* %[[VAL_3]], align 4
// CHECK:         %[[VAL_5:.*]] = insertvalue { i32, { i1*, i64 } } undef, i32 %[[VAL_4]], 0
// CHECK:         %[[VAL_6:.*]] = tail call dereferenceable_or_null(24) i8* @malloc(i64 24)
// CHECK:         %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to { i32, { i1*, i64 } }*
// CHECK:         store { i32, { i1*, i64 } } %[[VAL_5]], { i32, { i1*, i64 } }* %[[VAL_7]], align 8
// CHECK:         store i8* %[[VAL_6]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 24
// CHECK:       }

// CHECK-LABEL: define void @test_0.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_0.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_0.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_0.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_1.returnOffset()
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_1.thunk(i8* nocapture writeonly 
// CHECK-SAME:      %[[VAL_0:.*]], i1
// CHECK-SAME:      %[[VAL_1:.*]]) {
// CHECK:         %[[VAL_2:.*]] = tail call %[[VAL_3:.*]]* @__quantum__rt__qubit_allocate_array(i64 2)
// CHECK:         %[[VAL_4:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_3]]* %[[VAL_2]], i64 0)
// CHECK:         %[[VAL_5:.*]] = bitcast i8* %[[VAL_4]] to %[[VAL_6:.*]]**
// CHECK:         %[[VAL_7:.*]] = load %[[VAL_6]]*, %[[VAL_6]]** %[[VAL_5]], align 8
// CHECK:         %[[VAL_8:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_3]]* %[[VAL_2]], i64 1)
// CHECK:         %[[VAL_9:.*]] = bitcast i8* %[[VAL_8]] to %[[VAL_6]]**
// CHECK:         %[[VAL_10:.*]] = load %[[VAL_6]]*, %[[VAL_6]]** %[[VAL_9]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_6]]* %[[VAL_7]])
// CHECK:         tail call void (i64, void (%[[VAL_3]]*, %[[VAL_6]]*)*, ...) @invokeWithControlQubits(i64 1, void (%[[VAL_3]]*, %[[VAL_6]]*)* nonnull @__quantum__qis__x__ctl, %[[VAL_6]]* %[[VAL_7]], %[[VAL_6]]* %[[VAL_10]])
// CHECK:         %[[VAL_11:.*]] = tail call %[[VAL_12:.*]]* @__quantum__qis__mz(%[[VAL_6]]* %[[VAL_7]])
// CHECK:         %[[VAL_13:.*]] = bitcast %[[VAL_12]]* %[[VAL_11]] to i1*
// CHECK:         %[[VAL_14:.*]] = load i1, i1* %[[VAL_13]], align 1
// CHECK:         %[[VAL_15:.*]] = tail call %[[VAL_12]]* @__quantum__qis__mz(%[[VAL_6]]* %[[VAL_10]])
// CHECK:         %[[VAL_16:.*]] = bitcast %[[VAL_12]]* %[[VAL_15]] to i1*
// CHECK:         %[[VAL_17:.*]] = load i1, i1* %[[VAL_16]], align 1
// CHECK:         %[[VAL_18:.*]] = bitcast i8* %[[VAL_0]] to i1*
// CHECK:         store i1 %[[VAL_14]], i1* %[[VAL_18]], align 1
// CHECK:         %[[VAL_19:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 1
// CHECK:         %[[VAL_20:.*]] = bitcast i8* %[[VAL_19]] to i1*
// CHECK:         store i1 %[[VAL_17]], i1* %[[VAL_20]], align 1
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_3]]* %[[VAL_2]])
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_1.argsCreator(i8** nocapture readnone 
// CHECK-SAME:        %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:        %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(2) i8* @malloc(i64 2)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 2
// CHECK:       }

// CHECK-LABEL: define void @test_1.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_1.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_1.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_1.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_2.returnOffset()
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_2.thunk(i8* nocapture writeonly 
// CHECK-SAME:      %[[VAL_0:.*]], i1
// CHECK-SAME:      %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to { i16, float, double, i64 }*
// CHECK:         store { i16, float, double, i64 } { i16 8, float 0x40159999A0000000, double 3.783000e+01, i64 1479 }, { i16, float, double, i64 }* %[[VAL_2]], align 8
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_2.argsCreator(i8** nocapture readnone 
// CHECK-SAME:         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:         %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(24) i8* @malloc(i64 24)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 24
// CHECK:       }

// CHECK-LABEL: define void @test_2.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_2.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_2.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_2.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_3.returnOffset()
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_3.thunk(i8* nocapture writeonly 
// CHECK-SAME:          %[[VAL_0:.*]], i1 %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i64*
// CHECK:         store i64 5, i64* %[[VAL_2]], align 4
// CHECK:         %[[VAL_3:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to i64*
// CHECK:         store i64 74, i64* %[[VAL_4]], align 4
// CHECK:         %[[VAL_5:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 16
// CHECK:         %[[VAL_6:.*]] = bitcast i8* %[[VAL_5]] to i64*
// CHECK:         store i64 299, i64* %[[VAL_6]], align 4
// CHECK:         %[[VAL_7:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 24
// CHECK:         %[[VAL_8:.*]] = bitcast i8* %[[VAL_7]] to i64*
// CHECK:         store i64 1659, i64* %[[VAL_8]], align 4
// CHECK:         %[[VAL_9:.*]] = getelementptr inbounds i8, i8* %[[VAL_0]], i64 32
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to i64*
// CHECK:         store i64 61234, i64* %[[VAL_10]], align 4
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_3.argsCreator(i8** nocapture readnone 
// CHECK-SAME:       %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:       %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(40) i8* @malloc(i64 40)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 40
// CHECK:       }

// CHECK-LABEL: define void @test_3.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_3.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_3.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_3.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_4.returnOffset()
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_4.thunk(i8* nocapture writeonly 
// CHECK-SAME:        %[[VAL_0:.*]], i1 %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i64*
// CHECK:         store i64 537892, i64* %[[VAL_2]], align 4
// CHECK:         %[[VAL_3:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to double*
// CHECK:         store double 0x40578DA858793DD9, double* %[[VAL_4]], align 8
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_4.argsCreator(i8** nocapture readnone 
// CHECK-SAME:         %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:         %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(16) i8* @malloc(i64 16)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 16
// CHECK:       }

// CHECK-LABEL: define void @test_4.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_4.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_4.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_4.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }

// CHECK-LABEL: define i64 @test_5.returnOffset()
// CHECK:         ret i64 0
// CHECK:       }

// CHECK-LABEL: define { i8*, i64 } @test_5.thunk(i8* nocapture writeonly 
// CHECK-SAME:      %[[VAL_0:.*]], i1 %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = bitcast i8* %[[VAL_0]] to i64*
// CHECK:         store i64 537892, i64* %[[VAL_2]], align 4
// CHECK:         %[[VAL_3:.*]] = getelementptr i8, i8* %[[VAL_0]], i64 8
// CHECK:         %[[VAL_4:.*]] = bitcast i8* %[[VAL_3]] to double*
// CHECK:         store double 0x40578DA858793DD9, double* %[[VAL_4]], align 8
// CHECK:         ret { i8*, i64 } zeroinitializer
// CHECK:       }

// CHECK-LABEL: define i64 @test_5.argsCreator(i8** nocapture readnone 
// CHECK-SAME:      %[[VAL_0:.*]], i8** nocapture writeonly
// CHECK-SAME:      %[[VAL_1:.*]]) #{{[0-9]+}} {
// CHECK:         %[[VAL_2:.*]] = tail call dereferenceable_or_null(16) i8* @malloc(i64 16)
// CHECK:         store i8* %[[VAL_2]], i8** %[[VAL_1]], align 8
// CHECK:         ret i64 16
// CHECK:       }

// CHECK-LABEL: define void @test_5.kernelRegFunc() {
// CHECK:         tail call void @cudaqRegisterKernelName(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_5.kernelName, i64 0, i64 0))
// CHECK:         tail call void @cudaqRegisterArgsCreator(i8* nonnull getelementptr inbounds ([7 x i8], [7 x i8]* @test_5.kernelName, i64 0, i64 0), i8* nonnull bitcast (i64 (i8**, i8**)* @test_5.argsCreator to i8*))
// CHECK:         ret void
// CHECK:       }
