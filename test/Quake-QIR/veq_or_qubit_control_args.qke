// ========================================================================== //
// Copyright (c) 2022 - 2023 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt %s | cudaq-translate | FileCheck %s

module attributes {quake.mangled_name_map = {__nvqpp__mlirgen__function_fancyCnot._Z9fancyCnotRN5cudaq5quditILm2EEES2_ = "_Z9fancyCnotRN5cudaq5quditILm2EEES2_", __nvqpp__mlirgen__function_toffoli._Z7toffoliv = "_Z7toffoliv"}} {
  func.func private @__nvqpp__mlirgen__function_fancyCnot._Z9fancyCnotRN5cudaq5quditILm2EEES2_.ctrl(%arg0: !quake.veq<?>, %arg1: !quake.ref, %arg2: !quake.ref) {
    quake.x [%arg0, %arg1] %arg2 : (!quake.veq<?>, !quake.ref, !quake.ref) -> ()
    return
  }
  func.func @__nvqpp__mlirgen__function_toffoli._Z7toffoliv() attributes {"cudaq-entrypoint", "cudaq-kernel", no_this} {
    %0 = quake.alloca !quake.veq<3>
    %1 = quake.extract_ref %0[0] : (!quake.veq<3>) -> !quake.ref
    %2 = quake.extract_ref %0[2] : (!quake.veq<3>) -> !quake.ref
    quake.x %1 : (!quake.ref) -> ()
    quake.x %2 : (!quake.ref) -> ()
    %3 = quake.extract_ref %0[1] : (!quake.veq<3>) -> !quake.ref
    %4 = quake.extract_ref %0[2] : (!quake.veq<3>) -> !quake.ref
    %5 = quake.concat %1 : (!quake.ref) -> !quake.veq<?>
    call @__nvqpp__mlirgen__function_fancyCnot._Z9fancyCnotRN5cudaq5quditILm2EEES2_.ctrl(%5, %3, %4) : (!quake.veq<?>, !quake.ref, !quake.ref) -> ()
    return
  }
}

// CHECK:         %[[VAL_0:.*]] = tail call
// CHECK:         %[[VAL_1:.*]]* @__quantum__rt__qubit_allocate_array(i64 3)
// CHECK:         %[[VAL_2:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 0)
// CHECK:         %[[VAL_3:.*]] = bitcast i8* %[[VAL_2]] to %[[VAL_4:.*]]**
// CHECK:         %[[VAL_5:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_3]], align 8
// CHECK:         %[[VAL_6:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 2)
// CHECK:         %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to %[[VAL_4]]**
// CHECK:         %[[VAL_8:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_7]], align 8
// CHECK:         tail call void @__quantum__qis__x(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         tail call void @__quantum__qis__x(%[[VAL_4]]* %[[VAL_8]])
// CHECK:         %[[VAL_9:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 1)
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to %[[VAL_4]]**
// CHECK:         %[[VAL_11:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_10]], align 8
// CHECK:         %[[VAL_12:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_13:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_12]], i64 0)
// CHECK:         %[[VAL_14:.*]] = bitcast i8* %[[VAL_13]] to %[[VAL_4]]**
// CHECK:         store %[[VAL_4]]* %[[VAL_5]], %[[VAL_4]]** %[[VAL_14]], align 8
// CHECK:         %[[VAL_15:.*]] = alloca [2 x i64], align 8
// CHECK:         %[[VAL_16:.*]] = getelementptr inbounds [2 x i64], [2 x i64]* %[[VAL_15]], i64 0, i64 0
// CHECK:         %[[VAL_17:.*]] = tail call i64 @__quantum__rt__array_get_size_1d(%[[VAL_1]]* %[[VAL_12]])
// CHECK:         store i64 %[[VAL_17]], i64* %[[VAL_16]], align 8
// CHECK:         %[[VAL_18:.*]] = getelementptr inbounds [2 x i64], [2 x i64]* %[[VAL_15]], i64 0, i64 1
// CHECK:         store i64 0, i64* %[[VAL_18]], align 8
// CHECK:         call void (i64, i64*, i64, void (%[[VAL_1]]*, %[[VAL_4]]*)*, ...) @invokeWithControlRegisterOrQubits(i64 2, i64* nonnull %[[VAL_16]], i64 1, void (%[[VAL_1]]*, %[[VAL_4]]*)* nonnull @__quantum__qis__x__ctl, %[[VAL_1]]* %[[VAL_12]], %[[VAL_4]]* %[[VAL_11]], %[[VAL_4]]* %[[VAL_8]])
// CHECK:         call void @__quantum__rt__qubit_release_array(%[[VAL_1]]* %[[VAL_0]])
// CHECK:         ret void
