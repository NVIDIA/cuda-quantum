// ========================================================================== //
// Copyright (c) 2022 - 2023 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt %s --quake-add-deallocs --canonicalize | cudaq-translate --convert-to=qir | FileCheck %s

module {
  func.func @adder_n4() {
// CHECK:    %[[VAL_0:.*]] = tail call
// CHECK-SAME:    %[[VAL_1:.*]]* @__quantum__rt__qubit_allocate_array(i64 4)
// CHECK:         %[[VAL_2:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 0)
// CHECK:         %[[VAL_3:.*]] = bitcast i8* %[[VAL_2]] to %[[VAL_4:.*]]**
// CHECK:         %[[VAL_5:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_3]], align 8
// CHECK:         tail call void @__quantum__qis__x(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         %[[VAL_6:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 1)
// CHECK:         %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to %[[VAL_4]]**
// CHECK:         %[[VAL_8:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_7]], align 8
// CHECK:         tail call void @__quantum__qis__x(%[[VAL_4]]* %[[VAL_8]])
// CHECK:         %[[VAL_9:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 3)
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to %[[VAL_4]]**
// CHECK:         %[[VAL_11:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_10]], align 8
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_4]]* %[[VAL_11]])
// CHECK:         %[[VAL_12:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 2)
// CHECK:         %[[VAL_13:.*]] = bitcast i8* %[[VAL_12]] to %[[VAL_4]]**
// CHECK:         %[[VAL_14:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_13]], align 8
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_14]], %Qubit* %[[VAL_11]])
// CHECK:         tail call void @__quantum__qis__t(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         tail call void @__quantum__qis__t(%[[VAL_4]]* %[[VAL_8]])
// CHECK:         tail call void @__quantum__qis__t(%[[VAL_4]]* %[[VAL_14]])
// CHECK:         tail call void @__quantum__qis__t__adj(%[[VAL_4]]* %[[VAL_11]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_5]], %Qubit* %[[VAL_8]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_14]], %Qubit* %[[VAL_11]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_11]], %Qubit* %[[VAL_5]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_8]], %Qubit* %[[VAL_14]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_5]], %Qubit* %[[VAL_8]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_14]], %Qubit* %[[VAL_11]])
// CHECK:         tail call void @__quantum__qis__t__adj(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         tail call void @__quantum__qis__t__adj(%[[VAL_4]]* %[[VAL_8]])
// CHECK:         tail call void @__quantum__qis__t__adj(%[[VAL_4]]* %[[VAL_14]])
// CHECK:         tail call void @__quantum__qis__t(%[[VAL_4]]* %[[VAL_11]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_5]], %Qubit* %[[VAL_8]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_14]], %Qubit* %[[VAL_11]])
// CHECK:         tail call void @__quantum__qis__s(%[[VAL_4]]* %[[VAL_11]])
// CHECK: tail call void (i64, void (%Array*, %Qubit*)*, ...) @invokeWithControlQubits(i64 1, void (%Array*, %Qubit*)* nonnull @__quantum__qis__x__ctl, %Qubit* %[[VAL_11]], %Qubit* %[[VAL_5]])
// CHECK:         tail call void @__quantum__qis__h(%[[VAL_4]]* %[[VAL_11]])
// CHECK:         %[[VAL_15:.*]] = tail call %Result* @__quantum__qis__mz(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         %[[VAL_17:.*]] = tail call %Result* @__quantum__qis__mz(%[[VAL_4]]* %[[VAL_8]])
// CHECK:         %[[VAL_18:.*]] = tail call %Result* @__quantum__qis__mz(%[[VAL_4]]* %[[VAL_14]])
// CHECK:         %[[VAL_19:.*]] = tail call %Result* @__quantum__qis__mz(%[[VAL_4]]* %[[VAL_11]])
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_1]]* %[[VAL_0]])
// CHECK:         ret void
    %0 = quake.alloca !quake.veq<4>
    %1 = memref.alloc() : memref<4xi1>
    %c0 = arith.constant 0 : index
    %2 = quake.extract_ref %0[%c0] : (!quake.veq<4>, index) -> !quake.ref
    quake.x %2 : (!quake.ref) -> ()
    %c1 = arith.constant 1 : index
    %3 = quake.extract_ref %0[%c1] : (!quake.veq<4>, index) -> !quake.ref
    quake.x %3 : (!quake.ref) -> ()
    %c3 = arith.constant 3 : index
    %4 = quake.extract_ref %0[%c3] : (!quake.veq<4>, index) -> !quake.ref
    quake.h %4 : (!quake.ref) -> ()
    %c2 = arith.constant 2 : index
    %5 = quake.extract_ref %0[%c2] : (!quake.veq<4>, index) -> !quake.ref
    quake.x [%5] %4 : (!quake.ref, !quake.ref) -> ()
    quake.t %2 : (!quake.ref) -> ()
    quake.t %3 : (!quake.ref) -> ()
    quake.t %5 : (!quake.ref) -> ()
    quake.t<adj> %4 : (!quake.ref) -> ()
    quake.x [%2] %3 : (!quake.ref, !quake.ref) -> ()
    quake.x [%5] %4 : (!quake.ref, !quake.ref) -> ()
    quake.x [%4] %2 : (!quake.ref, !quake.ref) -> ()
    quake.x [%3] %5 : (!quake.ref, !quake.ref) -> ()
    quake.x [%2] %3 : (!quake.ref, !quake.ref) -> ()
    quake.x [%5] %4 : (!quake.ref, !quake.ref) -> ()
    quake.t<adj> %2 : (!quake.ref) -> ()
    quake.t<adj> %3 : (!quake.ref) -> ()
    quake.t<adj> %5 : (!quake.ref) -> ()
    quake.t %4 : (!quake.ref) -> ()
    quake.x [%2] %3 : (!quake.ref, !quake.ref) -> ()
    quake.x [%5] %4 : (!quake.ref, !quake.ref) -> ()
    quake.s %4 : (!quake.ref) -> ()
    quake.x [%4] %2 : (!quake.ref, !quake.ref) -> ()
    quake.h %4 : (!quake.ref) -> ()
    %6 = quake.mz %2 : (!quake.ref) -> i1
    %c0_0 = arith.constant 0 : index
    memref.store %6, %1[%c0_0] : memref<4xi1>
    %7 = quake.mz %3 : (!quake.ref) -> i1
    %c1_1 = arith.constant 1 : index
    memref.store %7, %1[%c1_1] : memref<4xi1>
    %8 = quake.mz %5 : (!quake.ref) -> i1
    %c2_2 = arith.constant 2 : index
    memref.store %8, %1[%c2_2] : memref<4xi1>
    %9 = quake.mz %4 : (!quake.ref) -> i1
    %c3_3 = arith.constant 3 : index
    memref.store %9, %1[%c3_3] : memref<4xi1>
    return
  }
}
