// ========================================================================== //
// Copyright (c) 2022 - 2023 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt %s --canonicalize --quake-add-deallocs | cudaq-translate --convert-to=qir | FileCheck %s


module attributes {quake.mangled_name_map = {__nvqpp__mlirgen__Z4mainE3$_0 = "_ZZ4mainENK3$_0clEd"}} {
  func.func @__nvqpp__mlirgen__Z4mainE3$_0(%arg0: f64) attributes {"cudaq-entrypoint", "cudaq-kernel"} {
    %0 = cc.alloca f64
    cc.store %arg0, %0 : !cc.ptr<f64>
    %1 = quake.alloca !quake.veq<4>
    %2 = quake.extract_ref %1[0] : (!quake.veq<4>) -> !quake.ref
    quake.x %2 : (!quake.ref) -> ()
    %3 = quake.extract_ref %1[1] : (!quake.veq<4>) -> !quake.ref
    quake.x %3 : (!quake.ref) -> ()
    %4 = cc.load %0 : !cc.ptr<f64>
    %5 = cc.string_literal "XXXY" : !cc.ptr<!cc.array<i8 x 5>>
    %6 = quake.extract_ref %1[0] : (!quake.veq<4>) -> !quake.ref
    %7 = quake.extract_ref %1[1] : (!quake.veq<4>) -> !quake.ref
    %8 = quake.extract_ref %1[2] : (!quake.veq<4>) -> !quake.ref
    %9 = quake.extract_ref %1[3] : (!quake.veq<4>) -> !quake.ref
    %10 = quake.concat %6, %7, %8, %9 : (!quake.ref, !quake.ref, !quake.ref, !quake.ref) -> !quake.veq<4>
    quake.exp_pauli(%4) %10, %5 : (f64, !quake.veq<4>, !cc.ptr<!cc.array<i8 x 5>>) -> ()
    return
  }
}

// CHECK:    %[[VAL_0:.*]] = tail call
// CHECK:    %[[VAL_1:.*]]* @__quantum__rt__qubit_allocate_array(i64 4)
// CHECK:         %[[VAL_2:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 0)
// CHECK:         %[[VAL_3:.*]] = bitcast i8* %[[VAL_2]] to %[[VAL_4:.*]]**
// CHECK:         %[[VAL_5:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_3]], align 8
// CHECK:         tail call void @__quantum__qis__x(%[[VAL_4]]* %[[VAL_5]])
// CHECK:         %[[VAL_6:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 1)
// CHECK:         %[[VAL_7:.*]] = bitcast i8* %[[VAL_6]] to %[[VAL_4]]**
// CHECK:         %[[VAL_8:.*]] = load %[[VAL_4]]*, %[[VAL_4]]** %[[VAL_7]], align 8
// CHECK:         tail call void @__quantum__qis__x(%[[VAL_4]]* %[[VAL_8]])
// CHECK:         %[[VAL_9:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 2)
// CHECK:         %[[VAL_10:.*]] = bitcast i8* %[[VAL_9]] to i8**
// CHECK:         %[[VAL_11:.*]] = load i8*, i8** %[[VAL_10]], align 8
// CHECK:         %[[VAL_12:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_0]], i64 3)
// CHECK:         %[[VAL_13:.*]] = bitcast i8* %[[VAL_12]] to i8**
// CHECK:         %[[VAL_14:.*]] = load i8*, i8** %[[VAL_13]], align 8
// CHECK:         %[[VAL_15:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_16:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_15]], i64 0)
// CHECK:         %[[VAL_17:.*]] = bitcast i8* %[[VAL_16]] to %[[VAL_4]]**
// CHECK:         store %[[VAL_4]]* %[[VAL_5]], %[[VAL_4]]** %[[VAL_17]], align 8
// CHECK:         %[[VAL_18:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_19:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_18]], i64 0)
// CHECK:         %[[VAL_20:.*]] = bitcast i8* %[[VAL_19]] to %[[VAL_4]]**
// CHECK:         store %[[VAL_4]]* %[[VAL_8]], %[[VAL_4]]** %[[VAL_20]], align 8
// CHECK:         %[[VAL_21:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_concatenate(%[[VAL_1]]* %[[VAL_15]], %[[VAL_1]]* %[[VAL_18]])
// CHECK:         %[[VAL_22:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_23:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_22]], i64 0)
// CHECK:         %[[VAL_24:.*]] = bitcast i8* %[[VAL_23]] to i8**
// CHECK:         store i8* %[[VAL_11]], i8** %[[VAL_24]], align 8
// CHECK:         %[[VAL_25:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_concatenate(%[[VAL_1]]* %[[VAL_21]], %[[VAL_1]]* %[[VAL_22]])
// CHECK:         %[[VAL_26:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_create_1d(i32 8, i64 1)
// CHECK:         %[[VAL_27:.*]] = tail call i8* @__quantum__rt__array_get_element_ptr_1d(%[[VAL_1]]* %[[VAL_26]], i64 0)
// CHECK:         %[[VAL_28:.*]] = bitcast i8* %[[VAL_27]] to i8**
// CHECK:         store i8* %[[VAL_14]], i8** %[[VAL_28]], align 8
// CHECK:         %[[VAL_29:.*]] = tail call %[[VAL_1]]* @__quantum__rt__array_concatenate(%[[VAL_1]]* %[[VAL_25]], %[[VAL_1]]* %[[VAL_26]])
// CHECK:         tail call void @__quantum__qis__exp_pauli(double %[[VAL_30:.*]], %[[VAL_1]]* %[[VAL_29]], i8* nonnull getelementptr inbounds ([5 x i8], [5 x i8]* @cstr.5858585900, i64 0, i64 0))
// CHECK:         tail call void @__quantum__rt__qubit_release_array(%[[VAL_1]]* %[[VAL_0]])
// CHECK:         ret void
