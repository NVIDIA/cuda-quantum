// ========================================================================== //
// Copyright (c) 2022 - 2023 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt --lambda-lifting --canonicalize %s | FileCheck %s
// RUN: cudaq-opt --lambda-lifting --canonicalize %s | cudaq-translate --convert-to=qir | FileCheck --check-prefixes=QIR %s

// CHECK-LABEL:   func.func @__nvqpp__mlirgen__kernel_b(
// CHECK-SAME:        %[[VAL_0:.*]]: !cc.lambda<() -> ()>)
// CHECK:           return
// CHECK:         }

// CHECK-LABEL:   func.func @__nvqpp__mlirgen__kernel_a()
// CHECK:           %[[VAL_0:.*]] = quake.alloca !quake.veq<4>
// CHECK:           %[[VAL_2:.*]] = cc.instantiate_callable @__nvqpp__callable.thunk.lambda.0(%[[VAL_0]]) : (!quake.veq<4>) -> !cc.lambda<() -> ()>
// CHECK:           call @__nvqpp__mlirgen__kernel_b(%[[VAL_2]]) : (!cc.lambda<() -> ()>) -> ()
// CHECK:           return
// CHECK:         }

// CHECK-LABEL:   func.func private @__nvqpp__callable.thunk.lambda.0(
// CHECK-SAME:        %[[VAL_0:.*]]: !cc.lambda<() -> ()>) {
// CHECK:           %[[VAL_1:.*]] = cc.callable_closure %[[VAL_0]] : (!cc.lambda<() -> ()>) -> !quake.veq<4>
// CHECK:           call @__nvqpp__lifted.lambda.0(%[[VAL_1]]) : (!quake.veq<4>) -> ()
// CHECK:           return
// CHECK:         }

// CHECK-LABEL:   func.func private @__nvqpp__lifted.lambda.0(
// CHECK-SAME:        %[[VAL_0:.*]]: !quake.veq<4>) {
// CHECK:           %[[VAL_1:.*]] = arith.constant 1 : i32
// CHECK:           %[[VAL_2:.*]] = arith.constant 4 : i32
// CHECK:           %[[VAL_3:.*]] = arith.constant 0 : i32
// CHECK:           cc.scope {
// CHECK:             %[[VAL_4:.*]] = memref.alloca() : memref<i32>
// CHECK:             memref.store %[[VAL_3]], %[[VAL_4]][] : memref<i32>
// CHECK:             cc.loop while {
// CHECK:               %[[VAL_5:.*]] = memref.load %[[VAL_4]][] : memref<i32>
// CHECK:               %[[VAL_6:.*]] = arith.cmpi slt, %[[VAL_5]], %[[VAL_2]] : i32
// CHECK:               cc.condition %[[VAL_6]]
// CHECK:             } do {
// CHECK:               %[[VAL_7:.*]] = memref.load %[[VAL_4]][] : memref<i32>
// CHECK:               %[[VAL_8:.*]] = arith.extsi %[[VAL_7]] : i32 to i64
// CHECK:               %[[VAL_9:.*]] = quake.extract_ref %[[VAL_0]]{{\[}}%[[VAL_8]]] : (!quake.veq<4>, i64) -> !quake.ref
// CHECK:               quake.h %[[VAL_9]] :
// CHECK:               cc.continue
// CHECK:             } step {
// CHECK:               %[[VAL_10:.*]] = memref.load %[[VAL_4]][] : memref<i32>
// CHECK:               %[[VAL_11:.*]] = arith.addi %[[VAL_10]], %[[VAL_1]] : i32
// CHECK:               memref.store %[[VAL_11]], %[[VAL_4]][] : memref<i32>
// CHECK:             }
// CHECK:           }
// CHECK:           return
// CHECK:         }

// QIR-LABEL: define void @__nvqpp__mlirgen__kernel_b({ i8*, i8* } 

// QIR-LABEL: define void @__nvqpp__mlirgen__kernel_a()
// QIR:         call {{.*}} @__quantum__rt__qubit_allocate_array(i64 4)
// QIR:         call void @__quantum__rt__qubit_release_array(

// QIR-LABEL: define void @__nvqpp__lifted.lambda.0(
// QIR:         call i8* @__quantum__rt__array_get_element_ptr_1d(
// QIR:         call void @__quantum__qis__h(
// QIR:         call i8* @__quantum__rt__array_get_element_ptr_1d(
// QIR:         call void @__quantum__qis__h(
// QIR:         call i8* @__quantum__rt__array_get_element_ptr_1d(
// QIR:         call void @__quantum__qis__h(
// QIR:         call i8* @__quantum__rt__array_get_element_ptr_1d(
// QIR:         call void @__quantum__qis__h(

module attributes {qtx.mangled_name_map = {__nvqpp__mlirgen__kernel_a = "_ZN8kernel_aclEv", __nvqpp__mlirgen__kernel_b = "_ZN8kernel_bclEOSt8functionIFvvEE"}} {
  func.func @__nvqpp__mlirgen__kernel_b(%arg0: !cc.lambda<() -> ()>) attributes {"cudaq-entrypoint", "cudaq-kernel"} {
    return
  }
  func.func @__nvqpp__mlirgen__kernel_a() attributes {"cudaq-entrypoint", "cudaq-kernel"} {
    %1 = quake.alloca !quake.veq<4>
    %5 = cc.create_lambda {
      cc.scope {
        %alloca = memref.alloca() : memref<i32>
        cc.scope {
          %c0_i32 = arith.constant 0 : i32
          memref.store %c0_i32, %alloca[] : memref<i32>
          cc.loop while {
            %6 = memref.load %alloca[] : memref<i32>
            %c4_i32_0 = arith.constant 4 : i32
            %7 = arith.cmpi slt, %6, %c4_i32_0 : i32
            cc.condition %7
          } do {
            %6 = memref.load %alloca[] : memref<i32>
            %7 = arith.extsi %6 : i32 to i64
            %8 = quake.extract_ref %1[%7] : (!quake.veq<4>, i64) -> !quake.ref
            quake.h %8 : (!quake.ref) -> ()
            cc.continue
          } step {
            %6 = memref.load %alloca[] : memref<i32>
            %c1_i32 = arith.constant 1 : i32
            %7 = arith.addi %6, %c1_i32 : i32
            memref.store %7, %alloca[] : memref<i32>
          }
        }
      }
    } : !cc.lambda<() -> ()>
    call @__nvqpp__mlirgen__kernel_b(%5) : (!cc.lambda<() -> ()>) -> ()
    return
  }
}
