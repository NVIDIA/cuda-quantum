// ========================================================================== //
// Copyright (c) 2022 - 2025 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt -convert-to-qir-api %s | FileCheck %s

func.func @__nvqpp__mlirgen__kernel() attributes {"cudaq-entrypoint", "cudaq-kernel"} {
  %c0_i64 = arith.constant 0 : i64
  %c6_i64 = arith.constant 6 : i64
  %c1_i64 = arith.constant 1 : i64
  %c-1_i64 = arith.constant -1 : i64
  %c4_i64 = arith.constant 4 : i64
  %0 = quake.alloca !quake.veq<12>
  %1 = cc.alloca !cc.array<i64 x 1>
  %2 = cc.cast %1 : (!cc.ptr<!cc.array<i64 x 1>>) -> !cc.ptr<i64>
  cc.store %c6_i64, %2 : !cc.ptr<i64>
  %3 = cc.load %2 : !cc.ptr<i64>
  %4 = quake.extract_ref %0[4] : (!quake.veq<12>) -> !quake.ref
  quake.h %4 : (!quake.ref) -> ()
  %5 = arith.subi %c4_i64, %3 : i64
  %6 = arith.divsi %5, %c-1_i64 : i64
  %7 = arith.cmpi sgt, %6, %c0_i64 : i64
  %8 = arith.select %7, %6, %c0_i64 : i64
  cf.br ^bb1(%c0_i64 : i64)
^bb1(%9: i64):  // 2 preds: ^bb0, ^bb2
  %10 = arith.cmpi ne, %9, %8 : i64
  cf.cond_br %10, ^bb2(%9 : i64), ^bb3(%4 : !quake.ref)
^bb2(%11: i64):  // pred: ^bb1
  %12 = arith.muli %11, %c-1_i64 : i64
  %13 = arith.addi %3, %12 : i64
  %14 = arith.subi %13, %c1_i64 : i64
  %15 = quake.extract_ref %0[%14] : (!quake.veq<12>, i64) -> !quake.ref
  %16 = quake.extract_ref %0[%13] : (!quake.veq<12>, i64) -> !quake.ref
  quake.x [%15] %16 : (!quake.ref, !quake.ref) -> ()
  %17 = arith.addi %11, %c1_i64 : i64
  cf.br ^bb1(%17 : i64)
^bb3(%18: !quake.ref):  // pred: ^bb1
  %19 = quake.extract_ref %0[2] : (!quake.veq<12>) -> !quake.ref
  quake.x [%19] %18 : (!quake.ref, !quake.ref) -> ()
  quake.dealloc %0 : !quake.veq<12>
  return
}

// CHECK-LABEL:   func.func @__nvqpp__mlirgen__kernel() attributes {"cudaq-entrypoint", "cudaq-kernel", "qir-api"} {
// CHECK-DAG:       %[[VAL_0:.*]] = arith.constant 2 : i64
// CHECK-DAG:       %[[VAL_1:.*]] = constant @__quantum__qis__x__ctl : (!cc.ptr<!llvm.struct<"Array", opaque>>, !cc.ptr<!llvm.struct<"Qubit", opaque>>) -> ()
// CHECK-DAG:       %[[VAL_2:.*]] = arith.constant 0 : i64
// CHECK-DAG:       %[[VAL_3:.*]] = arith.constant 6 : i64
// CHECK-DAG:       %[[VAL_4:.*]] = arith.constant 1 : i64
// CHECK-DAG:       %[[VAL_5:.*]] = arith.constant -1 : i64
// CHECK-DAG:       %[[VAL_6:.*]] = arith.constant 4 : i64
// CHECK-DAG:       %[[VAL_7:.*]] = arith.constant 12 : i64
// CHECK:           %[[VAL_8:.*]] = call @__quantum__rt__qubit_allocate_array(%[[VAL_7]]) : (i64) -> !cc.ptr<!llvm.struct<"Array", opaque>>
// CHECK:           %[[VAL_9:.*]] = cc.alloca !cc.array<i64 x 1>
// CHECK:           %[[VAL_10:.*]] = cc.cast %[[VAL_9]] : (!cc.ptr<!cc.array<i64 x 1>>) -> !cc.ptr<i64>
// CHECK:           cc.store %[[VAL_3]], %[[VAL_10]] : !cc.ptr<i64>
// CHECK:           %[[VAL_11:.*]] = cc.load %[[VAL_10]] : !cc.ptr<i64>
// CHECK:           %[[VAL_12:.*]] = call @__quantum__rt__array_get_element_ptr_1d(%[[VAL_8]], %[[VAL_6]]) : (!cc.ptr<!llvm.struct<"Array", opaque>>, i64) -> !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_13:.*]] = cc.load %[[VAL_12]] : !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           call @__quantum__qis__h(%[[VAL_13]]) : (!cc.ptr<!llvm.struct<"Qubit", opaque>>) -> ()
// CHECK:           %[[VAL_14:.*]] = arith.subi %[[VAL_6]], %[[VAL_11]] : i64
// CHECK:           %[[VAL_15:.*]] = arith.divsi %[[VAL_14]], %[[VAL_5]] : i64
// CHECK:           %[[VAL_16:.*]] = arith.cmpi sgt, %[[VAL_15]], %[[VAL_2]] : i64
// CHECK:           %[[VAL_17:.*]] = arith.select %[[VAL_16]], %[[VAL_15]], %[[VAL_2]] : i64
// CHECK:           cf.br ^bb1(%[[VAL_2]] : i64)
// CHECK:         ^bb1(%[[VAL_18:.*]]: i64):
// CHECK:           %[[VAL_19:.*]] = arith.cmpi ne, %[[VAL_18]], %[[VAL_17]] : i64
// CHECK:           cf.cond_br %[[VAL_19]], ^bb2(%[[VAL_18]] : i64), ^bb3(%[[VAL_13]] : !cc.ptr<!llvm.struct<"Qubit", opaque>>)
// CHECK:         ^bb2(%[[VAL_20:.*]]: i64):
// CHECK:           %[[VAL_21:.*]] = arith.muli %[[VAL_20]], %[[VAL_5]] : i64
// CHECK:           %[[VAL_22:.*]] = arith.addi %[[VAL_11]], %[[VAL_21]] : i64
// CHECK:           %[[VAL_23:.*]] = arith.subi %[[VAL_22]], %[[VAL_4]] : i64
// CHECK:           %[[VAL_24:.*]] = call @__quantum__rt__array_get_element_ptr_1d(%[[VAL_8]], %[[VAL_23]]) : (!cc.ptr<!llvm.struct<"Array", opaque>>, i64) -> !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_25:.*]] = cc.load %[[VAL_24]] : !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_26:.*]] = call @__quantum__rt__array_get_element_ptr_1d(%[[VAL_8]], %[[VAL_22]]) : (!cc.ptr<!llvm.struct<"Array", opaque>>, i64) -> !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_27:.*]] = cc.load %[[VAL_26]] : !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_28:.*]] = cc.cast %[[VAL_25]] : (!cc.ptr<!llvm.struct<"Qubit", opaque>>) -> !llvm.ptr<i8>
// CHECK:           %[[VAL_29:.*]] = cc.func_ptr %[[VAL_1]] : ((!cc.ptr<!llvm.struct<"Array", opaque>>, !cc.ptr<!llvm.struct<"Qubit", opaque>>) -> ()) -> !llvm.ptr<i8>
// CHECK:           %[[VAL_30:.*]] = cc.cast %[[VAL_27]] : (!cc.ptr<!llvm.struct<"Qubit", opaque>>) -> !llvm.ptr<i8>
// CHECK:           cc.call_vararg @generalizedInvokeWithRotationsControlsTargets(%[[VAL_2]], %[[VAL_2]], %[[VAL_4]], %[[VAL_4]], %[[VAL_29]], %[[VAL_28]], %[[VAL_30]]) : (i64, i64, i64, i64, !llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<i8>) -> ()
// CHECK:           %[[VAL_31:.*]] = arith.addi %[[VAL_20]], %[[VAL_4]] : i64
// CHECK:           cf.br ^bb1(%[[VAL_31]] : i64)
// CHECK:         ^bb3(%[[VAL_32:.*]]: !cc.ptr<!llvm.struct<"Qubit", opaque>>):
// CHECK:           %[[VAL_33:.*]] = call @__quantum__rt__array_get_element_ptr_1d(%[[VAL_8]], %[[VAL_0]]) : (!cc.ptr<!llvm.struct<"Array", opaque>>, i64) -> !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_34:.*]] = cc.load %[[VAL_33]] : !cc.ptr<!cc.ptr<!llvm.struct<"Qubit", opaque>>>
// CHECK:           %[[VAL_35:.*]] = cc.cast %[[VAL_34]] : (!cc.ptr<!llvm.struct<"Qubit", opaque>>) -> !llvm.ptr<i8>
// CHECK:           %[[VAL_36:.*]] = cc.func_ptr %[[VAL_1]] : ((!cc.ptr<!llvm.struct<"Array", opaque>>, !cc.ptr<!llvm.struct<"Qubit", opaque>>) -> ()) -> !llvm.ptr<i8>
// CHECK:           %[[VAL_37:.*]] = cc.cast %[[VAL_32]] : (!cc.ptr<!llvm.struct<"Qubit", opaque>>) -> !llvm.ptr<i8>
// CHECK:           cc.call_vararg @generalizedInvokeWithRotationsControlsTargets(%[[VAL_2]], %[[VAL_2]], %[[VAL_4]], %[[VAL_4]], %[[VAL_36]], %[[VAL_35]], %[[VAL_37]]) : (i64, i64, i64, i64, !llvm.ptr<i8>, !llvm.ptr<i8>, !llvm.ptr<i8>) -> ()
// CHECK:           call @__quantum__rt__qubit_release_array(%[[VAL_8]]) : (!cc.ptr<!llvm.struct<"Array", opaque>>) -> ()
// CHECK:           return
// CHECK:         }
