// ========================================================================== //
// Copyright (c) 2022 - 2023 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// CUDA-Q code
// struct iqft {
//   void operator()(cudaq::qreg q) __qpu__ {
//     int N = q.size();
//     // Swap qubits
//     for (int i = 0; i < N / 2; ++i) {
//       swap(q[i], q[N - i - 1]);
//     }

//     for (int i = 0; i < N - 1; ++i) {
//       h(q[i]);
//       int j = i + 1;
//       for (int y = i; y >= 0; --y) { // for (int y = -i; y < 1; y++)
//         const double theta = -M_PI / std::pow(2.0, j - y);
//         cphase(theta, q[j], q[y]);
//       }
//     }

//     h(q[N - 1]);
//   }
// };

// RUN: cudaq-opt %s --canonicalize | FileCheck %s

// CHECK: #map = affine_map<(d0) -> (-d0)>
// CHECK: module {
// CHECK:   func.func @iqft(%arg0: !quake.veq<?>) {
// CHECK:     %[[CF0:.*]] = arith.constant 2.000000e+00 : f64
// CHECK:     %[[CF1:.*]] = arith.constant -3.1415926535897931 : f64
// CHECK:     %[[CI1:.*]] = arith.constant 1 : index
// CHECK:     %[[CI0:.*]] = arith.constant 0 : index
// CHECK:     %[[C1:.*]] = arith.constant 1 : i32
// CHECK:     %[[C2:.*]] = arith.constant 2 : i32
// CHECK:     %c-1_i32 = arith.constant -1 : i32
// CHECK:     %0 = quake.veq_size %arg0 : (!quake.veq<?>) -> i64
// CHECK:     %1 = arith.trunci %0 : i64 to i32
// CHECK:     %2 = arith.subi %1, %[[C1]] : i32
// CHECK:     %3 = arith.index_cast %2 : i32 to index
// CHECK:     %4 = arith.divsi %1, %[[C2]] : i32
// CHECK:     %5 = arith.index_cast %4 : i32 to index
// CHECK:     scf.for %arg1 = %[[CI0]] to %5 step %[[CI1]] {
// CHECK:       %7 = arith.index_cast %arg1 : index to i32
// CHECK:       %8 = arith.subi %1, %7 : i32
// CHECK:       %9 = arith.subi %8, %[[C1]] : i32
// CHECK:       %10 = quake.extract_ref %arg0[%7] : (!quake.veq<?>, i32) -> !quake.ref
// CHECK:       %11 = quake.extract_ref %arg0[%9] : (!quake.veq<?>, i32) -> !quake.ref
// CHECK:       quake.swap %10, %11 : (!quake.ref, !quake.ref) -> ()
// CHECK:     }
// CHECK:     affine.for %arg1 = 0 to %3 {
// CHECK:       %7 = arith.index_cast %arg1 : index to i32
// CHECK:       %8 = quake.extract_ref %arg0[%7] : (!quake.veq<?>, i32) -> !quake.ref
// CHECK:       quake.h %8 : (!quake.ref) -> ()
// CHECK:       %9 = arith.addi %7, %[[C1]] : i32
// CHECK:       affine.for %arg2 = #map(%arg1) to 1 {
// CHECK:         %10 = arith.index_cast %arg2 : index to i32
// CHECK:         %11 = arith.muli %10, %c-1_i32 : i32
// CHECK:         %12 = arith.subi %9, %11 : i32
// CHECK:         %13 = arith.sitofp %12 : i32 to f64
// CHECK:         %14 = math.powf %[[CF0]], %13 : f64
// CHECK:         %15 = arith.divf %[[CF1]], %14 : f64
// CHECK:         %16 = quake.extract_ref %arg0[%9] : (!quake.veq<?>, i32) -> !quake.ref
// CHECK:         %17 = quake.extract_ref %arg0[%11] : (!quake.veq<?>, i32) -> !quake.ref
// CHECK:         quake.r1 (%15) [%16] %17 : (f64, !quake.ref, !quake.ref) -> ()
// CHECK:       }
// CHECK:     }
// CHECK:     %6 = quake.extract_ref %arg0[%2] : (!quake.veq<?>, i32) -> !quake.ref
// CHECK:     quake.h %6 : (!quake.ref) -> ()
// CHECK:     return
// CHECK:   }
// CHECK: }


#lb = affine_map<(d0) -> (-1*d0)>

module {
    func.func @iqft(%arg0 : !quake.veq<?>) {
        %c1 = arith.constant 1 : i32
        %c0 = arith.constant 0 : i32
        %c2 = arith.constant 2 : i32
        %cn1 = arith.constant -1 : i32
        %nn = quake.veq_size %arg0 : (!quake.veq<?>) -> i64
        %n = arith.trunci %nn : i64 to i32
        %nm1 = arith.subi %n, %c1 : i32
        %nm1idx = arith.index_cast %nm1 : i32 to index
        %upper = arith.divsi %n, %c2 : i32
        %upper_cast = arith.index_cast %upper : i32 to index
        %lower = arith.index_cast %c0 : i32 to index
        %c1idx = arith.index_cast %c1 : i32 to index

        scf.for %arg2 = %lower to %upper_cast step %c1idx {
            %7 = arith.index_cast %arg2 : index to i32
            %9 = arith.subi %n, %7 : i32
            %10 = arith.subi %9, %c1 : i32
            %qi = quake.extract_ref %arg0 [%7] : (!quake.veq<?>,i32) -> !quake.ref
            %qi1 = quake.extract_ref %arg0 [%10] : (!quake.veq<?>,i32) -> !quake.ref
            quake.swap %qi, %qi1 : (!quake.ref, !quake.ref) -> ()
        }

        affine.for %arg3 = 0 to %nm1idx {
            %11 = arith.index_cast %arg3 : index to i32
            %qi = quake.extract_ref %arg0[%11] : (!quake.veq<?>, i32) -> !quake.ref
            quake.h %qi : (!quake.ref) -> ()
            %13 = arith.addi %11, %c1 : i32
            %12 = memref.alloca() : memref<i32>
            memref.store %13, %12[] : memref<i32>

            %lb = arith.muli %11, %cn1 : i32
            %lbidx = arith.index_cast %lb : i32 to index
            affine.for %arg4 = #lb(%arg3) to %c1idx {
                %14 = arith.index_cast %arg4 : index to i32
                %15 = arith.muli %14, %cn1 : i32
                %cst = arith.constant 3.1415926535897931 : f64
                %cst_3 = arith.constant -1.000000e+00 : f64
                %16 = arith.mulf %cst_3, %cst : f64
                %c2f = arith.sitofp %c2 : i32 to f64
                %jmy = arith.subi %13, %15 : i32
                %s2f = arith.sitofp %jmy : i32 to f64
                %denom = math.powf %c2f, %s2f : f64
                %24 = arith.divf %16, %denom : f64
                %qj = quake.extract_ref %arg0[%13] : (!quake.veq<?>, i32) -> !quake.ref
                %qy = quake.extract_ref %arg0[%15] : (!quake.veq<?>, i32) -> !quake.ref
                quake.r1 (%24)[%qj] %qy : (f64,!quake.ref,!quake.ref) -> ()
            }
        }
        %qnm1 = quake.extract_ref %arg0[%nm1] : (!quake.veq<?>,i32) -> !quake.ref
        quake.h %qnm1 : (!quake.ref) -> ()
        return
    }
}

