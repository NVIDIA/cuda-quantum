// ========================================================================== //
// Copyright (c) 2022 - 2024 NVIDIA Corporation & Affiliates.                 //
// All rights reserved.                                                       //
//                                                                            //
// This source code and the accompanying materials are made available under   //
// the terms of the Apache License 2.0 which accompanies this distribution.   //
// ========================================================================== //

// RUN: cudaq-opt --kernel-execution=codegen=1 %s | FileCheck %s
// RUN: cudaq-opt --kernel-execution=codegen=2 %s | FileCheck --check-prefix=STREAM %s
// RUN: cudaq-opt --kernel-execution %s | FileCheck --check-prefix=HYBRID %s

module attributes {quake.mangled_name_map = {
  __nvqpp__mlirgen__ghz = "_ZN3ghzclEi"}} {

// CHECK-LABEL:   func.func @__nvqpp__mlirgen__ghz(

  func.func @__nvqpp__mlirgen__ghz(%arg0: i32) -> f64 {
    %0 = cc.alloca i32
    cc.store %arg0, %0 : !cc.ptr<i32>
    %1 = cc.load %0 : !cc.ptr<i32>
    %2 = arith.extsi %1 : i32 to i64
    %3 = quake.alloca !quake.veq<?>[%2 : i64]
    %c0_i32 = arith.constant 0 : i32
    %4 = arith.extsi %c0_i32 : i32 to i64
    %5 = quake.extract_ref %3[%4] : (!quake.veq<?>,i64) -> !quake.ref
    quake.h %5 : (!quake.ref) -> ()
    cc.scope {
      %7 = cc.alloca i32
      cc.store %c0_i32, %7 : !cc.ptr<i32>
      %8 = cc.load %7 : !cc.ptr<i32>
      %9 = cc.load %0 : !cc.ptr<i32>
      %c1_i32 = arith.constant 1 : i32
      %10 = arith.subi %9, %c1_i32 : i32
      %11 = arith.index_cast %10 : i32 to index
      %12 = cc.load %7 : !cc.ptr<i32>
      %13 = arith.index_cast %12 : i32 to index
      cc.loop while ((%arg1 = %13) -> index) {
        %cond = arith.cmpi slt, %arg1, %11 : index
	cc.condition %cond (%arg1 : index)
      } do {
      ^bb1 (%arg11 : index):
        %18 = arith.index_cast %arg11 : index to i64
        %19 = quake.extract_ref %3[%18] : (!quake.veq<?>,i64) -> !quake.ref
        %20 = arith.trunci %18 : i64 to i32
        %21 = arith.addi %20, %c1_i32 : i32
        %22 = arith.extsi %21 : i32 to i64
        %23 = quake.extract_ref %3[%22] : (!quake.veq<?>,i64) -> !quake.ref
        quake.x [%19] %23 : (!quake.ref, !quake.ref) -> ()
	cc.continue %arg11 : index
      } step {
      ^bb2 (%arg21 : index):
        %c1_index = arith.constant 1 : index
        %incr = arith.addi %arg21, %c1_index : index
        cc.continue %incr : index
      }
    }
    %15 = quake.veq_size %3 : (!quake.veq<?>) -> i64
    %16 = arith.index_cast %15 : i64 to index
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1 : index
    cc.loop while ((%argx1 = %c0) -> index) {
        %cond = arith.cmpi slt, %argx1, %16 : index
	cc.condition %cond (%argx1 : index)
    } do {
    ^bb1 (%arg31 : index):
      %18 = quake.extract_ref %3[%arg31] : (!quake.veq<?>,index) -> !quake.ref
      %19 = quake.mz %18 : (!quake.ref) -> !quake.measure
      cc.continue %arg31 : index
    } step {
    ^bb1 (%arg32 : index):
      %c1_index = arith.constant 1 : index
      %incr.2 = arith.addi %arg32, %c1_index : index
      cc.continue %incr.2 : index
    }
    %cst = arith.constant 1.000000e+00 : f64
    return %cst : f64
  }

  func.func @_ZN3ghzclEi(%0: !cc.ptr<i8>, %1: i32) -> f64 {
    %2 = cc.undef f64
    return %2 : f64
  }
}

// Check the generated code.

// CHECK-LABEL:   func.func @_ZN3ghzclEi(
// CHECK-SAME:      %[[VAL_0:.*]]: !cc.ptr<i8>, %[[VAL_1:.*]]: i32) -> f64 {
// CHECK-DAG:       %[[VAL_2:.*]] = cc.undef !cc.struct<{i32, f64}>
// CHECK-DAG:       %[[VAL_3:.*]] = arith.constant 0 : i64
// CHECK:           %[[VAL_4:.*]] = cc.insert_value %[[VAL_1]], %[[VAL_2]][0] : (!cc.struct<{i32, f64}>, i32) -> !cc.struct<{i32, f64}>
// CHECK:           %[[VAL_7:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// CHECK:           %[[VAL_8:.*]] = arith.addi %[[VAL_7]], %[[VAL_3]] : i64
// CHECK:           %[[VAL_9:.*]] = cc.alloca i8[%[[VAL_8]] : i64]
// CHECK:           %[[VAL_10:.*]] = cc.cast %[[VAL_9]] : (!cc.ptr<!cc.array<i8 x ?>>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// CHECK:           cc.store %[[VAL_4]], %[[VAL_10]] : !cc.ptr<!cc.struct<{i32, f64}>>
// CHECK:           %[[VAL_11:.*]] = cc.cast %[[VAL_9]] : (!cc.ptr<!cc.array<i8 x ?>>) -> !cc.ptr<!cc.array<!cc.struct<{i32, f64}> x ?>>
// CHECK:           %[[VAL_13:.*]] = constant @ghz.thunk : (!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>
// CHECK:           %[[VAL_15:.*]] = cc.func_ptr %[[VAL_13]] : ((!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>) -> !cc.ptr<i8>
// CHECK:           %[[VAL_16:.*]] = cc.cast %[[VAL_11]] : (!cc.ptr<!cc.array<!cc.struct<{i32, f64}> x ?>>) -> !cc.ptr<i8>
// CHECK:           %[[VAL_18:.*]] = cc.offsetof !cc.struct<{i32, f64}> [1] : i64
// CHECK:           %[[VAL_12:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// CHECK:           %[[VAL_14:.*]] = cc.cast %[[VAL_12]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// CHECK:           call @altLaunchKernel(%[[VAL_14]], %[[VAL_15]], %[[VAL_16]], %[[VAL_8]], %[[VAL_18]]) : (!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64) -> ()
// CHECK:           %[[VAL_19:.*]] = cc.compute_ptr %[[VAL_11]][0, 1] : (!cc.ptr<!cc.array<!cc.struct<{i32, f64}> x ?>>) -> !cc.ptr<f64>
// CHECK:           %[[VAL_20:.*]] = cc.load %[[VAL_19]] : !cc.ptr<f64>
// CHECK:           return %[[VAL_20]] : f64
// CHECK:         }

// CHECK:         func.func private @altLaunchKernel(!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64)
// CHECK:         func.func private @cudaqRegisterKernelName(!cc.ptr<i8>)

// CHECK:         llvm.mlir.global external constant @ghz.kernelName("ghz\00") {addr_space = 0 : i32}

// CHECK-LABEL:   func.func @ghz.thunk(
// CHECK-SAME:                         %[[VAL_0:.*]]: !cc.ptr<i8>,
// CHECK-SAME:                         %[[VAL_1:.*]]: i1) -> !cc.struct<{!cc.ptr<i8>, i64}> {
// CHECK:           %[[VAL_2:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// CHECK:           %[[VAL_3:.*]] = cc.load %[[VAL_2]] : !cc.ptr<!cc.struct<{i32, f64}>>
// CHECK:           %[[VAL_7:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// CHECK:           %[[VAL_20:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.array<i8 x ?>>
// CHECK:           %[[VAL_8:.*]] = cc.compute_ptr %[[VAL_20]][%[[VAL_7]]] : (!cc.ptr<!cc.array<i8 x ?>>, i64) -> !cc.ptr<i8>
// CHECK:           %[[VAL_9:.*]] = cc.extract_value %[[VAL_3]][0] : (!cc.struct<{i32, f64}>) -> i32
// CHECK:           %[[VAL_10:.*]] = call @__nvqpp__mlirgen__ghz(%[[VAL_9]]) : (i32) -> f64
// CHECK:           %[[VAL_11:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{i32, f64}>>) -> !cc.ptr<f64>
// CHECK:           cc.store %[[VAL_10]], %[[VAL_11]] : !cc.ptr<f64>
// CHECK:           %[[VAL_12:.*]] = call @__nvqpp_zeroDynamicResult() : () -> !cc.struct<{!cc.ptr<i8>, i64}>
// CHECK:           return %[[VAL_12]] : !cc.struct<{!cc.ptr<i8>, i64}>
// CHECK:         }

// CHECK-LABEL:   func.func @ghz.argsCreator(
// CHECK-SAME:      %[[VAL_0:.*]]: !cc.ptr<!cc.ptr<i8>>,
// CHECK-SAME:      %[[VAL_1:.*]]: !cc.ptr<!cc.ptr<i8>>) -> i64 {
// CHECK:           %[[VAL_2:.*]] = cc.undef !cc.struct<{i32, f64}>
// CHECK:           %[[VAL_14:.*]] = cc.cast %[[VAL_0]] : (!cc.ptr<!cc.ptr<i8>>) -> !cc.ptr<!cc.array<!cc.ptr<i8> x ?>>
// CHECK:           %[[VAL_3:.*]] = arith.constant 0 : i64
// CHECK:           %[[VAL_4:.*]] = cc.compute_ptr %[[VAL_14]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x ?>>) -> !cc.ptr<!cc.ptr<i8>>
// CHECK:           %[[VAL_5:.*]] = cc.load %[[VAL_4]] : !cc.ptr<!cc.ptr<i8>>
// CHECK:           %[[VAL_6:.*]] = cc.cast %[[VAL_5]] : (!cc.ptr<i8>) -> !cc.ptr<i32>
// CHECK:           %[[VAL_7:.*]] = cc.load %[[VAL_6]] : !cc.ptr<i32>
// CHECK:           %[[VAL_8:.*]] = cc.insert_value %[[VAL_7]], %[[VAL_2]][0] : (!cc.struct<{i32, f64}>, i32) -> !cc.struct<{i32, f64}>
// CHECK:           %[[VAL_11:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// CHECK:           %[[VAL_12:.*]] = call @malloc(%[[VAL_11]]) : (i64) -> !cc.ptr<i8>
// CHECK:           %[[VAL_13:.*]] = cc.cast %[[VAL_12]] : (!cc.ptr<i8>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// CHECK:           cc.store %[[VAL_8]], %[[VAL_13]] : !cc.ptr<!cc.struct<{i32, f64}>>
// CHECK:           cc.store %[[VAL_12]], %[[VAL_1]] : !cc.ptr<!cc.ptr<i8>>
// CHECK:           return %[[VAL_11]] : i64
// CHECK:         }

// CHECK-LABEL:   llvm.func @ghz.kernelRegFunc() {
// CHECK:           %[[VAL_0:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// CHECK:           %[[VAL_1:.*]] = cc.cast %[[VAL_0]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// CHECK:           func.call @cudaqRegisterKernelName(%[[VAL_1]]) : (!cc.ptr<i8>) -> ()
// CHECK:           %[[VAL_2:.*]] = func.constant @ghz.argsCreator : (!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>) -> i64
// CHECK:           %[[VAL_3:.*]] = cc.func_ptr %[[VAL_2]] : ((!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>) -> i64) -> !cc.ptr<i8>
// CHECK:           func.call @cudaqRegisterArgsCreator(%[[VAL_1]], %[[VAL_3]]) : (!cc.ptr<i8>, !cc.ptr<i8>) -> ()
// CHECK:           llvm.return
// CHECK:         }


// STREAM-LABEL:   func.func @_ZN3ghzclEi(
// STREAM-SAME:      %[[VAL_0:.*]]: !cc.ptr<i8>, %[[VAL_1:.*]]: i32) -> f64 {
// STREAM:           %[[VAL_2:.*]] = cc.alloca !cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>
// STREAM:           %[[VAL_3:.*]] = cc.alloca !cc.array<!cc.ptr<i8> x 1>
// STREAM:           %[[VAL_4:.*]] = cc.sizeof !cc.array<!cc.ptr<i8> x 1> : i64
// STREAM:           %[[VAL_5:.*]] = cc.cast %[[VAL_3]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// STREAM:           %[[VAL_6:.*]] = cc.cast %[[VAL_2]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAM:           cc.store %[[VAL_5]], %[[VAL_6]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAM:           %[[VAL_7:.*]] = cc.cast %[[VAL_3]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> i64
// STREAM:           %[[VAL_8:.*]] = arith.addi %[[VAL_7]], %[[VAL_4]] : i64
// STREAM:           %[[VAL_9:.*]] = cc.cast %[[VAL_8]] : (i64) -> !cc.ptr<!cc.ptr<i8>>
// STREAM:           %[[VAL_10:.*]] = cc.compute_ptr %[[VAL_2]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAM:           cc.store %[[VAL_9]], %[[VAL_10]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAM:           %[[VAL_11:.*]] = cc.compute_ptr %[[VAL_2]][2] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAM:           cc.store %[[VAL_9]], %[[VAL_11]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// STREAM:           %[[VAL_14:.*]] = cc.compute_ptr %[[VAL_3]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// STREAM:           %[[VAL_15:.*]] = cc.alloca i32
// STREAM:           cc.store %[[VAL_1]], %[[VAL_15]] : !cc.ptr<i32>
// STREAM:           %[[VAL_16:.*]] = cc.cast %[[VAL_15]] : (!cc.ptr<i32>) -> !cc.ptr<i8>
// STREAM:           cc.store %[[VAL_16]], %[[VAL_14]] : !cc.ptr<!cc.ptr<i8>>
// STREAM:           %[[VAL_19:.*]] = cc.cast %[[VAL_2]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<i8>
// STREAM:           %[[VAL_20:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// STREAM:           %[[VAL_21:.*]] = cc.cast %[[VAL_20]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// STREAM:           call @streamlinedLaunchKernel(%[[VAL_21]], %[[VAL_19]]) : (!cc.ptr<i8>, !cc.ptr<i8>) -> ()
// STREAM:           %[[VAL_22:.*]] = cc.undef f64
// STREAM:           return %[[VAL_22]] : f64
// STREAM:         }

// HYBRID-LABEL:   func.func @_ZN3ghzclEi(
// HYBRID-SAME:      %[[VAL_0:.*]]: !cc.ptr<i8>, %[[VAL_1:.*]]: i32) -> f64 {
// HYBRID:           %[[VAL_2:.*]] = cc.undef !cc.struct<{i32, f64}>
// HYBRID:           %[[VAL_3:.*]] = arith.constant 0 : i64
// HYBRID:           %[[VAL_4:.*]] = cc.insert_value %[[VAL_1]], %[[VAL_2]][0] : (!cc.struct<{i32, f64}>, i32) -> !cc.struct<{i32, f64}>
// HYBRID:           %[[VAL_6:.*]] = cc.sizeof !cc.struct<{i32, f64}> : i64
// HYBRID:           %[[VAL_7:.*]] = arith.addi %[[VAL_6]], %[[VAL_3]] : i64
// HYBRID:           %[[VAL_8:.*]] = cc.alloca i8{{\[}}%[[VAL_7]] : i64]
// HYBRID:           %[[VAL_9:.*]] = cc.cast %[[VAL_8]] : (!cc.ptr<!cc.array<i8 x ?>>) -> !cc.ptr<!cc.struct<{i32, f64}>>
// HYBRID:           cc.store %[[VAL_4]], %[[VAL_9]] : !cc.ptr<!cc.struct<{i32, f64}>>
// HYBRID:           %[[VAL_10:.*]] = cc.cast %[[VAL_8]] : (!cc.ptr<!cc.array<i8 x ?>>) -> !cc.ptr<!cc.array<!cc.struct<{i32, f64}> x ?>>
// HYBRID:           %[[VAL_11:.*]] = constant @ghz.thunk : (!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>
// HYBRID:           %[[VAL_12:.*]] = cc.func_ptr %[[VAL_11]] : ((!cc.ptr<i8>, i1) -> !cc.struct<{!cc.ptr<i8>, i64}>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_13:.*]] = cc.cast %[[VAL_10]] : (!cc.ptr<!cc.array<!cc.struct<{i32, f64}> x ?>>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_15:.*]] = cc.offsetof !cc.struct<{i32, f64}> [1] : i64
// HYBRID:           %[[VAL_16:.*]] = cc.alloca !cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>
// HYBRID:           %[[VAL_17:.*]] = cc.alloca !cc.array<!cc.ptr<i8> x 1>
// HYBRID:           %[[VAL_18:.*]] = cc.sizeof !cc.array<!cc.ptr<i8> x 1> : i64
// HYBRID:           %[[VAL_19:.*]] = cc.cast %[[VAL_17]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_20:.*]] = cc.cast %[[VAL_16]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           cc.store %[[VAL_19]], %[[VAL_20]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           %[[VAL_21:.*]] = cc.cast %[[VAL_17]] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> i64
// HYBRID:           %[[VAL_22:.*]] = arith.addi %[[VAL_21]], %[[VAL_18]] : i64
// HYBRID:           %[[VAL_23:.*]] = cc.cast %[[VAL_22]] : (i64) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_24:.*]] = cc.compute_ptr %[[VAL_16]][1] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           cc.store %[[VAL_23]], %[[VAL_24]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           %[[VAL_25:.*]] = cc.compute_ptr %[[VAL_16]][2] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           cc.store %[[VAL_23]], %[[VAL_25]] : !cc.ptr<!cc.ptr<!cc.ptr<i8>>>
// HYBRID:           %[[VAL_26:.*]] = cc.compute_ptr %[[VAL_17]][0] : (!cc.ptr<!cc.array<!cc.ptr<i8> x 1>>) -> !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_27:.*]] = cc.alloca i32
// HYBRID:           cc.store %[[VAL_1]], %[[VAL_27]] : !cc.ptr<i32>
// HYBRID:           %[[VAL_28:.*]] = cc.cast %[[VAL_27]] : (!cc.ptr<i32>) -> !cc.ptr<i8>
// HYBRID:           cc.store %[[VAL_28]], %[[VAL_26]] : !cc.ptr<!cc.ptr<i8>>
// HYBRID:           %[[VAL_29:.*]] = cc.cast %[[VAL_16]] : (!cc.ptr<!cc.struct<{!cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>, !cc.ptr<!cc.ptr<i8>>}>>) -> !cc.ptr<i8>
// HYBRID:           %[[VAL_30:.*]] = llvm.mlir.addressof @ghz.kernelName : !llvm.ptr<array<4 x i8>>
// HYBRID:           %[[VAL_31:.*]] = cc.cast %[[VAL_30]] : (!llvm.ptr<array<4 x i8>>) -> !cc.ptr<i8>
// HYBRID:           call @hybridLaunchKernel(%[[VAL_31]], %[[VAL_12]], %[[VAL_13]], %[[VAL_7]], %[[VAL_15]], %[[VAL_29]]) : (!cc.ptr<i8>, !cc.ptr<i8>, !cc.ptr<i8>, i64, i64, !cc.ptr<i8>) -> ()
// HYBRID:           %[[VAL_32:.*]] = cc.compute_ptr %[[VAL_10]][0, 1] : (!cc.ptr<!cc.array<!cc.struct<{i32, f64}> x ?>>) -> !cc.ptr<f64>
// HYBRID:           %[[VAL_33:.*]] = cc.load %[[VAL_32]] : !cc.ptr<f64>
// HYBRID:           return %[[VAL_33]] : f64
// HYBRID:         }
